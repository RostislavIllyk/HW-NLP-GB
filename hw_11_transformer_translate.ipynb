{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Neural machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNvjhDyAKk3U",
    "outputId": "9ea8f8e0-5589-4178-961f-4f068f3f9f77"
   },
   "outputs": [],
   "source": [
    "#!wget http://www.manythings.org/anki/rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83bg17Lr-7XK",
    "outputId": "955b8cae-5f8c-44da-aabd-11f826582455"
   },
   "outputs": [],
   "source": [
    "#!mkdir rus-eng\n",
    "#!unzip rus-eng.zip -d rus-eng/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7o5L92efMMhf",
    "outputId": "8e75c2a4-8601-45ba-8118-dbdaadc49fc1"
   },
   "outputs": [],
   "source": [
    "#!ls /content/rus-eng/ -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_file = \"./content/rus-eng/rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yV9lZXQXNbnH",
    "outputId": "91c2ca2a-4e3a-4cb2-a4b6-57898c682af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i can't go . <end>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"I can't go.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTbSbBz55QtF",
    "outputId": "68deb5e0-bf6c-4e8e-8cf9-f19a1ab4bb4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
      "<start> несомненно , для каждого мужчины в этом мире где то есть подходящая женщина , которая может стать ему женой , обратное верно и для женщин . но если учесть , что у человека может быть максимум несколько сотен знакомых , из которых лишь дюжина , а то и меньше , тех , кого он знает близко , а из этой дюжины у него один или от силы два друга , то можно легко увидеть , что с уч том миллионов живущих на земле людей , ни один подходящий мужчина , возможно , ещ не встретил подходящую женщину . <end>\n"
     ]
    }
   ],
   "source": [
    "en, ru = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(ru[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8j9g9AnIeZV",
    "outputId": "07e6e03d-4a80-4419-c092-bedfaffd87b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(429117, 429117)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 100000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QILQkOs3jFG",
    "outputId": "cd610938-12a5-4618-9ddd-a8777253e6ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXukARTDd7MT",
    "outputId": "a3c58700-25d9-4093-cc2a-4cea4c26637b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "6 ----> том\n",
      "218 ----> слышал\n",
      "3427 ----> крик\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "5 ----> tom\n",
      "205 ----> heard\n",
      "9 ----> a\n",
      "1429 ----> shout\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc6-NK1GtWQt",
    "outputId": "d49eb7f4-eed6-43fa-91e9-2a142698e60f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 15]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "<img src=\"images/transformer.png\"/>\n",
    "\n",
    "\n",
    "Идея в том, что каждое слово параллельно проходит через слои, изображенные на картинке.\n",
    "Некоторые из них — это стандартные fully-connected layers, некоторые — shortcut connections как в ResNet (там, где на картинке Add).\n",
    "\n",
    "\n",
    "Multi-head attention - это специальный новый слой, который дает возможность каждому входному вектору взаимодействовать с другими словами через attention mechanism, вместо передачи hidden state как в RNN или соседних слов как в CNN.\n",
    "\n",
    "\n",
    "<img src=\"images/mha.png\"/>\n",
    "\n",
    "\n",
    "<img src=\"images/AttTr.png\"/>\n",
    "\n",
    "\n",
    "Работа энкодера:\n",
    "\n",
    "\n",
    "Делаются эмбеддинги для всех слов предложения (вектора одинаковой размерности). Для примера пусть это будет предложение I am stupid. В эмбеддинг добавляется еще позиция слова в предложении.\n",
    "\n",
    "\n",
    "Берется вектор первого слова и вектор второго слова (I, am), подаются на однослойную сеть с одним выходом, которая выдает степень их похожести (скалярная величина). Эта скалярная величина умножается на вектор второго слова, получая его некоторую \"ослабленную\" на величину похожести копию.\n",
    "\n",
    "\n",
    "Вместо второго слова подается третье слово и делается тоже самое что в п.2. с той же самой сетью с теми же весами (для векторов I, stupid).\n",
    "\n",
    "\n",
    "Делая тоже самое для всех оставшихся слов предложения получаются их \"ослабленные\" (взвешенные) копии, которые выражают степень их похожести на первое слово. Далее эти все взвешенные вектора складываются друг с другом, получая один результирующий вектор размерности одного эмбединга:\n",
    "output=am * weight(I, am) + stupid * weight(I, stupid)\n",
    "\n",
    "\n",
    "Это механизм \"обычного\" attention.\n",
    "Так как оценка похожести слов всего одним способом (по одному критерию) считается недостаточной, тоже самое (п.2-4) повторяется несколько раз с другими весами. Типа одна один attention может определять похожесть слов по смысловой нагрузке, другой по грамматической, остальные еще как-то и т.п.\n",
    "\n",
    "\n",
    "На выходе п.5. получается несколько векторов, каждый из которых является взвешенной суммой всех остальных слов предложения относительно их похожести на первое слово (I). Конкантенируем этот вректор в один.\n",
    "\n",
    "\n",
    "Дальше ставится еще один слой линейного преобразования, уменьшающий размерность результата п.6. до размерности вектора одного эмбединга. Получается некое представление первого слова предложения, составленное из взвешенных векторов всех остальных слов предложения.\n",
    "\n",
    "\n",
    "Такой же процесс производится для всех других слов в предложении.\n",
    "\n",
    "\n",
    "Так как размерность выхода та же, то можно проделать все тоже самое еще раз (п.2-8), но вместо оригинальных эмбеддингов слов взять то, что получается после прохода через этот Multi-head attention, а нейросети аттеншенов внутри взять с другими весами (веса между слоями не общие). И таких слоев можно сделать много (у гугла 6). Однако между первым и вторым слоем добавляется еще полносвязный слой и residual соединения, чтобы добавить сети выразительности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    \n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "input_vocab_size = vocab_inp_size + 1\n",
    "target_vocab_size = vocab_tar_size + 1\n",
    "\n",
    "inp_lang_tokenizer, targ_lang_tokenizer = inp_lang, targ_lang\n",
    "#input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
    "#target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.8860 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.8126 Accuracy 0.0003\n",
      "Epoch 1 Batch 100 Loss 8.6453 Accuracy 0.0475\n",
      "Epoch 1 Batch 150 Loss 8.4623 Accuracy 0.0745\n",
      "Epoch 1 Batch 200 Loss 8.2602 Accuracy 0.0929\n",
      "Epoch 1 Batch 250 Loss 8.0232 Accuracy 0.1083\n",
      "Epoch 1 Batch 300 Loss 7.7574 Accuracy 0.1201\n",
      "Epoch 1 Batch 350 Loss 7.4765 Accuracy 0.1285\n",
      "Epoch 1 Batch 400 Loss 7.1971 Accuracy 0.1353\n",
      "Epoch 1 Batch 450 Loss 6.9326 Accuracy 0.1418\n",
      "Epoch 1 Batch 500 Loss 6.6893 Accuracy 0.1501\n",
      "Epoch 1 Batch 550 Loss 6.4620 Accuracy 0.1583\n",
      "Epoch 1 Batch 600 Loss 6.2534 Accuracy 0.1656\n",
      "Epoch 1 Batch 650 Loss 6.0637 Accuracy 0.1722\n",
      "Epoch 1 Batch 700 Loss 5.8891 Accuracy 0.1784\n",
      "Epoch 1 Batch 750 Loss 5.7275 Accuracy 0.1844\n",
      "Epoch 1 Batch 800 Loss 5.5795 Accuracy 0.1900\n",
      "Epoch 1 Batch 850 Loss 5.4420 Accuracy 0.1955\n",
      "Epoch 1 Batch 900 Loss 5.3134 Accuracy 0.2009\n",
      "Epoch 1 Batch 950 Loss 5.1928 Accuracy 0.2061\n",
      "Epoch 1 Batch 1000 Loss 5.0804 Accuracy 0.2109\n",
      "Epoch 1 Batch 1050 Loss 4.9749 Accuracy 0.2156\n",
      "Epoch 1 Batch 1100 Loss 4.8759 Accuracy 0.2201\n",
      "Epoch 1 Batch 1150 Loss 4.7817 Accuracy 0.2243\n",
      "Epoch 1 Batch 1200 Loss 4.6932 Accuracy 0.2285\n",
      "Epoch 1 Loss 4.6109 Accuracy 0.2324\n",
      "=================================================================\n",
      "Epoch 2 Batch 0 Loss 2.6001 Accuracy 0.3313\n",
      "Epoch 2 Batch 50 Loss 2.4765 Accuracy 0.3388\n",
      "Epoch 2 Batch 100 Loss 2.4482 Accuracy 0.3398\n",
      "Epoch 2 Batch 150 Loss 2.4236 Accuracy 0.3417\n",
      "Epoch 2 Batch 200 Loss 2.4002 Accuracy 0.3438\n",
      "Epoch 2 Batch 250 Loss 2.3785 Accuracy 0.3455\n",
      "Epoch 2 Batch 300 Loss 2.3490 Accuracy 0.3482\n",
      "Epoch 2 Batch 350 Loss 2.3288 Accuracy 0.3499\n",
      "Epoch 2 Batch 400 Loss 2.3057 Accuracy 0.3520\n",
      "Epoch 2 Batch 450 Loss 2.2816 Accuracy 0.3540\n",
      "Epoch 2 Batch 500 Loss 2.2569 Accuracy 0.3560\n",
      "Epoch 2 Batch 550 Loss 2.2323 Accuracy 0.3581\n",
      "Epoch 2 Batch 600 Loss 2.2099 Accuracy 0.3599\n",
      "Epoch 2 Batch 650 Loss 2.1901 Accuracy 0.3615\n",
      "Epoch 2 Batch 700 Loss 2.1701 Accuracy 0.3631\n",
      "Epoch 2 Batch 750 Loss 2.1500 Accuracy 0.3649\n",
      "Epoch 2 Batch 800 Loss 2.1319 Accuracy 0.3664\n",
      "Epoch 2 Batch 850 Loss 2.1113 Accuracy 0.3682\n",
      "Epoch 2 Batch 900 Loss 2.0932 Accuracy 0.3698\n",
      "Epoch 2 Batch 950 Loss 2.0740 Accuracy 0.3715\n",
      "Epoch 2 Batch 1000 Loss 2.0552 Accuracy 0.3732\n",
      "Epoch 2 Batch 1050 Loss 2.0362 Accuracy 0.3749\n",
      "Epoch 2 Batch 1100 Loss 2.0172 Accuracy 0.3767\n",
      "Epoch 2 Batch 1150 Loss 2.0001 Accuracy 0.3782\n",
      "Epoch 2 Batch 1200 Loss 1.9837 Accuracy 0.3797\n",
      "Epoch 2 Loss 1.9663 Accuracy 0.3813\n",
      "=================================================================\n",
      "Epoch 3 Batch 0 Loss 1.5803 Accuracy 0.3953\n",
      "Epoch 3 Batch 50 Loss 1.4508 Accuracy 0.4274\n",
      "Epoch 3 Batch 100 Loss 1.4251 Accuracy 0.4293\n",
      "Epoch 3 Batch 150 Loss 1.4115 Accuracy 0.4301\n",
      "Epoch 3 Batch 200 Loss 1.4078 Accuracy 0.4307\n",
      "Epoch 3 Batch 250 Loss 1.4000 Accuracy 0.4313\n",
      "Epoch 3 Batch 300 Loss 1.3943 Accuracy 0.4323\n",
      "Epoch 3 Batch 350 Loss 1.3929 Accuracy 0.4325\n",
      "Epoch 3 Batch 400 Loss 1.3805 Accuracy 0.4335\n",
      "Epoch 3 Batch 450 Loss 1.3730 Accuracy 0.4342\n",
      "Epoch 3 Batch 500 Loss 1.3630 Accuracy 0.4351\n",
      "Epoch 3 Batch 550 Loss 1.3609 Accuracy 0.4353\n",
      "Epoch 3 Batch 600 Loss 1.3545 Accuracy 0.4357\n",
      "Epoch 3 Batch 650 Loss 1.3467 Accuracy 0.4364\n",
      "Epoch 3 Batch 700 Loss 1.3425 Accuracy 0.4368\n",
      "Epoch 3 Batch 750 Loss 1.3348 Accuracy 0.4374\n",
      "Epoch 3 Batch 800 Loss 1.3282 Accuracy 0.4380\n",
      "Epoch 3 Batch 850 Loss 1.3228 Accuracy 0.4385\n",
      "Epoch 3 Batch 900 Loss 1.3151 Accuracy 0.4391\n",
      "Epoch 3 Batch 950 Loss 1.3097 Accuracy 0.4396\n",
      "Epoch 3 Batch 1000 Loss 1.3037 Accuracy 0.4401\n",
      "Epoch 3 Batch 1050 Loss 1.2987 Accuracy 0.4405\n",
      "Epoch 3 Batch 1100 Loss 1.2921 Accuracy 0.4412\n",
      "Epoch 3 Batch 1150 Loss 1.2852 Accuracy 0.4418\n",
      "Epoch 3 Batch 1200 Loss 1.2793 Accuracy 0.4424\n",
      "Epoch 3 Loss 1.2728 Accuracy 0.4430\n",
      "=================================================================\n",
      "Epoch 4 Batch 0 Loss 0.9184 Accuracy 0.4563\n",
      "Epoch 4 Batch 50 Loss 0.9877 Accuracy 0.4676\n",
      "Epoch 4 Batch 100 Loss 0.9904 Accuracy 0.4666\n",
      "Epoch 4 Batch 150 Loss 0.9917 Accuracy 0.4666\n",
      "Epoch 4 Batch 200 Loss 0.9963 Accuracy 0.4664\n",
      "Epoch 4 Batch 250 Loss 0.9955 Accuracy 0.4664\n",
      "Epoch 4 Batch 300 Loss 0.9941 Accuracy 0.4662\n",
      "Epoch 4 Batch 350 Loss 0.9953 Accuracy 0.4660\n",
      "Epoch 4 Batch 400 Loss 0.9938 Accuracy 0.4665\n",
      "Epoch 4 Batch 450 Loss 0.9938 Accuracy 0.4663\n",
      "Epoch 4 Batch 500 Loss 0.9950 Accuracy 0.4660\n",
      "Epoch 4 Batch 550 Loss 0.9959 Accuracy 0.4659\n",
      "Epoch 4 Batch 600 Loss 0.9918 Accuracy 0.4663\n",
      "Epoch 4 Batch 650 Loss 0.9897 Accuracy 0.4665\n",
      "Epoch 4 Batch 700 Loss 0.9884 Accuracy 0.4669\n",
      "Epoch 4 Batch 750 Loss 0.9870 Accuracy 0.4671\n",
      "Epoch 4 Batch 800 Loss 0.9871 Accuracy 0.4674\n",
      "Epoch 4 Batch 850 Loss 0.9844 Accuracy 0.4679\n",
      "Epoch 4 Batch 900 Loss 0.9850 Accuracy 0.4679\n",
      "Epoch 4 Batch 950 Loss 0.9829 Accuracy 0.4682\n",
      "Epoch 4 Batch 1000 Loss 0.9817 Accuracy 0.4684\n",
      "Epoch 4 Batch 1050 Loss 0.9801 Accuracy 0.4687\n",
      "Epoch 4 Batch 1100 Loss 0.9772 Accuracy 0.4690\n",
      "Epoch 4 Batch 1150 Loss 0.9771 Accuracy 0.4692\n",
      "Epoch 4 Batch 1200 Loss 0.9736 Accuracy 0.4696\n",
      "Epoch 4 Loss 0.9721 Accuracy 0.4698\n",
      "=================================================================\n",
      "Epoch 5 Batch 0 Loss 0.7846 Accuracy 0.4969\n",
      "Epoch 5 Batch 50 Loss 0.7729 Accuracy 0.4858\n",
      "Epoch 5 Batch 100 Loss 0.7703 Accuracy 0.4867\n",
      "Epoch 5 Batch 150 Loss 0.7647 Accuracy 0.4879\n",
      "Epoch 5 Batch 200 Loss 0.7647 Accuracy 0.4885\n",
      "Epoch 5 Batch 250 Loss 0.7680 Accuracy 0.4889\n",
      "Epoch 5 Batch 300 Loss 0.7736 Accuracy 0.4888\n",
      "Epoch 5 Batch 350 Loss 0.7734 Accuracy 0.4890\n",
      "Epoch 5 Batch 400 Loss 0.7760 Accuracy 0.4888\n",
      "Epoch 5 Batch 450 Loss 0.7810 Accuracy 0.4884\n",
      "Epoch 5 Batch 500 Loss 0.7827 Accuracy 0.4884\n",
      "Epoch 5 Batch 550 Loss 0.7825 Accuracy 0.4881\n",
      "Epoch 5 Batch 600 Loss 0.7830 Accuracy 0.4883\n",
      "Epoch 5 Batch 650 Loss 0.7834 Accuracy 0.4886\n",
      "Epoch 5 Batch 700 Loss 0.7859 Accuracy 0.4882\n",
      "Epoch 5 Batch 750 Loss 0.7852 Accuracy 0.4883\n",
      "Epoch 5 Batch 800 Loss 0.7860 Accuracy 0.4883\n",
      "Epoch 5 Batch 850 Loss 0.7855 Accuracy 0.4883\n",
      "Epoch 5 Batch 900 Loss 0.7842 Accuracy 0.4884\n",
      "Epoch 5 Batch 950 Loss 0.7834 Accuracy 0.4885\n",
      "Epoch 5 Batch 1000 Loss 0.7831 Accuracy 0.4887\n",
      "Epoch 5 Batch 1050 Loss 0.7827 Accuracy 0.4887\n",
      "Epoch 5 Batch 1100 Loss 0.7827 Accuracy 0.4889\n",
      "Epoch 5 Batch 1150 Loss 0.7828 Accuracy 0.4889\n",
      "Epoch 5 Batch 1200 Loss 0.7829 Accuracy 0.4888\n",
      "Epoch 5 Loss 0.7828 Accuracy 0.4888\n",
      "=================================================================\n",
      "Epoch 6 Batch 0 Loss 0.8417 Accuracy 0.4469\n",
      "Epoch 6 Batch 50 Loss 0.6293 Accuracy 0.5037\n",
      "Epoch 6 Batch 100 Loss 0.6318 Accuracy 0.5031\n",
      "Epoch 6 Batch 150 Loss 0.6284 Accuracy 0.5042\n",
      "Epoch 6 Batch 200 Loss 0.6351 Accuracy 0.5032\n",
      "Epoch 6 Batch 250 Loss 0.6377 Accuracy 0.5034\n",
      "Epoch 6 Batch 300 Loss 0.6447 Accuracy 0.5030\n",
      "Epoch 6 Batch 350 Loss 0.6466 Accuracy 0.5028\n",
      "Epoch 6 Batch 400 Loss 0.6488 Accuracy 0.5021\n",
      "Epoch 6 Batch 450 Loss 0.6531 Accuracy 0.5019\n",
      "Epoch 6 Batch 500 Loss 0.6525 Accuracy 0.5020\n",
      "Epoch 6 Batch 550 Loss 0.6547 Accuracy 0.5020\n",
      "Epoch 6 Batch 600 Loss 0.6561 Accuracy 0.5017\n",
      "Epoch 6 Batch 650 Loss 0.6554 Accuracy 0.5017\n",
      "Epoch 6 Batch 700 Loss 0.6577 Accuracy 0.5013\n",
      "Epoch 6 Batch 750 Loss 0.6606 Accuracy 0.5009\n",
      "Epoch 6 Batch 800 Loss 0.6624 Accuracy 0.5009\n",
      "Epoch 6 Batch 850 Loss 0.6634 Accuracy 0.5008\n",
      "Epoch 6 Batch 900 Loss 0.6639 Accuracy 0.5007\n",
      "Epoch 6 Batch 950 Loss 0.6665 Accuracy 0.5004\n",
      "Epoch 6 Batch 1000 Loss 0.6670 Accuracy 0.5005\n",
      "Epoch 6 Batch 1050 Loss 0.6679 Accuracy 0.5006\n",
      "Epoch 6 Batch 1100 Loss 0.6691 Accuracy 0.5005\n",
      "Epoch 6 Batch 1150 Loss 0.6693 Accuracy 0.5004\n",
      "Epoch 6 Batch 1200 Loss 0.6699 Accuracy 0.5005\n",
      "Epoch 6 Loss 0.6701 Accuracy 0.5005\n",
      "=================================================================\n",
      "Epoch 7 Batch 0 Loss 0.3805 Accuracy 0.5266\n",
      "Epoch 7 Batch 50 Loss 0.5593 Accuracy 0.5112\n",
      "Epoch 7 Batch 100 Loss 0.5625 Accuracy 0.5123\n",
      "Epoch 7 Batch 150 Loss 0.5600 Accuracy 0.5120\n",
      "Epoch 7 Batch 200 Loss 0.5602 Accuracy 0.5118\n",
      "Epoch 7 Batch 250 Loss 0.5606 Accuracy 0.5120\n",
      "Epoch 7 Batch 300 Loss 0.5631 Accuracy 0.5116\n",
      "Epoch 7 Batch 350 Loss 0.5651 Accuracy 0.5110\n",
      "Epoch 7 Batch 400 Loss 0.5704 Accuracy 0.5103\n",
      "Epoch 7 Batch 450 Loss 0.5708 Accuracy 0.5103\n",
      "Epoch 7 Batch 500 Loss 0.5738 Accuracy 0.5100\n",
      "Epoch 7 Batch 550 Loss 0.5784 Accuracy 0.5097\n",
      "Epoch 7 Batch 600 Loss 0.5799 Accuracy 0.5095\n",
      "Epoch 7 Batch 650 Loss 0.5822 Accuracy 0.5092\n",
      "Epoch 7 Batch 700 Loss 0.5851 Accuracy 0.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 750 Loss 0.5855 Accuracy 0.5090\n",
      "Epoch 7 Batch 800 Loss 0.5873 Accuracy 0.5089\n",
      "Epoch 7 Batch 850 Loss 0.5885 Accuracy 0.5088\n",
      "Epoch 7 Batch 900 Loss 0.5899 Accuracy 0.5086\n",
      "Epoch 7 Batch 950 Loss 0.5909 Accuracy 0.5085\n",
      "Epoch 7 Batch 1000 Loss 0.5922 Accuracy 0.5086\n",
      "Epoch 7 Batch 1050 Loss 0.5945 Accuracy 0.5085\n",
      "Epoch 7 Batch 1100 Loss 0.5951 Accuracy 0.5085\n",
      "Epoch 7 Batch 1150 Loss 0.5968 Accuracy 0.5083\n",
      "Epoch 7 Batch 1200 Loss 0.5968 Accuracy 0.5084\n",
      "Epoch 7 Loss 0.5982 Accuracy 0.5083\n",
      "=================================================================\n",
      "Epoch 8 Batch 0 Loss 0.4686 Accuracy 0.5063\n",
      "Epoch 8 Batch 50 Loss 0.4960 Accuracy 0.5186\n",
      "Epoch 8 Batch 100 Loss 0.4938 Accuracy 0.5190\n",
      "Epoch 8 Batch 150 Loss 0.4973 Accuracy 0.5188\n",
      "Epoch 8 Batch 200 Loss 0.4981 Accuracy 0.5182\n",
      "Epoch 8 Batch 250 Loss 0.4972 Accuracy 0.5184\n",
      "Epoch 8 Batch 300 Loss 0.5040 Accuracy 0.5177\n",
      "Epoch 8 Batch 350 Loss 0.5071 Accuracy 0.5177\n",
      "Epoch 8 Batch 400 Loss 0.5102 Accuracy 0.5175\n",
      "Epoch 8 Batch 450 Loss 0.5148 Accuracy 0.5171\n",
      "Epoch 8 Batch 500 Loss 0.5188 Accuracy 0.5169\n",
      "Epoch 8 Batch 550 Loss 0.5204 Accuracy 0.5167\n",
      "Epoch 8 Batch 600 Loss 0.5203 Accuracy 0.5168\n",
      "Epoch 8 Batch 650 Loss 0.5237 Accuracy 0.5165\n",
      "Epoch 8 Batch 700 Loss 0.5250 Accuracy 0.5162\n",
      "Epoch 8 Batch 750 Loss 0.5281 Accuracy 0.5157\n",
      "Epoch 8 Batch 800 Loss 0.5283 Accuracy 0.5156\n",
      "Epoch 8 Batch 850 Loss 0.5308 Accuracy 0.5155\n",
      "Epoch 8 Batch 900 Loss 0.5329 Accuracy 0.5153\n",
      "Epoch 8 Batch 950 Loss 0.5345 Accuracy 0.5151\n",
      "Epoch 8 Batch 1000 Loss 0.5358 Accuracy 0.5151\n",
      "Epoch 8 Batch 1050 Loss 0.5377 Accuracy 0.5149\n",
      "Epoch 8 Batch 1100 Loss 0.5402 Accuracy 0.5147\n",
      "Epoch 8 Batch 1150 Loss 0.5429 Accuracy 0.5146\n",
      "Epoch 8 Batch 1200 Loss 0.5449 Accuracy 0.5144\n",
      "Epoch 8 Loss 0.5454 Accuracy 0.5144\n",
      "=================================================================\n",
      "Epoch 9 Batch 0 Loss 0.3743 Accuracy 0.5656\n",
      "Epoch 9 Batch 50 Loss 0.4419 Accuracy 0.5233\n",
      "Epoch 9 Batch 100 Loss 0.4495 Accuracy 0.5229\n",
      "Epoch 9 Batch 150 Loss 0.4548 Accuracy 0.5231\n",
      "Epoch 9 Batch 200 Loss 0.4564 Accuracy 0.5230\n",
      "Epoch 9 Batch 250 Loss 0.4623 Accuracy 0.5231\n",
      "Epoch 9 Batch 300 Loss 0.4657 Accuracy 0.5230\n",
      "Epoch 9 Batch 350 Loss 0.4699 Accuracy 0.5227\n",
      "Epoch 9 Batch 400 Loss 0.4717 Accuracy 0.5223\n",
      "Epoch 9 Batch 450 Loss 0.4732 Accuracy 0.5220\n",
      "Epoch 9 Batch 500 Loss 0.4759 Accuracy 0.5216\n",
      "Epoch 9 Batch 550 Loss 0.4798 Accuracy 0.5213\n",
      "Epoch 9 Batch 600 Loss 0.4845 Accuracy 0.5208\n",
      "Epoch 9 Batch 650 Loss 0.4859 Accuracy 0.5208\n",
      "Epoch 9 Batch 700 Loss 0.4881 Accuracy 0.5207\n",
      "Epoch 9 Batch 750 Loss 0.4905 Accuracy 0.5204\n",
      "Epoch 9 Batch 800 Loss 0.4925 Accuracy 0.5203\n",
      "Epoch 9 Batch 850 Loss 0.4947 Accuracy 0.5200\n",
      "Epoch 9 Batch 900 Loss 0.4968 Accuracy 0.5198\n",
      "Epoch 9 Batch 950 Loss 0.4988 Accuracy 0.5196\n",
      "Epoch 9 Batch 1000 Loss 0.5004 Accuracy 0.5195\n",
      "Epoch 9 Batch 1050 Loss 0.5024 Accuracy 0.5194\n",
      "Epoch 9 Batch 1100 Loss 0.5038 Accuracy 0.5193\n",
      "Epoch 9 Batch 1150 Loss 0.5060 Accuracy 0.5191\n",
      "Epoch 9 Batch 1200 Loss 0.5074 Accuracy 0.5191\n",
      "Epoch 9 Loss 0.5088 Accuracy 0.5190\n",
      "=================================================================\n",
      "Epoch 10 Batch 0 Loss 0.4239 Accuracy 0.5344\n",
      "Epoch 10 Batch 50 Loss 0.4369 Accuracy 0.5263\n",
      "Epoch 10 Batch 100 Loss 0.4378 Accuracy 0.5256\n",
      "Epoch 10 Batch 150 Loss 0.4321 Accuracy 0.5264\n",
      "Epoch 10 Batch 200 Loss 0.4356 Accuracy 0.5259\n",
      "Epoch 10 Batch 250 Loss 0.4314 Accuracy 0.5268\n",
      "Epoch 10 Batch 300 Loss 0.4341 Accuracy 0.5263\n",
      "Epoch 10 Batch 350 Loss 0.4378 Accuracy 0.5258\n",
      "Epoch 10 Batch 400 Loss 0.4413 Accuracy 0.5258\n",
      "Epoch 10 Batch 450 Loss 0.4442 Accuracy 0.5257\n",
      "Epoch 10 Batch 500 Loss 0.4467 Accuracy 0.5254\n",
      "Epoch 10 Batch 550 Loss 0.4499 Accuracy 0.5251\n",
      "Epoch 10 Batch 600 Loss 0.4534 Accuracy 0.5246\n",
      "Epoch 10 Batch 650 Loss 0.4555 Accuracy 0.5244\n",
      "Epoch 10 Batch 700 Loss 0.4580 Accuracy 0.5242\n",
      "Epoch 10 Batch 750 Loss 0.4601 Accuracy 0.5239\n",
      "Epoch 10 Batch 800 Loss 0.4622 Accuracy 0.5237\n",
      "Epoch 10 Batch 850 Loss 0.4648 Accuracy 0.5234\n",
      "Epoch 10 Batch 900 Loss 0.4663 Accuracy 0.5235\n",
      "Epoch 10 Batch 950 Loss 0.4678 Accuracy 0.5233\n",
      "Epoch 10 Batch 1000 Loss 0.4697 Accuracy 0.5231\n",
      "Epoch 10 Batch 1050 Loss 0.4717 Accuracy 0.5230\n",
      "Epoch 10 Batch 1100 Loss 0.4743 Accuracy 0.5228\n",
      "Epoch 10 Batch 1150 Loss 0.4768 Accuracy 0.5225\n",
      "Epoch 10 Batch 1200 Loss 0.4777 Accuracy 0.5224\n",
      "Epoch 10 Loss 0.4789 Accuracy 0.5224\n",
      "=================================================================\n",
      "Epoch 11 Batch 0 Loss 0.4243 Accuracy 0.5391\n",
      "Epoch 11 Batch 50 Loss 0.3964 Accuracy 0.5311\n",
      "Epoch 11 Batch 100 Loss 0.4038 Accuracy 0.5283\n",
      "Epoch 11 Batch 150 Loss 0.4090 Accuracy 0.5286\n",
      "Epoch 11 Batch 200 Loss 0.4121 Accuracy 0.5282\n",
      "Epoch 11 Batch 250 Loss 0.4146 Accuracy 0.5275\n",
      "Epoch 11 Batch 300 Loss 0.4182 Accuracy 0.5271\n",
      "Epoch 11 Batch 350 Loss 0.4198 Accuracy 0.5271\n",
      "Epoch 11 Batch 400 Loss 0.4234 Accuracy 0.5272\n",
      "Epoch 11 Batch 450 Loss 0.4276 Accuracy 0.5271\n",
      "Epoch 11 Batch 500 Loss 0.4307 Accuracy 0.5267\n",
      "Epoch 11 Batch 550 Loss 0.4339 Accuracy 0.5264\n",
      "Epoch 11 Batch 600 Loss 0.4339 Accuracy 0.5268\n",
      "Epoch 11 Batch 650 Loss 0.4355 Accuracy 0.5266\n",
      "Epoch 11 Batch 700 Loss 0.4371 Accuracy 0.5266\n",
      "Epoch 11 Batch 750 Loss 0.4391 Accuracy 0.5264\n",
      "Epoch 11 Batch 800 Loss 0.4418 Accuracy 0.5262\n",
      "Epoch 11 Batch 850 Loss 0.4442 Accuracy 0.5260\n",
      "Epoch 11 Batch 900 Loss 0.4452 Accuracy 0.5261\n",
      "Epoch 11 Batch 950 Loss 0.4473 Accuracy 0.5258\n",
      "Epoch 11 Batch 1000 Loss 0.4491 Accuracy 0.5258\n",
      "Epoch 11 Batch 1050 Loss 0.4503 Accuracy 0.5257\n",
      "Epoch 11 Batch 1100 Loss 0.4515 Accuracy 0.5256\n",
      "Epoch 11 Batch 1150 Loss 0.4529 Accuracy 0.5255\n",
      "Epoch 11 Batch 1200 Loss 0.4551 Accuracy 0.5254\n",
      "Epoch 11 Loss 0.4568 Accuracy 0.5252\n",
      "=================================================================\n",
      "Epoch 12 Batch 0 Loss 0.3457 Accuracy 0.5312\n",
      "Epoch 12 Batch 50 Loss 0.3731 Accuracy 0.5335\n",
      "Epoch 12 Batch 100 Loss 0.3746 Accuracy 0.5337\n",
      "Epoch 12 Batch 150 Loss 0.3882 Accuracy 0.5319\n",
      "Epoch 12 Batch 200 Loss 0.3942 Accuracy 0.5306\n",
      "Epoch 12 Batch 250 Loss 0.3972 Accuracy 0.5309\n",
      "Epoch 12 Batch 300 Loss 0.3996 Accuracy 0.5307\n",
      "Epoch 12 Batch 350 Loss 0.4017 Accuracy 0.5308\n",
      "Epoch 12 Batch 400 Loss 0.4043 Accuracy 0.5303\n",
      "Epoch 12 Batch 450 Loss 0.4056 Accuracy 0.5301\n",
      "Epoch 12 Batch 500 Loss 0.4089 Accuracy 0.5298\n",
      "Epoch 12 Batch 550 Loss 0.4118 Accuracy 0.5295\n",
      "Epoch 12 Batch 600 Loss 0.4152 Accuracy 0.5290\n",
      "Epoch 12 Batch 650 Loss 0.4172 Accuracy 0.5288\n",
      "Epoch 12 Batch 700 Loss 0.4195 Accuracy 0.5287\n",
      "Epoch 12 Batch 750 Loss 0.4219 Accuracy 0.5284\n",
      "Epoch 12 Batch 800 Loss 0.4237 Accuracy 0.5283\n",
      "Epoch 12 Batch 850 Loss 0.4255 Accuracy 0.5280\n",
      "Epoch 12 Batch 900 Loss 0.4269 Accuracy 0.5280\n",
      "Epoch 12 Batch 950 Loss 0.4286 Accuracy 0.5279\n",
      "Epoch 12 Batch 1000 Loss 0.4300 Accuracy 0.5277\n",
      "Epoch 12 Batch 1050 Loss 0.4318 Accuracy 0.5277\n",
      "Epoch 12 Batch 1100 Loss 0.4330 Accuracy 0.5277\n",
      "Epoch 12 Batch 1150 Loss 0.4341 Accuracy 0.5276\n",
      "Epoch 12 Batch 1200 Loss 0.4355 Accuracy 0.5275\n",
      "Epoch 12 Loss 0.4366 Accuracy 0.5275\n",
      "=================================================================\n",
      "Epoch 13 Batch 0 Loss 0.4993 Accuracy 0.5344\n",
      "Epoch 13 Batch 50 Loss 0.3753 Accuracy 0.5339\n",
      "Epoch 13 Batch 100 Loss 0.3777 Accuracy 0.5339\n",
      "Epoch 13 Batch 150 Loss 0.3770 Accuracy 0.5338\n",
      "Epoch 13 Batch 200 Loss 0.3797 Accuracy 0.5334\n",
      "Epoch 13 Batch 250 Loss 0.3841 Accuracy 0.5333\n",
      "Epoch 13 Batch 300 Loss 0.3862 Accuracy 0.5330\n",
      "Epoch 13 Batch 350 Loss 0.3866 Accuracy 0.5328\n",
      "Epoch 13 Batch 400 Loss 0.3885 Accuracy 0.5323\n",
      "Epoch 13 Batch 450 Loss 0.3903 Accuracy 0.5322\n",
      "Epoch 13 Batch 500 Loss 0.3934 Accuracy 0.5318\n",
      "Epoch 13 Batch 550 Loss 0.3958 Accuracy 0.5318\n",
      "Epoch 13 Batch 600 Loss 0.3978 Accuracy 0.5317\n",
      "Epoch 13 Batch 650 Loss 0.4001 Accuracy 0.5316\n",
      "Epoch 13 Batch 700 Loss 0.4021 Accuracy 0.5313\n",
      "Epoch 13 Batch 750 Loss 0.4039 Accuracy 0.5312\n",
      "Epoch 13 Batch 800 Loss 0.4062 Accuracy 0.5308\n",
      "Epoch 13 Batch 850 Loss 0.4074 Accuracy 0.5308\n",
      "Epoch 13 Batch 900 Loss 0.4093 Accuracy 0.5305\n",
      "Epoch 13 Batch 950 Loss 0.4098 Accuracy 0.5306\n",
      "Epoch 13 Batch 1000 Loss 0.4122 Accuracy 0.5304\n",
      "Epoch 13 Batch 1050 Loss 0.4144 Accuracy 0.5302\n",
      "Epoch 13 Batch 1100 Loss 0.4163 Accuracy 0.5299\n",
      "Epoch 13 Batch 1150 Loss 0.4175 Accuracy 0.5298\n",
      "Epoch 13 Batch 1200 Loss 0.4190 Accuracy 0.5296\n",
      "Epoch 13 Loss 0.4206 Accuracy 0.5296\n",
      "=================================================================\n",
      "Epoch 14 Batch 0 Loss 0.3472 Accuracy 0.5344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 50 Loss 0.3501 Accuracy 0.5356\n",
      "Epoch 14 Batch 100 Loss 0.3550 Accuracy 0.5351\n",
      "Epoch 14 Batch 150 Loss 0.3571 Accuracy 0.5354\n",
      "Epoch 14 Batch 200 Loss 0.3600 Accuracy 0.5357\n",
      "Epoch 14 Batch 250 Loss 0.3625 Accuracy 0.5359\n",
      "Epoch 14 Batch 300 Loss 0.3676 Accuracy 0.5353\n",
      "Epoch 14 Batch 350 Loss 0.3718 Accuracy 0.5349\n",
      "Epoch 14 Batch 400 Loss 0.3749 Accuracy 0.5346\n",
      "Epoch 14 Batch 450 Loss 0.3764 Accuracy 0.5345\n",
      "Epoch 14 Batch 500 Loss 0.3796 Accuracy 0.5341\n",
      "Epoch 14 Batch 550 Loss 0.3829 Accuracy 0.5337\n",
      "Epoch 14 Batch 600 Loss 0.3843 Accuracy 0.5335\n",
      "Epoch 14 Batch 650 Loss 0.3875 Accuracy 0.5332\n",
      "Epoch 14 Batch 700 Loss 0.3894 Accuracy 0.5330\n",
      "Epoch 14 Batch 750 Loss 0.3907 Accuracy 0.5330\n",
      "Epoch 14 Batch 800 Loss 0.3915 Accuracy 0.5329\n",
      "Epoch 14 Batch 850 Loss 0.3939 Accuracy 0.5326\n",
      "Epoch 14 Batch 900 Loss 0.3970 Accuracy 0.5324\n",
      "Epoch 14 Batch 950 Loss 0.3991 Accuracy 0.5321\n",
      "Epoch 14 Batch 1000 Loss 0.4002 Accuracy 0.5322\n",
      "Epoch 14 Batch 1050 Loss 0.4023 Accuracy 0.5318\n",
      "Epoch 14 Batch 1100 Loss 0.4037 Accuracy 0.5318\n",
      "Epoch 14 Batch 1150 Loss 0.4052 Accuracy 0.5316\n",
      "Epoch 14 Batch 1200 Loss 0.4068 Accuracy 0.5313\n",
      "Epoch 14 Loss 0.4084 Accuracy 0.5312\n",
      "=================================================================\n",
      "Epoch 15 Batch 0 Loss 0.3334 Accuracy 0.5391\n",
      "Epoch 15 Batch 50 Loss 0.3528 Accuracy 0.5376\n",
      "Epoch 15 Batch 100 Loss 0.3518 Accuracy 0.5356\n",
      "Epoch 15 Batch 150 Loss 0.3506 Accuracy 0.5362\n",
      "Epoch 15 Batch 200 Loss 0.3552 Accuracy 0.5365\n",
      "Epoch 15 Batch 250 Loss 0.3586 Accuracy 0.5363\n",
      "Epoch 15 Batch 300 Loss 0.3596 Accuracy 0.5361\n",
      "Epoch 15 Batch 350 Loss 0.3616 Accuracy 0.5363\n",
      "Epoch 15 Batch 400 Loss 0.3651 Accuracy 0.5359\n",
      "Epoch 15 Batch 450 Loss 0.3681 Accuracy 0.5356\n",
      "Epoch 15 Batch 500 Loss 0.3702 Accuracy 0.5350\n",
      "Epoch 15 Batch 550 Loss 0.3725 Accuracy 0.5349\n",
      "Epoch 15 Batch 600 Loss 0.3748 Accuracy 0.5346\n",
      "Epoch 15 Batch 650 Loss 0.3762 Accuracy 0.5345\n",
      "Epoch 15 Batch 700 Loss 0.3789 Accuracy 0.5341\n",
      "Epoch 15 Batch 750 Loss 0.3802 Accuracy 0.5340\n",
      "Epoch 15 Batch 800 Loss 0.3806 Accuracy 0.5340\n",
      "Epoch 15 Batch 850 Loss 0.3814 Accuracy 0.5338\n",
      "Epoch 15 Batch 900 Loss 0.3818 Accuracy 0.5337\n",
      "Epoch 15 Batch 950 Loss 0.3836 Accuracy 0.5337\n",
      "Epoch 15 Batch 1000 Loss 0.3859 Accuracy 0.5335\n",
      "Epoch 15 Batch 1050 Loss 0.3876 Accuracy 0.5334\n",
      "Epoch 15 Batch 1100 Loss 0.3890 Accuracy 0.5334\n",
      "Epoch 15 Batch 1150 Loss 0.3905 Accuracy 0.5332\n",
      "Epoch 15 Batch 1200 Loss 0.3929 Accuracy 0.5331\n",
      "Epoch 15 Loss 0.3946 Accuracy 0.5329\n",
      "=================================================================\n",
      "Epoch 16 Batch 0 Loss 0.3874 Accuracy 0.5328\n",
      "Epoch 16 Batch 50 Loss 0.3651 Accuracy 0.5345\n",
      "Epoch 16 Batch 100 Loss 0.3508 Accuracy 0.5363\n",
      "Epoch 16 Batch 150 Loss 0.3471 Accuracy 0.5368\n",
      "Epoch 16 Batch 200 Loss 0.3501 Accuracy 0.5374\n",
      "Epoch 16 Batch 250 Loss 0.3514 Accuracy 0.5369\n",
      "Epoch 16 Batch 300 Loss 0.3562 Accuracy 0.5363\n",
      "Epoch 16 Batch 350 Loss 0.3549 Accuracy 0.5365\n",
      "Epoch 16 Batch 400 Loss 0.3540 Accuracy 0.5365\n",
      "Epoch 16 Batch 450 Loss 0.3559 Accuracy 0.5362\n",
      "Epoch 16 Batch 500 Loss 0.3596 Accuracy 0.5359\n",
      "Epoch 16 Batch 550 Loss 0.3608 Accuracy 0.5359\n",
      "Epoch 16 Batch 600 Loss 0.3635 Accuracy 0.5360\n",
      "Epoch 16 Batch 650 Loss 0.3648 Accuracy 0.5357\n",
      "Epoch 16 Batch 700 Loss 0.3677 Accuracy 0.5354\n",
      "Epoch 16 Batch 750 Loss 0.3704 Accuracy 0.5351\n",
      "Epoch 16 Batch 800 Loss 0.3724 Accuracy 0.5350\n",
      "Epoch 16 Batch 850 Loss 0.3742 Accuracy 0.5347\n",
      "Epoch 16 Batch 900 Loss 0.3762 Accuracy 0.5347\n",
      "Epoch 16 Batch 950 Loss 0.3761 Accuracy 0.5348\n",
      "Epoch 16 Batch 1000 Loss 0.3767 Accuracy 0.5347\n",
      "Epoch 16 Batch 1050 Loss 0.3795 Accuracy 0.5345\n",
      "Epoch 16 Batch 1100 Loss 0.3808 Accuracy 0.5345\n",
      "Epoch 16 Batch 1150 Loss 0.3822 Accuracy 0.5345\n",
      "Epoch 16 Batch 1200 Loss 0.3837 Accuracy 0.5344\n",
      "Epoch 16 Loss 0.3854 Accuracy 0.5343\n",
      "=================================================================\n",
      "Epoch 17 Batch 0 Loss 0.3321 Accuracy 0.4906\n",
      "Epoch 17 Batch 50 Loss 0.3343 Accuracy 0.5380\n",
      "Epoch 17 Batch 100 Loss 0.3278 Accuracy 0.5381\n",
      "Epoch 17 Batch 150 Loss 0.3309 Accuracy 0.5373\n",
      "Epoch 17 Batch 200 Loss 0.3357 Accuracy 0.5375\n",
      "Epoch 17 Batch 250 Loss 0.3392 Accuracy 0.5378\n",
      "Epoch 17 Batch 300 Loss 0.3398 Accuracy 0.5381\n",
      "Epoch 17 Batch 350 Loss 0.3408 Accuracy 0.5381\n",
      "Epoch 17 Batch 400 Loss 0.3432 Accuracy 0.5381\n",
      "Epoch 17 Batch 450 Loss 0.3447 Accuracy 0.5380\n",
      "Epoch 17 Batch 500 Loss 0.3473 Accuracy 0.5376\n",
      "Epoch 17 Batch 550 Loss 0.3512 Accuracy 0.5371\n",
      "Epoch 17 Batch 600 Loss 0.3541 Accuracy 0.5369\n",
      "Epoch 17 Batch 650 Loss 0.3563 Accuracy 0.5367\n",
      "Epoch 17 Batch 700 Loss 0.3581 Accuracy 0.5366\n",
      "Epoch 17 Batch 750 Loss 0.3587 Accuracy 0.5367\n",
      "Epoch 17 Batch 800 Loss 0.3608 Accuracy 0.5365\n",
      "Epoch 17 Batch 850 Loss 0.3627 Accuracy 0.5364\n",
      "Epoch 17 Batch 900 Loss 0.3652 Accuracy 0.5363\n",
      "Epoch 17 Batch 950 Loss 0.3663 Accuracy 0.5364\n",
      "Epoch 17 Batch 1000 Loss 0.3687 Accuracy 0.5362\n",
      "Epoch 17 Batch 1050 Loss 0.3700 Accuracy 0.5360\n",
      "Epoch 17 Batch 1100 Loss 0.3718 Accuracy 0.5357\n",
      "Epoch 17 Batch 1150 Loss 0.3736 Accuracy 0.5356\n",
      "Epoch 17 Batch 1200 Loss 0.3746 Accuracy 0.5355\n",
      "Epoch 17 Loss 0.3760 Accuracy 0.5354\n",
      "=================================================================\n",
      "Epoch 18 Batch 0 Loss 0.3353 Accuracy 0.5312\n",
      "Epoch 18 Batch 50 Loss 0.2926 Accuracy 0.5443\n",
      "Epoch 18 Batch 100 Loss 0.3066 Accuracy 0.5432\n",
      "Epoch 18 Batch 150 Loss 0.3110 Accuracy 0.5428\n",
      "Epoch 18 Batch 200 Loss 0.3233 Accuracy 0.5414\n",
      "Epoch 18 Batch 250 Loss 0.3261 Accuracy 0.5406\n",
      "Epoch 18 Batch 300 Loss 0.3281 Accuracy 0.5400\n",
      "Epoch 18 Batch 350 Loss 0.3308 Accuracy 0.5401\n",
      "Epoch 18 Batch 400 Loss 0.3348 Accuracy 0.5394\n",
      "Epoch 18 Batch 450 Loss 0.3365 Accuracy 0.5392\n",
      "Epoch 18 Batch 500 Loss 0.3401 Accuracy 0.5387\n",
      "Epoch 18 Batch 550 Loss 0.3429 Accuracy 0.5384\n",
      "Epoch 18 Batch 600 Loss 0.3448 Accuracy 0.5382\n",
      "Epoch 18 Batch 650 Loss 0.3480 Accuracy 0.5379\n",
      "Epoch 18 Batch 700 Loss 0.3496 Accuracy 0.5377\n",
      "Epoch 18 Batch 750 Loss 0.3500 Accuracy 0.5377\n",
      "Epoch 18 Batch 800 Loss 0.3518 Accuracy 0.5376\n",
      "Epoch 18 Batch 850 Loss 0.3536 Accuracy 0.5375\n",
      "Epoch 18 Batch 900 Loss 0.3554 Accuracy 0.5373\n",
      "Epoch 18 Batch 950 Loss 0.3579 Accuracy 0.5372\n",
      "Epoch 18 Batch 1000 Loss 0.3591 Accuracy 0.5371\n",
      "Epoch 18 Batch 1050 Loss 0.3605 Accuracy 0.5370\n",
      "Epoch 18 Batch 1100 Loss 0.3623 Accuracy 0.5369\n",
      "Epoch 18 Batch 1150 Loss 0.3643 Accuracy 0.5368\n",
      "Epoch 18 Batch 1200 Loss 0.3666 Accuracy 0.5367\n",
      "Epoch 18 Loss 0.3679 Accuracy 0.5365\n",
      "=================================================================\n",
      "Epoch 19 Batch 0 Loss 0.1832 Accuracy 0.5547\n",
      "Epoch 19 Batch 50 Loss 0.3207 Accuracy 0.5446\n",
      "Epoch 19 Batch 100 Loss 0.3220 Accuracy 0.5429\n",
      "Epoch 19 Batch 150 Loss 0.3228 Accuracy 0.5421\n",
      "Epoch 19 Batch 200 Loss 0.3211 Accuracy 0.5419\n",
      "Epoch 19 Batch 250 Loss 0.3209 Accuracy 0.5417\n",
      "Epoch 19 Batch 300 Loss 0.3202 Accuracy 0.5421\n",
      "Epoch 19 Batch 350 Loss 0.3257 Accuracy 0.5411\n",
      "Epoch 19 Batch 400 Loss 0.3258 Accuracy 0.5409\n",
      "Epoch 19 Batch 450 Loss 0.3287 Accuracy 0.5403\n",
      "Epoch 19 Batch 500 Loss 0.3329 Accuracy 0.5400\n",
      "Epoch 19 Batch 550 Loss 0.3360 Accuracy 0.5398\n",
      "Epoch 19 Batch 600 Loss 0.3364 Accuracy 0.5398\n",
      "Epoch 19 Batch 650 Loss 0.3389 Accuracy 0.5396\n",
      "Epoch 19 Batch 700 Loss 0.3416 Accuracy 0.5393\n",
      "Epoch 19 Batch 750 Loss 0.3429 Accuracy 0.5392\n",
      "Epoch 19 Batch 800 Loss 0.3454 Accuracy 0.5390\n",
      "Epoch 19 Batch 850 Loss 0.3482 Accuracy 0.5388\n",
      "Epoch 19 Batch 900 Loss 0.3499 Accuracy 0.5386\n",
      "Epoch 19 Batch 950 Loss 0.3507 Accuracy 0.5385\n",
      "Epoch 19 Batch 1000 Loss 0.3515 Accuracy 0.5384\n",
      "Epoch 19 Batch 1050 Loss 0.3535 Accuracy 0.5381\n",
      "Epoch 19 Batch 1100 Loss 0.3552 Accuracy 0.5380\n",
      "Epoch 19 Batch 1150 Loss 0.3563 Accuracy 0.5379\n",
      "Epoch 19 Batch 1200 Loss 0.3578 Accuracy 0.5378\n",
      "Epoch 19 Loss 0.3593 Accuracy 0.5378\n",
      "=================================================================\n",
      "Epoch 20 Batch 0 Loss 0.3069 Accuracy 0.5484\n",
      "Epoch 20 Batch 50 Loss 0.2952 Accuracy 0.5441\n",
      "Epoch 20 Batch 100 Loss 0.3066 Accuracy 0.5428\n",
      "Epoch 20 Batch 150 Loss 0.3192 Accuracy 0.5414\n",
      "Epoch 20 Batch 200 Loss 0.3208 Accuracy 0.5412\n",
      "Epoch 20 Batch 250 Loss 0.3217 Accuracy 0.5409\n",
      "Epoch 20 Batch 300 Loss 0.3238 Accuracy 0.5407\n",
      "Epoch 20 Batch 350 Loss 0.3216 Accuracy 0.5405\n",
      "Epoch 20 Batch 400 Loss 0.3240 Accuracy 0.5407\n",
      "Epoch 20 Batch 450 Loss 0.3256 Accuracy 0.5406\n",
      "Epoch 20 Batch 500 Loss 0.3294 Accuracy 0.5399\n",
      "Epoch 20 Batch 550 Loss 0.3303 Accuracy 0.5397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 600 Loss 0.3336 Accuracy 0.5396\n",
      "Epoch 20 Batch 650 Loss 0.3353 Accuracy 0.5396\n",
      "Epoch 20 Batch 700 Loss 0.3366 Accuracy 0.5394\n",
      "Epoch 20 Batch 750 Loss 0.3390 Accuracy 0.5392\n",
      "Epoch 20 Batch 800 Loss 0.3410 Accuracy 0.5392\n",
      "Epoch 20 Batch 850 Loss 0.3412 Accuracy 0.5394\n",
      "Epoch 20 Batch 900 Loss 0.3428 Accuracy 0.5393\n",
      "Epoch 20 Batch 950 Loss 0.3443 Accuracy 0.5391\n",
      "Epoch 20 Batch 1000 Loss 0.3460 Accuracy 0.5390\n",
      "Epoch 20 Batch 1050 Loss 0.3472 Accuracy 0.5388\n",
      "Epoch 20 Batch 1100 Loss 0.3485 Accuracy 0.5386\n",
      "Epoch 20 Batch 1150 Loss 0.3495 Accuracy 0.5385\n",
      "Epoch 20 Batch 1200 Loss 0.3515 Accuracy 0.5384\n",
      "Epoch 20 Loss 0.3534 Accuracy 0.5383\n",
      "=================================================================\n",
      "Epoch 21 Batch 0 Loss 0.3302 Accuracy 0.5609\n",
      "Epoch 21 Batch 50 Loss 0.2921 Accuracy 0.5443\n",
      "Epoch 21 Batch 100 Loss 0.3023 Accuracy 0.5447\n",
      "Epoch 21 Batch 150 Loss 0.3028 Accuracy 0.5439\n",
      "Epoch 21 Batch 200 Loss 0.3045 Accuracy 0.5437\n",
      "Epoch 21 Batch 250 Loss 0.3074 Accuracy 0.5433\n",
      "Epoch 21 Batch 300 Loss 0.3104 Accuracy 0.5428\n",
      "Epoch 21 Batch 350 Loss 0.3128 Accuracy 0.5428\n",
      "Epoch 21 Batch 400 Loss 0.3127 Accuracy 0.5425\n",
      "Epoch 21 Batch 450 Loss 0.3142 Accuracy 0.5426\n",
      "Epoch 21 Batch 500 Loss 0.3170 Accuracy 0.5422\n",
      "Epoch 21 Batch 550 Loss 0.3218 Accuracy 0.5418\n",
      "Epoch 21 Batch 600 Loss 0.3246 Accuracy 0.5412\n",
      "Epoch 21 Batch 650 Loss 0.3270 Accuracy 0.5411\n",
      "Epoch 21 Batch 700 Loss 0.3296 Accuracy 0.5407\n",
      "Epoch 21 Batch 750 Loss 0.3310 Accuracy 0.5407\n",
      "Epoch 21 Batch 800 Loss 0.3324 Accuracy 0.5405\n",
      "Epoch 21 Batch 850 Loss 0.3349 Accuracy 0.5402\n",
      "Epoch 21 Batch 900 Loss 0.3373 Accuracy 0.5400\n",
      "Epoch 21 Batch 950 Loss 0.3396 Accuracy 0.5397\n",
      "Epoch 21 Batch 1000 Loss 0.3417 Accuracy 0.5397\n",
      "Epoch 21 Batch 1050 Loss 0.3431 Accuracy 0.5396\n",
      "Epoch 21 Batch 1100 Loss 0.3435 Accuracy 0.5395\n",
      "Epoch 21 Batch 1150 Loss 0.3442 Accuracy 0.5396\n",
      "Epoch 21 Batch 1200 Loss 0.3462 Accuracy 0.5394\n",
      "Epoch 21 Loss 0.3475 Accuracy 0.5394\n",
      "=================================================================\n",
      "Epoch 22 Batch 0 Loss 0.2698 Accuracy 0.5547\n",
      "Epoch 22 Batch 50 Loss 0.2875 Accuracy 0.5458\n",
      "Epoch 22 Batch 100 Loss 0.2950 Accuracy 0.5440\n",
      "Epoch 22 Batch 150 Loss 0.2998 Accuracy 0.5429\n",
      "Epoch 22 Batch 200 Loss 0.3021 Accuracy 0.5424\n",
      "Epoch 22 Batch 250 Loss 0.3080 Accuracy 0.5423\n",
      "Epoch 22 Batch 300 Loss 0.3097 Accuracy 0.5424\n",
      "Epoch 22 Batch 350 Loss 0.3117 Accuracy 0.5419\n",
      "Epoch 22 Batch 400 Loss 0.3148 Accuracy 0.5414\n",
      "Epoch 22 Batch 450 Loss 0.3160 Accuracy 0.5414\n",
      "Epoch 22 Batch 500 Loss 0.3201 Accuracy 0.5411\n",
      "Epoch 22 Batch 550 Loss 0.3195 Accuracy 0.5413\n",
      "Epoch 22 Batch 600 Loss 0.3225 Accuracy 0.5410\n",
      "Epoch 22 Batch 650 Loss 0.3238 Accuracy 0.5411\n",
      "Epoch 22 Batch 700 Loss 0.3267 Accuracy 0.5409\n",
      "Epoch 22 Batch 750 Loss 0.3285 Accuracy 0.5408\n",
      "Epoch 22 Batch 800 Loss 0.3305 Accuracy 0.5407\n",
      "Epoch 22 Batch 850 Loss 0.3325 Accuracy 0.5407\n",
      "Epoch 22 Batch 900 Loss 0.3346 Accuracy 0.5406\n",
      "Epoch 22 Batch 950 Loss 0.3354 Accuracy 0.5405\n",
      "Epoch 22 Batch 1000 Loss 0.3366 Accuracy 0.5404\n",
      "Epoch 22 Batch 1050 Loss 0.3378 Accuracy 0.5403\n",
      "Epoch 22 Batch 1100 Loss 0.3390 Accuracy 0.5403\n",
      "Epoch 22 Batch 1150 Loss 0.3406 Accuracy 0.5401\n",
      "Epoch 22 Batch 1200 Loss 0.3427 Accuracy 0.5399\n",
      "Epoch 22 Loss 0.3444 Accuracy 0.5398\n",
      "=================================================================\n",
      "Epoch 23 Batch 0 Loss 0.2552 Accuracy 0.5500\n",
      "Epoch 23 Batch 50 Loss 0.2874 Accuracy 0.5472\n",
      "Epoch 23 Batch 100 Loss 0.2955 Accuracy 0.5462\n",
      "Epoch 23 Batch 150 Loss 0.2998 Accuracy 0.5457\n",
      "Epoch 23 Batch 200 Loss 0.3019 Accuracy 0.5451\n",
      "Epoch 23 Batch 250 Loss 0.3047 Accuracy 0.5444\n",
      "Epoch 23 Batch 300 Loss 0.3079 Accuracy 0.5441\n",
      "Epoch 23 Batch 350 Loss 0.3089 Accuracy 0.5436\n",
      "Epoch 23 Batch 400 Loss 0.3107 Accuracy 0.5430\n",
      "Epoch 23 Batch 450 Loss 0.3117 Accuracy 0.5429\n",
      "Epoch 23 Batch 500 Loss 0.3137 Accuracy 0.5423\n",
      "Epoch 23 Batch 550 Loss 0.3163 Accuracy 0.5419\n",
      "Epoch 23 Batch 600 Loss 0.3193 Accuracy 0.5414\n",
      "Epoch 23 Batch 650 Loss 0.3227 Accuracy 0.5411\n",
      "Epoch 23 Batch 700 Loss 0.3241 Accuracy 0.5411\n",
      "Epoch 23 Batch 750 Loss 0.3264 Accuracy 0.5411\n",
      "Epoch 23 Batch 800 Loss 0.3275 Accuracy 0.5411\n",
      "Epoch 23 Batch 850 Loss 0.3280 Accuracy 0.5409\n",
      "Epoch 23 Batch 900 Loss 0.3301 Accuracy 0.5410\n",
      "Epoch 23 Batch 950 Loss 0.3314 Accuracy 0.5409\n",
      "Epoch 23 Batch 1000 Loss 0.3327 Accuracy 0.5408\n",
      "Epoch 23 Batch 1050 Loss 0.3344 Accuracy 0.5407\n",
      "Epoch 23 Batch 1100 Loss 0.3364 Accuracy 0.5405\n",
      "Epoch 23 Batch 1150 Loss 0.3374 Accuracy 0.5404\n",
      "Epoch 23 Batch 1200 Loss 0.3385 Accuracy 0.5404\n",
      "Epoch 23 Loss 0.3401 Accuracy 0.5403\n",
      "=================================================================\n",
      "Epoch 24 Batch 0 Loss 0.2830 Accuracy 0.5562\n",
      "Epoch 24 Batch 50 Loss 0.2727 Accuracy 0.5464\n",
      "Epoch 24 Batch 100 Loss 0.2776 Accuracy 0.5458\n",
      "Epoch 24 Batch 150 Loss 0.2809 Accuracy 0.5452\n",
      "Epoch 24 Batch 200 Loss 0.2856 Accuracy 0.5451\n",
      "Epoch 24 Batch 250 Loss 0.2887 Accuracy 0.5447\n",
      "Epoch 24 Batch 300 Loss 0.2937 Accuracy 0.5446\n",
      "Epoch 24 Batch 350 Loss 0.2959 Accuracy 0.5446\n",
      "Epoch 24 Batch 400 Loss 0.2987 Accuracy 0.5444\n",
      "Epoch 24 Batch 450 Loss 0.3020 Accuracy 0.5438\n",
      "Epoch 24 Batch 500 Loss 0.3033 Accuracy 0.5436\n",
      "Epoch 24 Batch 550 Loss 0.3060 Accuracy 0.5435\n",
      "Epoch 24 Batch 600 Loss 0.3081 Accuracy 0.5432\n",
      "Epoch 24 Batch 650 Loss 0.3107 Accuracy 0.5429\n",
      "Epoch 24 Batch 700 Loss 0.3126 Accuracy 0.5430\n",
      "Epoch 24 Batch 750 Loss 0.3152 Accuracy 0.5429\n",
      "Epoch 24 Batch 800 Loss 0.3169 Accuracy 0.5426\n",
      "Epoch 24 Batch 850 Loss 0.3200 Accuracy 0.5423\n",
      "Epoch 24 Batch 900 Loss 0.3213 Accuracy 0.5422\n",
      "Epoch 24 Batch 950 Loss 0.3226 Accuracy 0.5421\n",
      "Epoch 24 Batch 1000 Loss 0.3236 Accuracy 0.5422\n",
      "Epoch 24 Batch 1050 Loss 0.3257 Accuracy 0.5419\n",
      "Epoch 24 Batch 1100 Loss 0.3268 Accuracy 0.5419\n",
      "Epoch 24 Batch 1150 Loss 0.3293 Accuracy 0.5416\n",
      "Epoch 24 Batch 1200 Loss 0.3315 Accuracy 0.5414\n",
      "Epoch 24 Loss 0.3328 Accuracy 0.5414\n",
      "=================================================================\n",
      "Epoch 25 Batch 0 Loss 0.3000 Accuracy 0.5547\n",
      "Epoch 25 Batch 50 Loss 0.2836 Accuracy 0.5471\n",
      "Epoch 25 Batch 100 Loss 0.2766 Accuracy 0.5462\n",
      "Epoch 25 Batch 150 Loss 0.2820 Accuracy 0.5459\n",
      "Epoch 25 Batch 200 Loss 0.2866 Accuracy 0.5455\n",
      "Epoch 25 Batch 250 Loss 0.2891 Accuracy 0.5456\n",
      "Epoch 25 Batch 300 Loss 0.2920 Accuracy 0.5446\n",
      "Epoch 25 Batch 350 Loss 0.2938 Accuracy 0.5442\n",
      "Epoch 25 Batch 400 Loss 0.2960 Accuracy 0.5440\n",
      "Epoch 25 Batch 450 Loss 0.2964 Accuracy 0.5442\n",
      "Epoch 25 Batch 500 Loss 0.2981 Accuracy 0.5441\n",
      "Epoch 25 Batch 550 Loss 0.3003 Accuracy 0.5437\n",
      "Epoch 25 Batch 600 Loss 0.3047 Accuracy 0.5435\n",
      "Epoch 25 Batch 650 Loss 0.3085 Accuracy 0.5432\n",
      "Epoch 25 Batch 700 Loss 0.3089 Accuracy 0.5433\n",
      "Epoch 25 Batch 750 Loss 0.3106 Accuracy 0.5431\n",
      "Epoch 25 Batch 800 Loss 0.3126 Accuracy 0.5430\n",
      "Epoch 25 Batch 850 Loss 0.3143 Accuracy 0.5429\n",
      "Epoch 25 Batch 900 Loss 0.3169 Accuracy 0.5426\n",
      "Epoch 25 Batch 950 Loss 0.3185 Accuracy 0.5424\n",
      "Epoch 25 Batch 1000 Loss 0.3207 Accuracy 0.5423\n",
      "Epoch 25 Batch 1050 Loss 0.3223 Accuracy 0.5421\n",
      "Epoch 25 Batch 1100 Loss 0.3228 Accuracy 0.5420\n",
      "Epoch 25 Batch 1150 Loss 0.3245 Accuracy 0.5419\n",
      "Epoch 25 Batch 1200 Loss 0.3266 Accuracy 0.5418\n",
      "Epoch 25 Loss 0.3283 Accuracy 0.5417\n",
      "=================================================================\n",
      "Epoch 26 Batch 0 Loss 0.1900 Accuracy 0.5562\n",
      "Epoch 26 Batch 50 Loss 0.2791 Accuracy 0.5433\n",
      "Epoch 26 Batch 100 Loss 0.2867 Accuracy 0.5444\n",
      "Epoch 26 Batch 150 Loss 0.2885 Accuracy 0.5456\n",
      "Epoch 26 Batch 200 Loss 0.2890 Accuracy 0.5455\n",
      "Epoch 26 Batch 250 Loss 0.2921 Accuracy 0.5452\n",
      "Epoch 26 Batch 300 Loss 0.2949 Accuracy 0.5451\n",
      "Epoch 26 Batch 350 Loss 0.2968 Accuracy 0.5448\n",
      "Epoch 26 Batch 400 Loss 0.2996 Accuracy 0.5445\n",
      "Epoch 26 Batch 450 Loss 0.3017 Accuracy 0.5443\n",
      "Epoch 26 Batch 500 Loss 0.3040 Accuracy 0.5442\n",
      "Epoch 26 Batch 550 Loss 0.3060 Accuracy 0.5438\n",
      "Epoch 26 Batch 600 Loss 0.3076 Accuracy 0.5436\n",
      "Epoch 26 Batch 650 Loss 0.3086 Accuracy 0.5435\n",
      "Epoch 26 Batch 700 Loss 0.3100 Accuracy 0.5436\n",
      "Epoch 26 Batch 750 Loss 0.3105 Accuracy 0.5435\n",
      "Epoch 26 Batch 800 Loss 0.3114 Accuracy 0.5433\n",
      "Epoch 26 Batch 850 Loss 0.3131 Accuracy 0.5432\n",
      "Epoch 26 Batch 900 Loss 0.3140 Accuracy 0.5430\n",
      "Epoch 26 Batch 950 Loss 0.3151 Accuracy 0.5431\n",
      "Epoch 26 Batch 1000 Loss 0.3167 Accuracy 0.5430\n",
      "Epoch 26 Batch 1050 Loss 0.3181 Accuracy 0.5430\n",
      "Epoch 26 Batch 1100 Loss 0.3204 Accuracy 0.5428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 1150 Loss 0.3216 Accuracy 0.5428\n",
      "Epoch 26 Batch 1200 Loss 0.3222 Accuracy 0.5426\n",
      "Epoch 26 Loss 0.3250 Accuracy 0.5423\n",
      "=================================================================\n",
      "Epoch 27 Batch 0 Loss 0.2803 Accuracy 0.5609\n",
      "Epoch 27 Batch 50 Loss 0.2531 Accuracy 0.5502\n",
      "Epoch 27 Batch 100 Loss 0.2700 Accuracy 0.5486\n",
      "Epoch 27 Batch 150 Loss 0.2763 Accuracy 0.5466\n",
      "Epoch 27 Batch 200 Loss 0.2735 Accuracy 0.5475\n",
      "Epoch 27 Batch 250 Loss 0.2805 Accuracy 0.5466\n",
      "Epoch 27 Batch 300 Loss 0.2843 Accuracy 0.5463\n",
      "Epoch 27 Batch 350 Loss 0.2884 Accuracy 0.5455\n",
      "Epoch 27 Batch 400 Loss 0.2891 Accuracy 0.5456\n",
      "Epoch 27 Batch 450 Loss 0.2917 Accuracy 0.5454\n",
      "Epoch 27 Batch 500 Loss 0.2954 Accuracy 0.5448\n",
      "Epoch 27 Batch 550 Loss 0.2968 Accuracy 0.5448\n",
      "Epoch 27 Batch 600 Loss 0.2977 Accuracy 0.5444\n",
      "Epoch 27 Batch 650 Loss 0.2980 Accuracy 0.5446\n",
      "Epoch 27 Batch 700 Loss 0.2998 Accuracy 0.5444\n",
      "Epoch 27 Batch 750 Loss 0.3012 Accuracy 0.5442\n",
      "Epoch 27 Batch 800 Loss 0.3038 Accuracy 0.5440\n",
      "Epoch 27 Batch 850 Loss 0.3063 Accuracy 0.5438\n",
      "Epoch 27 Batch 900 Loss 0.3083 Accuracy 0.5436\n",
      "Epoch 27 Batch 950 Loss 0.3102 Accuracy 0.5435\n",
      "Epoch 27 Batch 1000 Loss 0.3117 Accuracy 0.5434\n",
      "Epoch 27 Batch 1050 Loss 0.3132 Accuracy 0.5435\n",
      "Epoch 27 Batch 1100 Loss 0.3159 Accuracy 0.5432\n",
      "Epoch 27 Batch 1150 Loss 0.3169 Accuracy 0.5431\n",
      "Epoch 27 Batch 1200 Loss 0.3185 Accuracy 0.5429\n",
      "Epoch 27 Loss 0.3199 Accuracy 0.5428\n",
      "=================================================================\n",
      "Epoch 28 Batch 0 Loss 0.3438 Accuracy 0.5125\n",
      "Epoch 28 Batch 50 Loss 0.2703 Accuracy 0.5495\n",
      "Epoch 28 Batch 100 Loss 0.2704 Accuracy 0.5488\n",
      "Epoch 28 Batch 150 Loss 0.2759 Accuracy 0.5481\n",
      "Epoch 28 Batch 200 Loss 0.2762 Accuracy 0.5471\n",
      "Epoch 28 Batch 250 Loss 0.2811 Accuracy 0.5463\n",
      "Epoch 28 Batch 300 Loss 0.2840 Accuracy 0.5458\n",
      "Epoch 28 Batch 350 Loss 0.2833 Accuracy 0.5461\n",
      "Epoch 28 Batch 400 Loss 0.2880 Accuracy 0.5457\n",
      "Epoch 28 Batch 450 Loss 0.2888 Accuracy 0.5457\n",
      "Epoch 28 Batch 500 Loss 0.2909 Accuracy 0.5454\n",
      "Epoch 28 Batch 550 Loss 0.2918 Accuracy 0.5453\n",
      "Epoch 28 Batch 600 Loss 0.2959 Accuracy 0.5448\n",
      "Epoch 28 Batch 650 Loss 0.2975 Accuracy 0.5447\n",
      "Epoch 28 Batch 700 Loss 0.2995 Accuracy 0.5446\n",
      "Epoch 28 Batch 750 Loss 0.3017 Accuracy 0.5445\n",
      "Epoch 28 Batch 800 Loss 0.3051 Accuracy 0.5442\n",
      "Epoch 28 Batch 850 Loss 0.3075 Accuracy 0.5440\n",
      "Epoch 28 Batch 900 Loss 0.3089 Accuracy 0.5439\n",
      "Epoch 28 Batch 950 Loss 0.3117 Accuracy 0.5437\n",
      "Epoch 28 Batch 1000 Loss 0.3135 Accuracy 0.5435\n",
      "Epoch 28 Batch 1050 Loss 0.3146 Accuracy 0.5435\n",
      "Epoch 28 Batch 1100 Loss 0.3160 Accuracy 0.5434\n",
      "Epoch 28 Batch 1150 Loss 0.3183 Accuracy 0.5432\n",
      "Epoch 28 Batch 1200 Loss 0.3187 Accuracy 0.5432\n",
      "Epoch 28 Loss 0.3193 Accuracy 0.5431\n",
      "=================================================================\n",
      "Epoch 29 Batch 0 Loss 0.2955 Accuracy 0.5422\n",
      "Epoch 29 Batch 50 Loss 0.2569 Accuracy 0.5466\n",
      "Epoch 29 Batch 100 Loss 0.2666 Accuracy 0.5475\n",
      "Epoch 29 Batch 150 Loss 0.2678 Accuracy 0.5478\n",
      "Epoch 29 Batch 200 Loss 0.2750 Accuracy 0.5470\n",
      "Epoch 29 Batch 250 Loss 0.2794 Accuracy 0.5465\n",
      "Epoch 29 Batch 300 Loss 0.2813 Accuracy 0.5459\n",
      "Epoch 29 Batch 350 Loss 0.2856 Accuracy 0.5455\n",
      "Epoch 29 Batch 400 Loss 0.2884 Accuracy 0.5453\n",
      "Epoch 29 Batch 450 Loss 0.2897 Accuracy 0.5450\n",
      "Epoch 29 Batch 500 Loss 0.2934 Accuracy 0.5449\n",
      "Epoch 29 Batch 550 Loss 0.2962 Accuracy 0.5446\n",
      "Epoch 29 Batch 600 Loss 0.2989 Accuracy 0.5443\n",
      "Epoch 29 Batch 650 Loss 0.3005 Accuracy 0.5442\n",
      "Epoch 29 Batch 700 Loss 0.3014 Accuracy 0.5442\n",
      "Epoch 29 Batch 750 Loss 0.3031 Accuracy 0.5442\n",
      "Epoch 29 Batch 800 Loss 0.3047 Accuracy 0.5441\n",
      "Epoch 29 Batch 850 Loss 0.3076 Accuracy 0.5439\n",
      "Epoch 29 Batch 900 Loss 0.3088 Accuracy 0.5440\n",
      "Epoch 29 Batch 950 Loss 0.3097 Accuracy 0.5438\n",
      "Epoch 29 Batch 1000 Loss 0.3105 Accuracy 0.5438\n",
      "Epoch 29 Batch 1050 Loss 0.3120 Accuracy 0.5436\n",
      "Epoch 29 Batch 1100 Loss 0.3133 Accuracy 0.5436\n",
      "Epoch 29 Batch 1150 Loss 0.3142 Accuracy 0.5437\n",
      "Epoch 29 Batch 1200 Loss 0.3159 Accuracy 0.5435\n",
      "Epoch 29 Loss 0.3168 Accuracy 0.5435\n",
      "=================================================================\n",
      "Epoch 30 Batch 0 Loss 0.2085 Accuracy 0.5641\n",
      "Epoch 30 Batch 50 Loss 0.2623 Accuracy 0.5485\n",
      "Epoch 30 Batch 100 Loss 0.2660 Accuracy 0.5466\n",
      "Epoch 30 Batch 150 Loss 0.2695 Accuracy 0.5462\n",
      "Epoch 30 Batch 200 Loss 0.2706 Accuracy 0.5462\n",
      "Epoch 30 Batch 250 Loss 0.2734 Accuracy 0.5459\n",
      "Epoch 30 Batch 300 Loss 0.2773 Accuracy 0.5457\n",
      "Epoch 30 Batch 350 Loss 0.2790 Accuracy 0.5458\n",
      "Epoch 30 Batch 400 Loss 0.2818 Accuracy 0.5457\n",
      "Epoch 30 Batch 450 Loss 0.2838 Accuracy 0.5457\n",
      "Epoch 30 Batch 500 Loss 0.2853 Accuracy 0.5455\n",
      "Epoch 30 Batch 550 Loss 0.2886 Accuracy 0.5452\n",
      "Epoch 30 Batch 600 Loss 0.2900 Accuracy 0.5454\n",
      "Epoch 30 Batch 650 Loss 0.2927 Accuracy 0.5453\n",
      "Epoch 30 Batch 700 Loss 0.2946 Accuracy 0.5451\n",
      "Epoch 30 Batch 750 Loss 0.2959 Accuracy 0.5452\n",
      "Epoch 30 Batch 800 Loss 0.2975 Accuracy 0.5449\n",
      "Epoch 30 Batch 850 Loss 0.2993 Accuracy 0.5447\n",
      "Epoch 30 Batch 900 Loss 0.2999 Accuracy 0.5447\n",
      "Epoch 30 Batch 950 Loss 0.3014 Accuracy 0.5446\n",
      "Epoch 30 Batch 1000 Loss 0.3037 Accuracy 0.5445\n",
      "Epoch 30 Batch 1050 Loss 0.3060 Accuracy 0.5443\n",
      "Epoch 30 Batch 1100 Loss 0.3079 Accuracy 0.5442\n",
      "Epoch 30 Batch 1150 Loss 0.3098 Accuracy 0.5443\n",
      "Epoch 30 Batch 1200 Loss 0.3117 Accuracy 0.5441\n",
      "Epoch 30 Loss 0.3134 Accuracy 0.5438\n",
      "=================================================================\n",
      "Epoch 31 Batch 0 Loss 0.2386 Accuracy 0.5391\n",
      "Epoch 31 Batch 50 Loss 0.2487 Accuracy 0.5509\n",
      "Epoch 31 Batch 100 Loss 0.2547 Accuracy 0.5504\n",
      "Epoch 31 Batch 150 Loss 0.2581 Accuracy 0.5494\n",
      "Epoch 31 Batch 200 Loss 0.2635 Accuracy 0.5483\n",
      "Epoch 31 Batch 250 Loss 0.2692 Accuracy 0.5478\n",
      "Epoch 31 Batch 300 Loss 0.2728 Accuracy 0.5473\n",
      "Epoch 31 Batch 350 Loss 0.2766 Accuracy 0.5470\n",
      "Epoch 31 Batch 400 Loss 0.2799 Accuracy 0.5465\n",
      "Epoch 31 Batch 450 Loss 0.2833 Accuracy 0.5462\n",
      "Epoch 31 Batch 500 Loss 0.2847 Accuracy 0.5462\n",
      "Epoch 31 Batch 550 Loss 0.2865 Accuracy 0.5461\n",
      "Epoch 31 Batch 600 Loss 0.2878 Accuracy 0.5461\n",
      "Epoch 31 Batch 650 Loss 0.2888 Accuracy 0.5460\n",
      "Epoch 31 Batch 700 Loss 0.2920 Accuracy 0.5455\n",
      "Epoch 31 Batch 750 Loss 0.2950 Accuracy 0.5451\n",
      "Epoch 31 Batch 800 Loss 0.2964 Accuracy 0.5451\n",
      "Epoch 31 Batch 850 Loss 0.2992 Accuracy 0.5450\n",
      "Epoch 31 Batch 900 Loss 0.3015 Accuracy 0.5451\n",
      "Epoch 31 Batch 950 Loss 0.3027 Accuracy 0.5449\n",
      "Epoch 31 Batch 1000 Loss 0.3041 Accuracy 0.5448\n",
      "Epoch 31 Batch 1050 Loss 0.3058 Accuracy 0.5446\n",
      "Epoch 31 Batch 1100 Loss 0.3065 Accuracy 0.5445\n",
      "Epoch 31 Batch 1150 Loss 0.3082 Accuracy 0.5444\n",
      "Epoch 31 Batch 1200 Loss 0.3099 Accuracy 0.5442\n",
      "Epoch 31 Loss 0.3105 Accuracy 0.5443\n",
      "=================================================================\n",
      "Epoch 32 Batch 0 Loss 0.1758 Accuracy 0.5375\n",
      "Epoch 32 Batch 50 Loss 0.2442 Accuracy 0.5479\n",
      "Epoch 32 Batch 100 Loss 0.2427 Accuracy 0.5489\n",
      "Epoch 32 Batch 150 Loss 0.2518 Accuracy 0.5481\n",
      "Epoch 32 Batch 200 Loss 0.2524 Accuracy 0.5484\n",
      "Epoch 32 Batch 250 Loss 0.2581 Accuracy 0.5476\n",
      "Epoch 32 Batch 300 Loss 0.2646 Accuracy 0.5474\n",
      "Epoch 32 Batch 350 Loss 0.2674 Accuracy 0.5473\n",
      "Epoch 32 Batch 400 Loss 0.2701 Accuracy 0.5473\n",
      "Epoch 32 Batch 450 Loss 0.2718 Accuracy 0.5469\n",
      "Epoch 32 Batch 500 Loss 0.2740 Accuracy 0.5468\n",
      "Epoch 32 Batch 550 Loss 0.2773 Accuracy 0.5465\n",
      "Epoch 32 Batch 600 Loss 0.2816 Accuracy 0.5463\n",
      "Epoch 32 Batch 650 Loss 0.2844 Accuracy 0.5462\n",
      "Epoch 32 Batch 700 Loss 0.2868 Accuracy 0.5459\n",
      "Epoch 32 Batch 750 Loss 0.2888 Accuracy 0.5458\n",
      "Epoch 32 Batch 800 Loss 0.2906 Accuracy 0.5458\n",
      "Epoch 32 Batch 850 Loss 0.2919 Accuracy 0.5457\n",
      "Epoch 32 Batch 900 Loss 0.2940 Accuracy 0.5456\n",
      "Epoch 32 Batch 950 Loss 0.2960 Accuracy 0.5455\n",
      "Epoch 32 Batch 1000 Loss 0.2982 Accuracy 0.5455\n",
      "Epoch 32 Batch 1050 Loss 0.3001 Accuracy 0.5454\n",
      "Epoch 32 Batch 1100 Loss 0.3016 Accuracy 0.5451\n",
      "Epoch 32 Batch 1150 Loss 0.3034 Accuracy 0.5450\n",
      "Epoch 32 Batch 1200 Loss 0.3051 Accuracy 0.5449\n",
      "Epoch 32 Loss 0.3073 Accuracy 0.5448\n",
      "=================================================================\n",
      "Epoch 33 Batch 0 Loss 0.2534 Accuracy 0.5484\n",
      "Epoch 33 Batch 50 Loss 0.2442 Accuracy 0.5502\n",
      "Epoch 33 Batch 100 Loss 0.2536 Accuracy 0.5494\n",
      "Epoch 33 Batch 150 Loss 0.2601 Accuracy 0.5486\n",
      "Epoch 33 Batch 200 Loss 0.2635 Accuracy 0.5489\n",
      "Epoch 33 Batch 250 Loss 0.2676 Accuracy 0.5482\n",
      "Epoch 33 Batch 300 Loss 0.2674 Accuracy 0.5488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 350 Loss 0.2704 Accuracy 0.5484\n",
      "Epoch 33 Batch 400 Loss 0.2722 Accuracy 0.5483\n",
      "Epoch 33 Batch 450 Loss 0.2765 Accuracy 0.5474\n",
      "Epoch 33 Batch 500 Loss 0.2790 Accuracy 0.5468\n",
      "Epoch 33 Batch 550 Loss 0.2824 Accuracy 0.5462\n",
      "Epoch 33 Batch 600 Loss 0.2843 Accuracy 0.5463\n",
      "Epoch 33 Batch 650 Loss 0.2848 Accuracy 0.5464\n",
      "Epoch 33 Batch 700 Loss 0.2859 Accuracy 0.5464\n",
      "Epoch 33 Batch 750 Loss 0.2889 Accuracy 0.5462\n",
      "Epoch 33 Batch 800 Loss 0.2905 Accuracy 0.5461\n",
      "Epoch 33 Batch 850 Loss 0.2913 Accuracy 0.5460\n",
      "Epoch 33 Batch 900 Loss 0.2926 Accuracy 0.5458\n",
      "Epoch 33 Batch 950 Loss 0.2945 Accuracy 0.5457\n",
      "Epoch 33 Batch 1000 Loss 0.2970 Accuracy 0.5456\n",
      "Epoch 33 Batch 1050 Loss 0.2984 Accuracy 0.5456\n",
      "Epoch 33 Batch 1100 Loss 0.3002 Accuracy 0.5456\n",
      "Epoch 33 Batch 1150 Loss 0.3013 Accuracy 0.5454\n",
      "Epoch 33 Batch 1200 Loss 0.3026 Accuracy 0.5454\n",
      "Epoch 33 Loss 0.3034 Accuracy 0.5453\n",
      "=================================================================\n",
      "Epoch 34 Batch 0 Loss 0.2674 Accuracy 0.5469\n",
      "Epoch 34 Batch 50 Loss 0.2478 Accuracy 0.5504\n",
      "Epoch 34 Batch 100 Loss 0.2602 Accuracy 0.5491\n",
      "Epoch 34 Batch 150 Loss 0.2645 Accuracy 0.5484\n",
      "Epoch 34 Batch 200 Loss 0.2652 Accuracy 0.5484\n",
      "Epoch 34 Batch 250 Loss 0.2661 Accuracy 0.5484\n",
      "Epoch 34 Batch 300 Loss 0.2679 Accuracy 0.5481\n",
      "Epoch 34 Batch 350 Loss 0.2715 Accuracy 0.5480\n",
      "Epoch 34 Batch 400 Loss 0.2747 Accuracy 0.5478\n",
      "Epoch 34 Batch 450 Loss 0.2746 Accuracy 0.5476\n",
      "Epoch 34 Batch 500 Loss 0.2767 Accuracy 0.5475\n",
      "Epoch 34 Batch 550 Loss 0.2788 Accuracy 0.5473\n",
      "Epoch 34 Batch 600 Loss 0.2802 Accuracy 0.5472\n",
      "Epoch 34 Batch 650 Loss 0.2828 Accuracy 0.5471\n",
      "Epoch 34 Batch 700 Loss 0.2849 Accuracy 0.5470\n",
      "Epoch 34 Batch 750 Loss 0.2858 Accuracy 0.5469\n",
      "Epoch 34 Batch 800 Loss 0.2866 Accuracy 0.5471\n",
      "Epoch 34 Batch 850 Loss 0.2888 Accuracy 0.5467\n",
      "Epoch 34 Batch 900 Loss 0.2909 Accuracy 0.5466\n",
      "Epoch 34 Batch 950 Loss 0.2923 Accuracy 0.5463\n",
      "Epoch 34 Batch 1000 Loss 0.2939 Accuracy 0.5461\n",
      "Epoch 34 Batch 1050 Loss 0.2959 Accuracy 0.5461\n",
      "Epoch 34 Batch 1100 Loss 0.2966 Accuracy 0.5461\n",
      "Epoch 34 Batch 1150 Loss 0.2987 Accuracy 0.5458\n",
      "Epoch 34 Batch 1200 Loss 0.2999 Accuracy 0.5457\n",
      "Epoch 34 Loss 0.3010 Accuracy 0.5456\n",
      "=================================================================\n",
      "Epoch 35 Batch 0 Loss 0.1681 Accuracy 0.5578\n",
      "Epoch 35 Batch 50 Loss 0.2667 Accuracy 0.5488\n",
      "Epoch 35 Batch 100 Loss 0.2587 Accuracy 0.5497\n",
      "Epoch 35 Batch 150 Loss 0.2578 Accuracy 0.5486\n",
      "Epoch 35 Batch 200 Loss 0.2595 Accuracy 0.5485\n",
      "Epoch 35 Batch 250 Loss 0.2626 Accuracy 0.5481\n",
      "Epoch 35 Batch 300 Loss 0.2673 Accuracy 0.5478\n",
      "Epoch 35 Batch 350 Loss 0.2691 Accuracy 0.5477\n",
      "Epoch 35 Batch 400 Loss 0.2722 Accuracy 0.5475\n",
      "Epoch 35 Batch 450 Loss 0.2751 Accuracy 0.5471\n",
      "Epoch 35 Batch 500 Loss 0.2756 Accuracy 0.5473\n",
      "Epoch 35 Batch 550 Loss 0.2759 Accuracy 0.5475\n",
      "Epoch 35 Batch 600 Loss 0.2775 Accuracy 0.5476\n",
      "Epoch 35 Batch 650 Loss 0.2805 Accuracy 0.5473\n",
      "Epoch 35 Batch 700 Loss 0.2825 Accuracy 0.5472\n",
      "Epoch 35 Batch 750 Loss 0.2841 Accuracy 0.5471\n",
      "Epoch 35 Batch 800 Loss 0.2844 Accuracy 0.5471\n",
      "Epoch 35 Batch 850 Loss 0.2871 Accuracy 0.5469\n",
      "Epoch 35 Batch 900 Loss 0.2890 Accuracy 0.5468\n",
      "Epoch 35 Batch 950 Loss 0.2906 Accuracy 0.5466\n",
      "Epoch 35 Batch 1000 Loss 0.2924 Accuracy 0.5464\n",
      "Epoch 35 Batch 1050 Loss 0.2935 Accuracy 0.5463\n",
      "Epoch 35 Batch 1100 Loss 0.2943 Accuracy 0.5463\n",
      "Epoch 35 Batch 1150 Loss 0.2960 Accuracy 0.5461\n",
      "Epoch 35 Batch 1200 Loss 0.2972 Accuracy 0.5459\n",
      "Epoch 35 Loss 0.2986 Accuracy 0.5459\n",
      "=================================================================\n",
      "Epoch 36 Batch 0 Loss 0.3290 Accuracy 0.5516\n",
      "Epoch 36 Batch 50 Loss 0.2483 Accuracy 0.5491\n",
      "Epoch 36 Batch 100 Loss 0.2546 Accuracy 0.5489\n",
      "Epoch 36 Batch 150 Loss 0.2559 Accuracy 0.5494\n",
      "Epoch 36 Batch 200 Loss 0.2596 Accuracy 0.5493\n",
      "Epoch 36 Batch 250 Loss 0.2600 Accuracy 0.5490\n",
      "Epoch 36 Batch 300 Loss 0.2625 Accuracy 0.5486\n",
      "Epoch 36 Batch 350 Loss 0.2662 Accuracy 0.5485\n",
      "Epoch 36 Batch 400 Loss 0.2688 Accuracy 0.5483\n",
      "Epoch 36 Batch 450 Loss 0.2690 Accuracy 0.5487\n",
      "Epoch 36 Batch 500 Loss 0.2709 Accuracy 0.5487\n",
      "Epoch 36 Batch 550 Loss 0.2727 Accuracy 0.5484\n",
      "Epoch 36 Batch 600 Loss 0.2735 Accuracy 0.5482\n",
      "Epoch 36 Batch 650 Loss 0.2762 Accuracy 0.5478\n",
      "Epoch 36 Batch 700 Loss 0.2780 Accuracy 0.5476\n",
      "Epoch 36 Batch 750 Loss 0.2804 Accuracy 0.5473\n",
      "Epoch 36 Batch 800 Loss 0.2820 Accuracy 0.5473\n",
      "Epoch 36 Batch 850 Loss 0.2847 Accuracy 0.5470\n",
      "Epoch 36 Batch 900 Loss 0.2872 Accuracy 0.5468\n",
      "Epoch 36 Batch 950 Loss 0.2896 Accuracy 0.5466\n",
      "Epoch 36 Batch 1000 Loss 0.2917 Accuracy 0.5464\n",
      "Epoch 36 Batch 1050 Loss 0.2931 Accuracy 0.5462\n",
      "Epoch 36 Batch 1100 Loss 0.2942 Accuracy 0.5462\n",
      "Epoch 36 Batch 1150 Loss 0.2953 Accuracy 0.5463\n",
      "Epoch 36 Batch 1200 Loss 0.2966 Accuracy 0.5462\n",
      "Epoch 36 Loss 0.2975 Accuracy 0.5460\n",
      "=================================================================\n",
      "Epoch 37 Batch 0 Loss 0.2271 Accuracy 0.5734\n",
      "Epoch 37 Batch 50 Loss 0.2449 Accuracy 0.5483\n",
      "Epoch 37 Batch 100 Loss 0.2535 Accuracy 0.5489\n",
      "Epoch 37 Batch 150 Loss 0.2548 Accuracy 0.5497\n",
      "Epoch 37 Batch 200 Loss 0.2580 Accuracy 0.5501\n",
      "Epoch 37 Batch 250 Loss 0.2578 Accuracy 0.5491\n",
      "Epoch 37 Batch 300 Loss 0.2617 Accuracy 0.5487\n",
      "Epoch 37 Batch 350 Loss 0.2625 Accuracy 0.5486\n",
      "Epoch 37 Batch 400 Loss 0.2658 Accuracy 0.5485\n",
      "Epoch 37 Batch 450 Loss 0.2683 Accuracy 0.5484\n",
      "Epoch 37 Batch 500 Loss 0.2698 Accuracy 0.5482\n",
      "Epoch 37 Batch 550 Loss 0.2707 Accuracy 0.5485\n",
      "Epoch 37 Batch 600 Loss 0.2728 Accuracy 0.5484\n",
      "Epoch 37 Batch 650 Loss 0.2738 Accuracy 0.5479\n",
      "Epoch 37 Batch 700 Loss 0.2761 Accuracy 0.5477\n",
      "Epoch 37 Batch 750 Loss 0.2797 Accuracy 0.5474\n",
      "Epoch 37 Batch 800 Loss 0.2804 Accuracy 0.5473\n",
      "Epoch 37 Batch 850 Loss 0.2820 Accuracy 0.5473\n",
      "Epoch 37 Batch 900 Loss 0.2846 Accuracy 0.5472\n",
      "Epoch 37 Batch 950 Loss 0.2864 Accuracy 0.5470\n",
      "Epoch 37 Batch 1000 Loss 0.2884 Accuracy 0.5468\n",
      "Epoch 37 Batch 1050 Loss 0.2896 Accuracy 0.5466\n",
      "Epoch 37 Batch 1100 Loss 0.2915 Accuracy 0.5464\n",
      "Epoch 37 Batch 1150 Loss 0.2934 Accuracy 0.5464\n",
      "Epoch 37 Batch 1200 Loss 0.2942 Accuracy 0.5464\n",
      "Epoch 37 Loss 0.2955 Accuracy 0.5462\n",
      "=================================================================\n",
      "Epoch 38 Batch 0 Loss 0.2204 Accuracy 0.5500\n",
      "Epoch 38 Batch 50 Loss 0.2486 Accuracy 0.5529\n",
      "Epoch 38 Batch 100 Loss 0.2460 Accuracy 0.5525\n",
      "Epoch 38 Batch 150 Loss 0.2518 Accuracy 0.5516\n",
      "Epoch 38 Batch 200 Loss 0.2556 Accuracy 0.5507\n",
      "Epoch 38 Batch 250 Loss 0.2568 Accuracy 0.5506\n",
      "Epoch 38 Batch 300 Loss 0.2607 Accuracy 0.5498\n",
      "Epoch 38 Batch 350 Loss 0.2612 Accuracy 0.5500\n",
      "Epoch 38 Batch 400 Loss 0.2652 Accuracy 0.5493\n",
      "Epoch 38 Batch 450 Loss 0.2680 Accuracy 0.5487\n",
      "Epoch 38 Batch 500 Loss 0.2698 Accuracy 0.5486\n",
      "Epoch 38 Batch 550 Loss 0.2732 Accuracy 0.5481\n",
      "Epoch 38 Batch 600 Loss 0.2753 Accuracy 0.5477\n",
      "Epoch 38 Batch 650 Loss 0.2777 Accuracy 0.5474\n",
      "Epoch 38 Batch 700 Loss 0.2806 Accuracy 0.5472\n",
      "Epoch 38 Batch 750 Loss 0.2819 Accuracy 0.5472\n",
      "Epoch 38 Batch 800 Loss 0.2838 Accuracy 0.5470\n",
      "Epoch 38 Batch 850 Loss 0.2844 Accuracy 0.5472\n",
      "Epoch 38 Batch 900 Loss 0.2853 Accuracy 0.5472\n",
      "Epoch 38 Batch 950 Loss 0.2870 Accuracy 0.5472\n",
      "Epoch 38 Batch 1000 Loss 0.2887 Accuracy 0.5471\n",
      "Epoch 38 Batch 1050 Loss 0.2902 Accuracy 0.5470\n",
      "Epoch 38 Batch 1100 Loss 0.2916 Accuracy 0.5469\n",
      "Epoch 38 Batch 1150 Loss 0.2930 Accuracy 0.5469\n",
      "Epoch 38 Batch 1200 Loss 0.2942 Accuracy 0.5469\n",
      "Epoch 38 Loss 0.2947 Accuracy 0.5468\n",
      "=================================================================\n",
      "Epoch 39 Batch 0 Loss 0.1852 Accuracy 0.5734\n",
      "Epoch 39 Batch 50 Loss 0.2369 Accuracy 0.5511\n",
      "Epoch 39 Batch 100 Loss 0.2447 Accuracy 0.5487\n",
      "Epoch 39 Batch 150 Loss 0.2479 Accuracy 0.5489\n",
      "Epoch 39 Batch 200 Loss 0.2543 Accuracy 0.5485\n",
      "Epoch 39 Batch 250 Loss 0.2552 Accuracy 0.5488\n",
      "Epoch 39 Batch 300 Loss 0.2580 Accuracy 0.5490\n",
      "Epoch 39 Batch 350 Loss 0.2628 Accuracy 0.5487\n",
      "Epoch 39 Batch 400 Loss 0.2660 Accuracy 0.5483\n",
      "Epoch 39 Batch 450 Loss 0.2667 Accuracy 0.5480\n",
      "Epoch 39 Batch 500 Loss 0.2695 Accuracy 0.5477\n",
      "Epoch 39 Batch 550 Loss 0.2715 Accuracy 0.5477\n",
      "Epoch 39 Batch 600 Loss 0.2709 Accuracy 0.5479\n",
      "Epoch 39 Batch 650 Loss 0.2711 Accuracy 0.5480\n",
      "Epoch 39 Batch 700 Loss 0.2737 Accuracy 0.5479\n",
      "Epoch 39 Batch 750 Loss 0.2750 Accuracy 0.5479\n",
      "Epoch 39 Batch 800 Loss 0.2772 Accuracy 0.5477\n",
      "Epoch 39 Batch 850 Loss 0.2786 Accuracy 0.5478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 900 Loss 0.2795 Accuracy 0.5475\n",
      "Epoch 39 Batch 950 Loss 0.2817 Accuracy 0.5474\n",
      "Epoch 39 Batch 1000 Loss 0.2833 Accuracy 0.5472\n",
      "Epoch 39 Batch 1050 Loss 0.2842 Accuracy 0.5472\n",
      "Epoch 39 Batch 1100 Loss 0.2858 Accuracy 0.5470\n",
      "Epoch 39 Batch 1150 Loss 0.2873 Accuracy 0.5469\n",
      "Epoch 39 Batch 1200 Loss 0.2886 Accuracy 0.5469\n",
      "Epoch 39 Loss 0.2896 Accuracy 0.5469\n",
      "=================================================================\n",
      "Epoch 40 Batch 0 Loss 0.2769 Accuracy 0.5578\n",
      "Epoch 40 Batch 50 Loss 0.2339 Accuracy 0.5514\n",
      "Epoch 40 Batch 100 Loss 0.2391 Accuracy 0.5513\n",
      "Epoch 40 Batch 150 Loss 0.2452 Accuracy 0.5506\n",
      "Epoch 40 Batch 200 Loss 0.2463 Accuracy 0.5499\n",
      "Epoch 40 Batch 250 Loss 0.2484 Accuracy 0.5507\n",
      "Epoch 40 Batch 300 Loss 0.2518 Accuracy 0.5503\n",
      "Epoch 40 Batch 350 Loss 0.2548 Accuracy 0.5501\n",
      "Epoch 40 Batch 400 Loss 0.2578 Accuracy 0.5498\n",
      "Epoch 40 Batch 450 Loss 0.2602 Accuracy 0.5497\n",
      "Epoch 40 Batch 500 Loss 0.2615 Accuracy 0.5498\n",
      "Epoch 40 Batch 550 Loss 0.2660 Accuracy 0.5493\n",
      "Epoch 40 Batch 600 Loss 0.2676 Accuracy 0.5492\n",
      "Epoch 40 Batch 650 Loss 0.2711 Accuracy 0.5490\n",
      "Epoch 40 Batch 700 Loss 0.2723 Accuracy 0.5487\n",
      "Epoch 40 Batch 750 Loss 0.2746 Accuracy 0.5484\n",
      "Epoch 40 Batch 800 Loss 0.2760 Accuracy 0.5484\n",
      "Epoch 40 Batch 850 Loss 0.2776 Accuracy 0.5483\n",
      "Epoch 40 Batch 900 Loss 0.2791 Accuracy 0.5481\n",
      "Epoch 40 Batch 950 Loss 0.2805 Accuracy 0.5480\n",
      "Epoch 40 Batch 1000 Loss 0.2817 Accuracy 0.5479\n",
      "Epoch 40 Batch 1050 Loss 0.2835 Accuracy 0.5476\n",
      "Epoch 40 Batch 1100 Loss 0.2847 Accuracy 0.5475\n",
      "Epoch 40 Batch 1150 Loss 0.2870 Accuracy 0.5473\n",
      "Epoch 40 Batch 1200 Loss 0.2881 Accuracy 0.5473\n",
      "Epoch 40 Loss 0.2894 Accuracy 0.5471\n",
      "=================================================================\n",
      "Epoch 41 Batch 0 Loss 0.2601 Accuracy 0.5437\n",
      "Epoch 41 Batch 50 Loss 0.2465 Accuracy 0.5525\n",
      "Epoch 41 Batch 100 Loss 0.2479 Accuracy 0.5515\n",
      "Epoch 41 Batch 150 Loss 0.2491 Accuracy 0.5502\n",
      "Epoch 41 Batch 200 Loss 0.2507 Accuracy 0.5500\n",
      "Epoch 41 Batch 250 Loss 0.2567 Accuracy 0.5491\n",
      "Epoch 41 Batch 300 Loss 0.2558 Accuracy 0.5494\n",
      "Epoch 41 Batch 350 Loss 0.2567 Accuracy 0.5494\n",
      "Epoch 41 Batch 400 Loss 0.2578 Accuracy 0.5493\n",
      "Epoch 41 Batch 450 Loss 0.2597 Accuracy 0.5492\n",
      "Epoch 41 Batch 500 Loss 0.2616 Accuracy 0.5492\n",
      "Epoch 41 Batch 550 Loss 0.2634 Accuracy 0.5491\n",
      "Epoch 41 Batch 600 Loss 0.2657 Accuracy 0.5488\n",
      "Epoch 41 Batch 650 Loss 0.2683 Accuracy 0.5487\n",
      "Epoch 41 Batch 700 Loss 0.2712 Accuracy 0.5485\n",
      "Epoch 41 Batch 750 Loss 0.2740 Accuracy 0.5482\n",
      "Epoch 41 Batch 800 Loss 0.2759 Accuracy 0.5481\n",
      "Epoch 41 Batch 850 Loss 0.2779 Accuracy 0.5481\n",
      "Epoch 41 Batch 900 Loss 0.2787 Accuracy 0.5480\n",
      "Epoch 41 Batch 950 Loss 0.2796 Accuracy 0.5478\n",
      "Epoch 41 Batch 1000 Loss 0.2813 Accuracy 0.5478\n",
      "Epoch 41 Batch 1050 Loss 0.2836 Accuracy 0.5476\n",
      "Epoch 41 Batch 1100 Loss 0.2850 Accuracy 0.5475\n",
      "Epoch 41 Batch 1150 Loss 0.2866 Accuracy 0.5475\n",
      "Epoch 41 Batch 1200 Loss 0.2885 Accuracy 0.5474\n",
      "Epoch 41 Loss 0.2899 Accuracy 0.5473\n",
      "=================================================================\n",
      "Epoch 42 Batch 0 Loss 0.2298 Accuracy 0.5516\n",
      "Epoch 42 Batch 50 Loss 0.2549 Accuracy 0.5474\n",
      "Epoch 42 Batch 100 Loss 0.2509 Accuracy 0.5495\n",
      "Epoch 42 Batch 150 Loss 0.2533 Accuracy 0.5500\n",
      "Epoch 42 Batch 200 Loss 0.2538 Accuracy 0.5497\n",
      "Epoch 42 Batch 250 Loss 0.2505 Accuracy 0.5508\n",
      "Epoch 42 Batch 300 Loss 0.2542 Accuracy 0.5508\n",
      "Epoch 42 Batch 350 Loss 0.2562 Accuracy 0.5502\n",
      "Epoch 42 Batch 400 Loss 0.2580 Accuracy 0.5499\n",
      "Epoch 42 Batch 450 Loss 0.2594 Accuracy 0.5493\n",
      "Epoch 42 Batch 500 Loss 0.2622 Accuracy 0.5490\n",
      "Epoch 42 Batch 550 Loss 0.2636 Accuracy 0.5490\n",
      "Epoch 42 Batch 600 Loss 0.2660 Accuracy 0.5490\n",
      "Epoch 42 Batch 650 Loss 0.2684 Accuracy 0.5489\n",
      "Epoch 42 Batch 700 Loss 0.2693 Accuracy 0.5487\n",
      "Epoch 42 Batch 750 Loss 0.2708 Accuracy 0.5486\n",
      "Epoch 42 Batch 800 Loss 0.2739 Accuracy 0.5481\n",
      "Epoch 42 Batch 850 Loss 0.2747 Accuracy 0.5482\n",
      "Epoch 42 Batch 900 Loss 0.2753 Accuracy 0.5484\n",
      "Epoch 42 Batch 950 Loss 0.2771 Accuracy 0.5482\n",
      "Epoch 42 Batch 1000 Loss 0.2788 Accuracy 0.5481\n",
      "Epoch 42 Batch 1050 Loss 0.2811 Accuracy 0.5479\n",
      "Epoch 42 Batch 1100 Loss 0.2830 Accuracy 0.5479\n",
      "Epoch 42 Batch 1150 Loss 0.2840 Accuracy 0.5478\n",
      "Epoch 42 Batch 1200 Loss 0.2848 Accuracy 0.5478\n",
      "Epoch 42 Loss 0.2869 Accuracy 0.5475\n",
      "=================================================================\n",
      "Epoch 43 Batch 0 Loss 0.3211 Accuracy 0.5391\n",
      "Epoch 43 Batch 50 Loss 0.2451 Accuracy 0.5521\n",
      "Epoch 43 Batch 100 Loss 0.2434 Accuracy 0.5514\n",
      "Epoch 43 Batch 150 Loss 0.2408 Accuracy 0.5519\n",
      "Epoch 43 Batch 200 Loss 0.2469 Accuracy 0.5511\n",
      "Epoch 43 Batch 250 Loss 0.2496 Accuracy 0.5512\n",
      "Epoch 43 Batch 300 Loss 0.2528 Accuracy 0.5507\n",
      "Epoch 43 Batch 350 Loss 0.2534 Accuracy 0.5508\n",
      "Epoch 43 Batch 400 Loss 0.2555 Accuracy 0.5505\n",
      "Epoch 43 Batch 450 Loss 0.2553 Accuracy 0.5505\n",
      "Epoch 43 Batch 500 Loss 0.2595 Accuracy 0.5500\n",
      "Epoch 43 Batch 550 Loss 0.2620 Accuracy 0.5497\n",
      "Epoch 43 Batch 600 Loss 0.2628 Accuracy 0.5495\n",
      "Epoch 43 Batch 650 Loss 0.2651 Accuracy 0.5496\n",
      "Epoch 43 Batch 700 Loss 0.2658 Accuracy 0.5496\n",
      "Epoch 43 Batch 750 Loss 0.2674 Accuracy 0.5494\n",
      "Epoch 43 Batch 800 Loss 0.2692 Accuracy 0.5491\n",
      "Epoch 43 Batch 850 Loss 0.2707 Accuracy 0.5489\n",
      "Epoch 43 Batch 900 Loss 0.2725 Accuracy 0.5487\n",
      "Epoch 43 Batch 950 Loss 0.2744 Accuracy 0.5486\n",
      "Epoch 43 Batch 1000 Loss 0.2766 Accuracy 0.5484\n",
      "Epoch 43 Batch 1050 Loss 0.2779 Accuracy 0.5485\n",
      "Epoch 43 Batch 1100 Loss 0.2793 Accuracy 0.5483\n",
      "Epoch 43 Batch 1150 Loss 0.2819 Accuracy 0.5481\n",
      "Epoch 43 Batch 1200 Loss 0.2833 Accuracy 0.5480\n",
      "Epoch 43 Loss 0.2854 Accuracy 0.5478\n",
      "=================================================================\n",
      "Epoch 44 Batch 0 Loss 0.1507 Accuracy 0.5672\n",
      "Epoch 44 Batch 50 Loss 0.2395 Accuracy 0.5489\n",
      "Epoch 44 Batch 100 Loss 0.2468 Accuracy 0.5489\n",
      "Epoch 44 Batch 150 Loss 0.2476 Accuracy 0.5496\n",
      "Epoch 44 Batch 200 Loss 0.2424 Accuracy 0.5503\n",
      "Epoch 44 Batch 250 Loss 0.2452 Accuracy 0.5504\n",
      "Epoch 44 Batch 300 Loss 0.2468 Accuracy 0.5505\n",
      "Epoch 44 Batch 350 Loss 0.2488 Accuracy 0.5499\n",
      "Epoch 44 Batch 400 Loss 0.2514 Accuracy 0.5499\n",
      "Epoch 44 Batch 450 Loss 0.2531 Accuracy 0.5499\n",
      "Epoch 44 Batch 500 Loss 0.2560 Accuracy 0.5500\n",
      "Epoch 44 Batch 550 Loss 0.2582 Accuracy 0.5501\n",
      "Epoch 44 Batch 600 Loss 0.2598 Accuracy 0.5501\n",
      "Epoch 44 Batch 650 Loss 0.2619 Accuracy 0.5499\n",
      "Epoch 44 Batch 700 Loss 0.2643 Accuracy 0.5498\n",
      "Epoch 44 Batch 750 Loss 0.2653 Accuracy 0.5496\n",
      "Epoch 44 Batch 800 Loss 0.2668 Accuracy 0.5493\n",
      "Epoch 44 Batch 850 Loss 0.2692 Accuracy 0.5491\n",
      "Epoch 44 Batch 900 Loss 0.2714 Accuracy 0.5489\n",
      "Epoch 44 Batch 950 Loss 0.2723 Accuracy 0.5488\n",
      "Epoch 44 Batch 1000 Loss 0.2738 Accuracy 0.5488\n",
      "Epoch 44 Batch 1050 Loss 0.2751 Accuracy 0.5487\n",
      "Epoch 44 Batch 1100 Loss 0.2766 Accuracy 0.5486\n",
      "Epoch 44 Batch 1150 Loss 0.2777 Accuracy 0.5485\n",
      "Epoch 44 Batch 1200 Loss 0.2793 Accuracy 0.5483\n",
      "Epoch 44 Loss 0.2810 Accuracy 0.5481\n",
      "=================================================================\n",
      "Epoch 45 Batch 0 Loss 0.2720 Accuracy 0.5359\n",
      "Epoch 45 Batch 50 Loss 0.2441 Accuracy 0.5485\n",
      "Epoch 45 Batch 100 Loss 0.2377 Accuracy 0.5503\n",
      "Epoch 45 Batch 150 Loss 0.2372 Accuracy 0.5505\n",
      "Epoch 45 Batch 200 Loss 0.2400 Accuracy 0.5502\n",
      "Epoch 45 Batch 250 Loss 0.2461 Accuracy 0.5500\n",
      "Epoch 45 Batch 300 Loss 0.2476 Accuracy 0.5498\n",
      "Epoch 45 Batch 350 Loss 0.2518 Accuracy 0.5497\n",
      "Epoch 45 Batch 400 Loss 0.2538 Accuracy 0.5499\n",
      "Epoch 45 Batch 450 Loss 0.2536 Accuracy 0.5501\n",
      "Epoch 45 Batch 500 Loss 0.2554 Accuracy 0.5502\n",
      "Epoch 45 Batch 550 Loss 0.2584 Accuracy 0.5498\n",
      "Epoch 45 Batch 600 Loss 0.2593 Accuracy 0.5500\n",
      "Epoch 45 Batch 650 Loss 0.2625 Accuracy 0.5496\n",
      "Epoch 45 Batch 700 Loss 0.2641 Accuracy 0.5495\n",
      "Epoch 45 Batch 750 Loss 0.2668 Accuracy 0.5491\n",
      "Epoch 45 Batch 800 Loss 0.2677 Accuracy 0.5491\n",
      "Epoch 45 Batch 850 Loss 0.2692 Accuracy 0.5490\n",
      "Epoch 45 Batch 900 Loss 0.2711 Accuracy 0.5490\n",
      "Epoch 45 Batch 950 Loss 0.2736 Accuracy 0.5486\n",
      "Epoch 45 Batch 1000 Loss 0.2755 Accuracy 0.5485\n",
      "Epoch 45 Batch 1050 Loss 0.2761 Accuracy 0.5484\n",
      "Epoch 45 Batch 1100 Loss 0.2772 Accuracy 0.5483\n",
      "Epoch 45 Batch 1150 Loss 0.2788 Accuracy 0.5482\n",
      "Epoch 45 Batch 1200 Loss 0.2800 Accuracy 0.5483\n",
      "Epoch 45 Loss 0.2815 Accuracy 0.5483\n",
      "=================================================================\n",
      "Epoch 46 Batch 0 Loss 0.2613 Accuracy 0.5266\n",
      "Epoch 46 Batch 50 Loss 0.2338 Accuracy 0.5521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Batch 100 Loss 0.2374 Accuracy 0.5530\n",
      "Epoch 46 Batch 150 Loss 0.2407 Accuracy 0.5518\n",
      "Epoch 46 Batch 200 Loss 0.2415 Accuracy 0.5513\n",
      "Epoch 46 Batch 250 Loss 0.2427 Accuracy 0.5512\n",
      "Epoch 46 Batch 300 Loss 0.2448 Accuracy 0.5513\n",
      "Epoch 46 Batch 350 Loss 0.2459 Accuracy 0.5509\n",
      "Epoch 46 Batch 400 Loss 0.2498 Accuracy 0.5505\n",
      "Epoch 46 Batch 450 Loss 0.2517 Accuracy 0.5506\n",
      "Epoch 46 Batch 500 Loss 0.2527 Accuracy 0.5502\n",
      "Epoch 46 Batch 550 Loss 0.2544 Accuracy 0.5503\n",
      "Epoch 46 Batch 600 Loss 0.2569 Accuracy 0.5504\n",
      "Epoch 46 Batch 650 Loss 0.2576 Accuracy 0.5503\n",
      "Epoch 46 Batch 700 Loss 0.2601 Accuracy 0.5504\n",
      "Epoch 46 Batch 750 Loss 0.2624 Accuracy 0.5501\n",
      "Epoch 46 Batch 800 Loss 0.2639 Accuracy 0.5499\n",
      "Epoch 46 Batch 850 Loss 0.2653 Accuracy 0.5498\n",
      "Epoch 46 Batch 900 Loss 0.2669 Accuracy 0.5496\n",
      "Epoch 46 Batch 950 Loss 0.2691 Accuracy 0.5494\n",
      "Epoch 46 Batch 1000 Loss 0.2708 Accuracy 0.5492\n",
      "Epoch 46 Batch 1050 Loss 0.2730 Accuracy 0.5489\n",
      "Epoch 46 Batch 1100 Loss 0.2744 Accuracy 0.5490\n",
      "Epoch 46 Batch 1150 Loss 0.2752 Accuracy 0.5490\n",
      "Epoch 46 Batch 1200 Loss 0.2761 Accuracy 0.5489\n",
      "Epoch 46 Loss 0.2787 Accuracy 0.5486\n",
      "=================================================================\n",
      "Epoch 47 Batch 0 Loss 0.2313 Accuracy 0.5484\n",
      "Epoch 47 Batch 50 Loss 0.2271 Accuracy 0.5543\n",
      "Epoch 47 Batch 100 Loss 0.2381 Accuracy 0.5527\n",
      "Epoch 47 Batch 150 Loss 0.2416 Accuracy 0.5519\n",
      "Epoch 47 Batch 200 Loss 0.2405 Accuracy 0.5516\n",
      "Epoch 47 Batch 250 Loss 0.2460 Accuracy 0.5513\n",
      "Epoch 47 Batch 300 Loss 0.2474 Accuracy 0.5507\n",
      "Epoch 47 Batch 350 Loss 0.2487 Accuracy 0.5510\n",
      "Epoch 47 Batch 400 Loss 0.2512 Accuracy 0.5508\n",
      "Epoch 47 Batch 450 Loss 0.2527 Accuracy 0.5508\n",
      "Epoch 47 Batch 500 Loss 0.2535 Accuracy 0.5506\n",
      "Epoch 47 Batch 550 Loss 0.2546 Accuracy 0.5504\n",
      "Epoch 47 Batch 600 Loss 0.2569 Accuracy 0.5503\n",
      "Epoch 47 Batch 650 Loss 0.2580 Accuracy 0.5501\n",
      "Epoch 47 Batch 700 Loss 0.2600 Accuracy 0.5500\n",
      "Epoch 47 Batch 750 Loss 0.2625 Accuracy 0.5498\n",
      "Epoch 47 Batch 800 Loss 0.2633 Accuracy 0.5498\n",
      "Epoch 47 Batch 850 Loss 0.2649 Accuracy 0.5495\n",
      "Epoch 47 Batch 900 Loss 0.2668 Accuracy 0.5494\n",
      "Epoch 47 Batch 950 Loss 0.2686 Accuracy 0.5493\n",
      "Epoch 47 Batch 1000 Loss 0.2710 Accuracy 0.5491\n",
      "Epoch 47 Batch 1050 Loss 0.2722 Accuracy 0.5491\n",
      "Epoch 47 Batch 1100 Loss 0.2747 Accuracy 0.5490\n",
      "Epoch 47 Batch 1150 Loss 0.2764 Accuracy 0.5488\n",
      "Epoch 47 Batch 1200 Loss 0.2787 Accuracy 0.5486\n",
      "Epoch 47 Loss 0.2804 Accuracy 0.5485\n",
      "=================================================================\n",
      "Epoch 48 Batch 0 Loss 0.2658 Accuracy 0.5312\n",
      "Epoch 48 Batch 50 Loss 0.2356 Accuracy 0.5517\n",
      "Epoch 48 Batch 100 Loss 0.2313 Accuracy 0.5518\n",
      "Epoch 48 Batch 150 Loss 0.2362 Accuracy 0.5499\n",
      "Epoch 48 Batch 200 Loss 0.2384 Accuracy 0.5498\n",
      "Epoch 48 Batch 250 Loss 0.2419 Accuracy 0.5501\n",
      "Epoch 48 Batch 300 Loss 0.2435 Accuracy 0.5502\n",
      "Epoch 48 Batch 350 Loss 0.2467 Accuracy 0.5505\n",
      "Epoch 48 Batch 400 Loss 0.2492 Accuracy 0.5505\n",
      "Epoch 48 Batch 450 Loss 0.2492 Accuracy 0.5506\n",
      "Epoch 48 Batch 500 Loss 0.2517 Accuracy 0.5503\n",
      "Epoch 48 Batch 550 Loss 0.2533 Accuracy 0.5503\n",
      "Epoch 48 Batch 600 Loss 0.2553 Accuracy 0.5501\n",
      "Epoch 48 Batch 650 Loss 0.2587 Accuracy 0.5499\n",
      "Epoch 48 Batch 700 Loss 0.2606 Accuracy 0.5498\n",
      "Epoch 48 Batch 750 Loss 0.2622 Accuracy 0.5496\n",
      "Epoch 48 Batch 800 Loss 0.2633 Accuracy 0.5495\n",
      "Epoch 48 Batch 850 Loss 0.2649 Accuracy 0.5495\n",
      "Epoch 48 Batch 900 Loss 0.2668 Accuracy 0.5494\n",
      "Epoch 48 Batch 950 Loss 0.2675 Accuracy 0.5493\n",
      "Epoch 48 Batch 1000 Loss 0.2685 Accuracy 0.5494\n",
      "Epoch 48 Batch 1050 Loss 0.2705 Accuracy 0.5492\n",
      "Epoch 48 Batch 1100 Loss 0.2725 Accuracy 0.5490\n",
      "Epoch 48 Batch 1150 Loss 0.2741 Accuracy 0.5490\n",
      "Epoch 48 Batch 1200 Loss 0.2753 Accuracy 0.5489\n",
      "Epoch 48 Loss 0.2765 Accuracy 0.5488\n",
      "=================================================================\n",
      "Epoch 49 Batch 0 Loss 0.2901 Accuracy 0.5625\n",
      "Epoch 49 Batch 50 Loss 0.2320 Accuracy 0.5538\n",
      "Epoch 49 Batch 100 Loss 0.2264 Accuracy 0.5529\n",
      "Epoch 49 Batch 150 Loss 0.2283 Accuracy 0.5525\n",
      "Epoch 49 Batch 200 Loss 0.2324 Accuracy 0.5523\n",
      "Epoch 49 Batch 250 Loss 0.2361 Accuracy 0.5521\n",
      "Epoch 49 Batch 300 Loss 0.2413 Accuracy 0.5513\n",
      "Epoch 49 Batch 350 Loss 0.2433 Accuracy 0.5514\n",
      "Epoch 49 Batch 400 Loss 0.2471 Accuracy 0.5510\n",
      "Epoch 49 Batch 450 Loss 0.2496 Accuracy 0.5510\n",
      "Epoch 49 Batch 500 Loss 0.2513 Accuracy 0.5508\n",
      "Epoch 49 Batch 550 Loss 0.2542 Accuracy 0.5508\n",
      "Epoch 49 Batch 600 Loss 0.2560 Accuracy 0.5506\n",
      "Epoch 49 Batch 650 Loss 0.2583 Accuracy 0.5503\n",
      "Epoch 49 Batch 700 Loss 0.2600 Accuracy 0.5501\n",
      "Epoch 49 Batch 750 Loss 0.2617 Accuracy 0.5501\n",
      "Epoch 49 Batch 800 Loss 0.2629 Accuracy 0.5499\n",
      "Epoch 49 Batch 850 Loss 0.2636 Accuracy 0.5499\n",
      "Epoch 49 Batch 900 Loss 0.2655 Accuracy 0.5497\n",
      "Epoch 49 Batch 950 Loss 0.2670 Accuracy 0.5495\n",
      "Epoch 49 Batch 1000 Loss 0.2674 Accuracy 0.5495\n",
      "Epoch 49 Batch 1050 Loss 0.2691 Accuracy 0.5493\n",
      "Epoch 49 Batch 1100 Loss 0.2713 Accuracy 0.5492\n",
      "Epoch 49 Batch 1150 Loss 0.2726 Accuracy 0.5491\n",
      "Epoch 49 Batch 1200 Loss 0.2745 Accuracy 0.5489\n",
      "Epoch 49 Loss 0.2756 Accuracy 0.5488\n",
      "=================================================================\n",
      "Epoch 50 Batch 0 Loss 0.1986 Accuracy 0.5562\n",
      "Epoch 50 Batch 50 Loss 0.2242 Accuracy 0.5544\n",
      "Epoch 50 Batch 100 Loss 0.2248 Accuracy 0.5538\n",
      "Epoch 50 Batch 150 Loss 0.2289 Accuracy 0.5535\n",
      "Epoch 50 Batch 200 Loss 0.2286 Accuracy 0.5533\n",
      "Epoch 50 Batch 250 Loss 0.2322 Accuracy 0.5533\n",
      "Epoch 50 Batch 300 Loss 0.2357 Accuracy 0.5534\n",
      "Epoch 50 Batch 350 Loss 0.2394 Accuracy 0.5529\n",
      "Epoch 50 Batch 400 Loss 0.2412 Accuracy 0.5526\n",
      "Epoch 50 Batch 450 Loss 0.2453 Accuracy 0.5521\n",
      "Epoch 50 Batch 500 Loss 0.2462 Accuracy 0.5516\n",
      "Epoch 50 Batch 550 Loss 0.2495 Accuracy 0.5514\n",
      "Epoch 50 Batch 600 Loss 0.2516 Accuracy 0.5513\n",
      "Epoch 50 Batch 650 Loss 0.2545 Accuracy 0.5508\n",
      "Epoch 50 Batch 700 Loss 0.2570 Accuracy 0.5507\n",
      "Epoch 50 Batch 750 Loss 0.2587 Accuracy 0.5505\n",
      "Epoch 50 Batch 800 Loss 0.2610 Accuracy 0.5504\n",
      "Epoch 50 Batch 850 Loss 0.2633 Accuracy 0.5502\n",
      "Epoch 50 Batch 900 Loss 0.2645 Accuracy 0.5501\n",
      "Epoch 50 Batch 950 Loss 0.2652 Accuracy 0.5501\n",
      "Epoch 50 Batch 1000 Loss 0.2660 Accuracy 0.5500\n",
      "Epoch 50 Batch 1050 Loss 0.2673 Accuracy 0.5498\n",
      "Epoch 50 Batch 1100 Loss 0.2691 Accuracy 0.5497\n",
      "Epoch 50 Batch 1150 Loss 0.2704 Accuracy 0.5496\n",
      "Epoch 50 Batch 1200 Loss 0.2729 Accuracy 0.5492\n",
      "Epoch 50 Loss 0.2741 Accuracy 0.5491\n",
      "=================================================================\n",
      "Epoch 51 Batch 0 Loss 0.2049 Accuracy 0.5328\n",
      "Epoch 51 Batch 50 Loss 0.2132 Accuracy 0.5547\n",
      "Epoch 51 Batch 100 Loss 0.2244 Accuracy 0.5546\n",
      "Epoch 51 Batch 150 Loss 0.2333 Accuracy 0.5531\n",
      "Epoch 51 Batch 200 Loss 0.2356 Accuracy 0.5528\n",
      "Epoch 51 Batch 250 Loss 0.2397 Accuracy 0.5520\n",
      "Epoch 51 Batch 300 Loss 0.2447 Accuracy 0.5511\n",
      "Epoch 51 Batch 350 Loss 0.2457 Accuracy 0.5508\n",
      "Epoch 51 Batch 400 Loss 0.2488 Accuracy 0.5507\n",
      "Epoch 51 Batch 450 Loss 0.2520 Accuracy 0.5505\n",
      "Epoch 51 Batch 500 Loss 0.2533 Accuracy 0.5506\n",
      "Epoch 51 Batch 550 Loss 0.2551 Accuracy 0.5505\n",
      "Epoch 51 Batch 600 Loss 0.2557 Accuracy 0.5505\n",
      "Epoch 51 Batch 650 Loss 0.2571 Accuracy 0.5501\n",
      "Epoch 51 Batch 700 Loss 0.2584 Accuracy 0.5501\n",
      "Epoch 51 Batch 750 Loss 0.2611 Accuracy 0.5497\n",
      "Epoch 51 Batch 800 Loss 0.2632 Accuracy 0.5497\n",
      "Epoch 51 Batch 850 Loss 0.2653 Accuracy 0.5496\n",
      "Epoch 51 Batch 900 Loss 0.2655 Accuracy 0.5496\n",
      "Epoch 51 Batch 950 Loss 0.2674 Accuracy 0.5494\n",
      "Epoch 51 Batch 1000 Loss 0.2686 Accuracy 0.5493\n",
      "Epoch 51 Batch 1050 Loss 0.2700 Accuracy 0.5493\n",
      "Epoch 51 Batch 1100 Loss 0.2716 Accuracy 0.5492\n",
      "Epoch 51 Batch 1150 Loss 0.2716 Accuracy 0.5492\n",
      "Epoch 51 Batch 1200 Loss 0.2730 Accuracy 0.5490\n",
      "Epoch 51 Loss 0.2745 Accuracy 0.5491\n",
      "=================================================================\n",
      "Epoch 52 Batch 0 Loss 0.2794 Accuracy 0.5375\n",
      "Epoch 52 Batch 50 Loss 0.2424 Accuracy 0.5502\n",
      "Epoch 52 Batch 100 Loss 0.2470 Accuracy 0.5507\n",
      "Epoch 52 Batch 150 Loss 0.2447 Accuracy 0.5502\n",
      "Epoch 52 Batch 200 Loss 0.2433 Accuracy 0.5511\n",
      "Epoch 52 Batch 250 Loss 0.2411 Accuracy 0.5514\n",
      "Epoch 52 Batch 300 Loss 0.2430 Accuracy 0.5516\n",
      "Epoch 52 Batch 350 Loss 0.2438 Accuracy 0.5519\n",
      "Epoch 52 Batch 400 Loss 0.2454 Accuracy 0.5522\n",
      "Epoch 52 Batch 450 Loss 0.2480 Accuracy 0.5519\n",
      "Epoch 52 Batch 500 Loss 0.2501 Accuracy 0.5518\n",
      "Epoch 52 Batch 550 Loss 0.2529 Accuracy 0.5515\n",
      "Epoch 52 Batch 600 Loss 0.2546 Accuracy 0.5514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Batch 650 Loss 0.2561 Accuracy 0.5513\n",
      "Epoch 52 Batch 700 Loss 0.2573 Accuracy 0.5512\n",
      "Epoch 52 Batch 750 Loss 0.2581 Accuracy 0.5511\n",
      "Epoch 52 Batch 800 Loss 0.2610 Accuracy 0.5510\n",
      "Epoch 52 Batch 850 Loss 0.2620 Accuracy 0.5508\n",
      "Epoch 52 Batch 900 Loss 0.2638 Accuracy 0.5507\n",
      "Epoch 52 Batch 950 Loss 0.2658 Accuracy 0.5503\n",
      "Epoch 52 Batch 1000 Loss 0.2672 Accuracy 0.5503\n",
      "Epoch 52 Batch 1050 Loss 0.2682 Accuracy 0.5503\n",
      "Epoch 52 Batch 1100 Loss 0.2687 Accuracy 0.5501\n",
      "Epoch 52 Batch 1150 Loss 0.2709 Accuracy 0.5498\n",
      "Epoch 52 Batch 1200 Loss 0.2716 Accuracy 0.5497\n",
      "Epoch 52 Loss 0.2730 Accuracy 0.5496\n",
      "=================================================================\n",
      "Epoch 53 Batch 0 Loss 0.2876 Accuracy 0.5484\n",
      "Epoch 53 Batch 50 Loss 0.2212 Accuracy 0.5528\n",
      "Epoch 53 Batch 100 Loss 0.2204 Accuracy 0.5541\n",
      "Epoch 53 Batch 150 Loss 0.2287 Accuracy 0.5529\n",
      "Epoch 53 Batch 200 Loss 0.2315 Accuracy 0.5533\n",
      "Epoch 53 Batch 250 Loss 0.2354 Accuracy 0.5533\n",
      "Epoch 53 Batch 300 Loss 0.2384 Accuracy 0.5531\n",
      "Epoch 53 Batch 350 Loss 0.2411 Accuracy 0.5527\n",
      "Epoch 53 Batch 400 Loss 0.2393 Accuracy 0.5527\n",
      "Epoch 53 Batch 450 Loss 0.2407 Accuracy 0.5526\n",
      "Epoch 53 Batch 500 Loss 0.2437 Accuracy 0.5525\n",
      "Epoch 53 Batch 550 Loss 0.2453 Accuracy 0.5525\n",
      "Epoch 53 Batch 600 Loss 0.2488 Accuracy 0.5522\n",
      "Epoch 53 Batch 650 Loss 0.2521 Accuracy 0.5518\n",
      "Epoch 53 Batch 700 Loss 0.2531 Accuracy 0.5516\n",
      "Epoch 53 Batch 750 Loss 0.2555 Accuracy 0.5513\n",
      "Epoch 53 Batch 800 Loss 0.2564 Accuracy 0.5511\n",
      "Epoch 53 Batch 850 Loss 0.2586 Accuracy 0.5508\n",
      "Epoch 53 Batch 900 Loss 0.2604 Accuracy 0.5507\n",
      "Epoch 53 Batch 950 Loss 0.2618 Accuracy 0.5503\n",
      "Epoch 53 Batch 1000 Loss 0.2632 Accuracy 0.5502\n",
      "Epoch 53 Batch 1050 Loss 0.2653 Accuracy 0.5501\n",
      "Epoch 53 Batch 1100 Loss 0.2666 Accuracy 0.5499\n",
      "Epoch 53 Batch 1150 Loss 0.2685 Accuracy 0.5497\n",
      "Epoch 53 Batch 1200 Loss 0.2699 Accuracy 0.5498\n",
      "Epoch 53 Loss 0.2715 Accuracy 0.5496\n",
      "=================================================================\n",
      "Epoch 54 Batch 0 Loss 0.1566 Accuracy 0.5531\n",
      "Epoch 54 Batch 50 Loss 0.2185 Accuracy 0.5543\n",
      "Epoch 54 Batch 100 Loss 0.2267 Accuracy 0.5530\n",
      "Epoch 54 Batch 150 Loss 0.2308 Accuracy 0.5520\n",
      "Epoch 54 Batch 200 Loss 0.2327 Accuracy 0.5519\n",
      "Epoch 54 Batch 250 Loss 0.2356 Accuracy 0.5514\n",
      "Epoch 54 Batch 300 Loss 0.2370 Accuracy 0.5514\n",
      "Epoch 54 Batch 350 Loss 0.2383 Accuracy 0.5517\n",
      "Epoch 54 Batch 400 Loss 0.2427 Accuracy 0.5513\n",
      "Epoch 54 Batch 450 Loss 0.2442 Accuracy 0.5512\n",
      "Epoch 54 Batch 500 Loss 0.2474 Accuracy 0.5511\n",
      "Epoch 54 Batch 550 Loss 0.2481 Accuracy 0.5513\n",
      "Epoch 54 Batch 600 Loss 0.2503 Accuracy 0.5513\n",
      "Epoch 54 Batch 650 Loss 0.2521 Accuracy 0.5509\n",
      "Epoch 54 Batch 700 Loss 0.2541 Accuracy 0.5507\n",
      "Epoch 54 Batch 750 Loss 0.2562 Accuracy 0.5507\n",
      "Epoch 54 Batch 800 Loss 0.2582 Accuracy 0.5507\n",
      "Epoch 54 Batch 850 Loss 0.2593 Accuracy 0.5506\n",
      "Epoch 54 Batch 900 Loss 0.2622 Accuracy 0.5505\n",
      "Epoch 54 Batch 950 Loss 0.2637 Accuracy 0.5504\n",
      "Epoch 54 Batch 1000 Loss 0.2649 Accuracy 0.5503\n",
      "Epoch 54 Batch 1050 Loss 0.2661 Accuracy 0.5502\n",
      "Epoch 54 Batch 1100 Loss 0.2674 Accuracy 0.5501\n",
      "Epoch 54 Batch 1150 Loss 0.2691 Accuracy 0.5499\n",
      "Epoch 54 Batch 1200 Loss 0.2710 Accuracy 0.5497\n",
      "Epoch 54 Loss 0.2730 Accuracy 0.5495\n",
      "=================================================================\n",
      "Epoch 55 Batch 0 Loss 0.2796 Accuracy 0.5391\n",
      "Epoch 55 Batch 50 Loss 0.2256 Accuracy 0.5548\n",
      "Epoch 55 Batch 100 Loss 0.2266 Accuracy 0.5543\n",
      "Epoch 55 Batch 150 Loss 0.2286 Accuracy 0.5533\n",
      "Epoch 55 Batch 200 Loss 0.2302 Accuracy 0.5531\n",
      "Epoch 55 Batch 250 Loss 0.2335 Accuracy 0.5530\n",
      "Epoch 55 Batch 300 Loss 0.2330 Accuracy 0.5524\n",
      "Epoch 55 Batch 350 Loss 0.2344 Accuracy 0.5525\n",
      "Epoch 55 Batch 400 Loss 0.2367 Accuracy 0.5526\n",
      "Epoch 55 Batch 450 Loss 0.2378 Accuracy 0.5523\n",
      "Epoch 55 Batch 500 Loss 0.2418 Accuracy 0.5521\n",
      "Epoch 55 Batch 550 Loss 0.2425 Accuracy 0.5520\n",
      "Epoch 55 Batch 600 Loss 0.2452 Accuracy 0.5517\n",
      "Epoch 55 Batch 650 Loss 0.2495 Accuracy 0.5513\n",
      "Epoch 55 Batch 700 Loss 0.2522 Accuracy 0.5509\n",
      "Epoch 55 Batch 750 Loss 0.2554 Accuracy 0.5508\n",
      "Epoch 55 Batch 800 Loss 0.2572 Accuracy 0.5507\n",
      "Epoch 55 Batch 850 Loss 0.2590 Accuracy 0.5507\n",
      "Epoch 55 Batch 900 Loss 0.2601 Accuracy 0.5506\n",
      "Epoch 55 Batch 950 Loss 0.2613 Accuracy 0.5505\n",
      "Epoch 55 Batch 1000 Loss 0.2624 Accuracy 0.5504\n",
      "Epoch 55 Batch 1050 Loss 0.2637 Accuracy 0.5503\n",
      "Epoch 55 Batch 1100 Loss 0.2653 Accuracy 0.5502\n",
      "Epoch 55 Batch 1150 Loss 0.2668 Accuracy 0.5501\n",
      "Epoch 55 Batch 1200 Loss 0.2680 Accuracy 0.5502\n",
      "Epoch 55 Loss 0.2688 Accuracy 0.5501\n",
      "=================================================================\n",
      "Epoch 56 Batch 0 Loss 0.2902 Accuracy 0.5297\n",
      "Epoch 56 Batch 50 Loss 0.2290 Accuracy 0.5534\n",
      "Epoch 56 Batch 100 Loss 0.2273 Accuracy 0.5537\n",
      "Epoch 56 Batch 150 Loss 0.2257 Accuracy 0.5535\n",
      "Epoch 56 Batch 200 Loss 0.2291 Accuracy 0.5530\n",
      "Epoch 56 Batch 250 Loss 0.2276 Accuracy 0.5535\n",
      "Epoch 56 Batch 300 Loss 0.2326 Accuracy 0.5526\n",
      "Epoch 56 Batch 350 Loss 0.2349 Accuracy 0.5524\n",
      "Epoch 56 Batch 400 Loss 0.2369 Accuracy 0.5525\n",
      "Epoch 56 Batch 450 Loss 0.2386 Accuracy 0.5525\n",
      "Epoch 56 Batch 500 Loss 0.2420 Accuracy 0.5522\n",
      "Epoch 56 Batch 550 Loss 0.2422 Accuracy 0.5523\n",
      "Epoch 56 Batch 600 Loss 0.2455 Accuracy 0.5520\n",
      "Epoch 56 Batch 650 Loss 0.2485 Accuracy 0.5517\n",
      "Epoch 56 Batch 700 Loss 0.2503 Accuracy 0.5516\n",
      "Epoch 56 Batch 750 Loss 0.2535 Accuracy 0.5513\n",
      "Epoch 56 Batch 800 Loss 0.2551 Accuracy 0.5513\n",
      "Epoch 56 Batch 850 Loss 0.2572 Accuracy 0.5511\n",
      "Epoch 56 Batch 900 Loss 0.2593 Accuracy 0.5509\n",
      "Epoch 56 Batch 950 Loss 0.2605 Accuracy 0.5508\n",
      "Epoch 56 Batch 1000 Loss 0.2622 Accuracy 0.5507\n",
      "Epoch 56 Batch 1050 Loss 0.2638 Accuracy 0.5505\n",
      "Epoch 56 Batch 1100 Loss 0.2645 Accuracy 0.5504\n",
      "Epoch 56 Batch 1150 Loss 0.2649 Accuracy 0.5503\n",
      "Epoch 56 Batch 1200 Loss 0.2666 Accuracy 0.5502\n",
      "Epoch 56 Loss 0.2680 Accuracy 0.5502\n",
      "=================================================================\n",
      "Epoch 57 Batch 0 Loss 0.2236 Accuracy 0.5594\n",
      "Epoch 57 Batch 50 Loss 0.2195 Accuracy 0.5525\n",
      "Epoch 57 Batch 100 Loss 0.2184 Accuracy 0.5533\n",
      "Epoch 57 Batch 150 Loss 0.2233 Accuracy 0.5532\n",
      "Epoch 57 Batch 200 Loss 0.2221 Accuracy 0.5533\n",
      "Epoch 57 Batch 250 Loss 0.2265 Accuracy 0.5530\n",
      "Epoch 57 Batch 300 Loss 0.2311 Accuracy 0.5526\n",
      "Epoch 57 Batch 350 Loss 0.2319 Accuracy 0.5527\n",
      "Epoch 57 Batch 400 Loss 0.2347 Accuracy 0.5526\n",
      "Epoch 57 Batch 450 Loss 0.2382 Accuracy 0.5521\n",
      "Epoch 57 Batch 500 Loss 0.2431 Accuracy 0.5516\n",
      "Epoch 57 Batch 550 Loss 0.2446 Accuracy 0.5517\n",
      "Epoch 57 Batch 600 Loss 0.2472 Accuracy 0.5514\n",
      "Epoch 57 Batch 650 Loss 0.2480 Accuracy 0.5516\n",
      "Epoch 57 Batch 700 Loss 0.2506 Accuracy 0.5512\n",
      "Epoch 57 Batch 750 Loss 0.2511 Accuracy 0.5513\n",
      "Epoch 57 Batch 800 Loss 0.2541 Accuracy 0.5511\n",
      "Epoch 57 Batch 850 Loss 0.2555 Accuracy 0.5510\n",
      "Epoch 57 Batch 900 Loss 0.2564 Accuracy 0.5510\n",
      "Epoch 57 Batch 950 Loss 0.2571 Accuracy 0.5510\n",
      "Epoch 57 Batch 1000 Loss 0.2597 Accuracy 0.5507\n",
      "Epoch 57 Batch 1050 Loss 0.2616 Accuracy 0.5504\n",
      "Epoch 57 Batch 1100 Loss 0.2629 Accuracy 0.5504\n",
      "Epoch 57 Batch 1150 Loss 0.2640 Accuracy 0.5503\n",
      "Epoch 57 Batch 1200 Loss 0.2655 Accuracy 0.5503\n",
      "Epoch 57 Loss 0.2669 Accuracy 0.5502\n",
      "=================================================================\n",
      "Epoch 58 Batch 0 Loss 0.2370 Accuracy 0.5422\n",
      "Epoch 58 Batch 50 Loss 0.2073 Accuracy 0.5567\n",
      "Epoch 58 Batch 100 Loss 0.2070 Accuracy 0.5565\n",
      "Epoch 58 Batch 150 Loss 0.2110 Accuracy 0.5550\n",
      "Epoch 58 Batch 200 Loss 0.2182 Accuracy 0.5544\n",
      "Epoch 58 Batch 250 Loss 0.2247 Accuracy 0.5538\n",
      "Epoch 58 Batch 300 Loss 0.2273 Accuracy 0.5532\n",
      "Epoch 58 Batch 350 Loss 0.2314 Accuracy 0.5527\n",
      "Epoch 58 Batch 400 Loss 0.2351 Accuracy 0.5524\n",
      "Epoch 58 Batch 450 Loss 0.2379 Accuracy 0.5523\n",
      "Epoch 58 Batch 500 Loss 0.2411 Accuracy 0.5521\n",
      "Epoch 58 Batch 550 Loss 0.2438 Accuracy 0.5521\n",
      "Epoch 58 Batch 600 Loss 0.2457 Accuracy 0.5520\n",
      "Epoch 58 Batch 650 Loss 0.2463 Accuracy 0.5520\n",
      "Epoch 58 Batch 700 Loss 0.2485 Accuracy 0.5518\n",
      "Epoch 58 Batch 750 Loss 0.2510 Accuracy 0.5517\n",
      "Epoch 58 Batch 800 Loss 0.2530 Accuracy 0.5517\n",
      "Epoch 58 Batch 850 Loss 0.2549 Accuracy 0.5516\n",
      "Epoch 58 Batch 900 Loss 0.2570 Accuracy 0.5514\n",
      "Epoch 58 Batch 950 Loss 0.2586 Accuracy 0.5512\n",
      "Epoch 58 Batch 1000 Loss 0.2595 Accuracy 0.5512\n",
      "Epoch 58 Batch 1050 Loss 0.2614 Accuracy 0.5510\n",
      "Epoch 58 Batch 1100 Loss 0.2632 Accuracy 0.5507\n",
      "Epoch 58 Batch 1150 Loss 0.2648 Accuracy 0.5505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Batch 1200 Loss 0.2656 Accuracy 0.5504\n",
      "Epoch 58 Loss 0.2664 Accuracy 0.5503\n",
      "=================================================================\n",
      "Epoch 59 Batch 0 Loss 0.2246 Accuracy 0.5672\n",
      "Epoch 59 Batch 50 Loss 0.2170 Accuracy 0.5547\n",
      "Epoch 59 Batch 100 Loss 0.2177 Accuracy 0.5533\n",
      "Epoch 59 Batch 150 Loss 0.2204 Accuracy 0.5531\n",
      "Epoch 59 Batch 200 Loss 0.2264 Accuracy 0.5521\n",
      "Epoch 59 Batch 250 Loss 0.2287 Accuracy 0.5524\n",
      "Epoch 59 Batch 300 Loss 0.2324 Accuracy 0.5525\n",
      "Epoch 59 Batch 350 Loss 0.2347 Accuracy 0.5526\n",
      "Epoch 59 Batch 400 Loss 0.2360 Accuracy 0.5522\n",
      "Epoch 59 Batch 450 Loss 0.2401 Accuracy 0.5521\n",
      "Epoch 59 Batch 500 Loss 0.2437 Accuracy 0.5518\n",
      "Epoch 59 Batch 550 Loss 0.2459 Accuracy 0.5516\n",
      "Epoch 59 Batch 600 Loss 0.2469 Accuracy 0.5517\n",
      "Epoch 59 Batch 650 Loss 0.2487 Accuracy 0.5516\n",
      "Epoch 59 Batch 700 Loss 0.2500 Accuracy 0.5516\n",
      "Epoch 59 Batch 750 Loss 0.2522 Accuracy 0.5514\n",
      "Epoch 59 Batch 800 Loss 0.2548 Accuracy 0.5511\n",
      "Epoch 59 Batch 850 Loss 0.2570 Accuracy 0.5511\n",
      "Epoch 59 Batch 900 Loss 0.2584 Accuracy 0.5510\n",
      "Epoch 59 Batch 950 Loss 0.2594 Accuracy 0.5510\n",
      "Epoch 59 Batch 1000 Loss 0.2610 Accuracy 0.5508\n",
      "Epoch 59 Batch 1050 Loss 0.2620 Accuracy 0.5508\n",
      "Epoch 59 Batch 1100 Loss 0.2626 Accuracy 0.5507\n",
      "Epoch 59 Batch 1150 Loss 0.2639 Accuracy 0.5506\n",
      "Epoch 59 Batch 1200 Loss 0.2652 Accuracy 0.5504\n",
      "Epoch 59 Loss 0.2661 Accuracy 0.5503\n",
      "=================================================================\n",
      "Epoch 60 Batch 0 Loss 0.1911 Accuracy 0.5641\n",
      "Epoch 60 Batch 50 Loss 0.2246 Accuracy 0.5520\n",
      "Epoch 60 Batch 100 Loss 0.2245 Accuracy 0.5531\n",
      "Epoch 60 Batch 150 Loss 0.2271 Accuracy 0.5526\n",
      "Epoch 60 Batch 200 Loss 0.2287 Accuracy 0.5527\n",
      "Epoch 60 Batch 250 Loss 0.2311 Accuracy 0.5522\n",
      "Epoch 60 Batch 300 Loss 0.2343 Accuracy 0.5526\n",
      "Epoch 60 Batch 350 Loss 0.2371 Accuracy 0.5527\n",
      "Epoch 60 Batch 400 Loss 0.2387 Accuracy 0.5525\n",
      "Epoch 60 Batch 450 Loss 0.2399 Accuracy 0.5521\n",
      "Epoch 60 Batch 500 Loss 0.2425 Accuracy 0.5518\n",
      "Epoch 60 Batch 550 Loss 0.2448 Accuracy 0.5515\n",
      "Epoch 60 Batch 600 Loss 0.2461 Accuracy 0.5516\n",
      "Epoch 60 Batch 650 Loss 0.2479 Accuracy 0.5516\n",
      "Epoch 60 Batch 700 Loss 0.2497 Accuracy 0.5515\n",
      "Epoch 60 Batch 750 Loss 0.2508 Accuracy 0.5514\n",
      "Epoch 60 Batch 800 Loss 0.2518 Accuracy 0.5512\n",
      "Epoch 60 Batch 850 Loss 0.2533 Accuracy 0.5510\n",
      "Epoch 60 Batch 900 Loss 0.2555 Accuracy 0.5508\n",
      "Epoch 60 Batch 950 Loss 0.2569 Accuracy 0.5509\n",
      "Epoch 60 Batch 1000 Loss 0.2586 Accuracy 0.5508\n",
      "Epoch 60 Batch 1050 Loss 0.2599 Accuracy 0.5508\n",
      "Epoch 60 Batch 1100 Loss 0.2613 Accuracy 0.5508\n",
      "Epoch 60 Batch 1150 Loss 0.2624 Accuracy 0.5507\n",
      "Epoch 60 Batch 1200 Loss 0.2638 Accuracy 0.5505\n",
      "Epoch 60 Loss 0.2648 Accuracy 0.5504\n",
      "=================================================================\n",
      "Epoch 61 Batch 0 Loss 0.2486 Accuracy 0.5484\n",
      "Epoch 61 Batch 50 Loss 0.2148 Accuracy 0.5527\n",
      "Epoch 61 Batch 100 Loss 0.2184 Accuracy 0.5528\n",
      "Epoch 61 Batch 150 Loss 0.2234 Accuracy 0.5524\n",
      "Epoch 61 Batch 200 Loss 0.2292 Accuracy 0.5523\n",
      "Epoch 61 Batch 250 Loss 0.2319 Accuracy 0.5521\n",
      "Epoch 61 Batch 300 Loss 0.2330 Accuracy 0.5523\n",
      "Epoch 61 Batch 350 Loss 0.2353 Accuracy 0.5520\n",
      "Epoch 61 Batch 400 Loss 0.2377 Accuracy 0.5519\n",
      "Epoch 61 Batch 450 Loss 0.2391 Accuracy 0.5519\n",
      "Epoch 61 Batch 500 Loss 0.2417 Accuracy 0.5518\n",
      "Epoch 61 Batch 550 Loss 0.2418 Accuracy 0.5518\n",
      "Epoch 61 Batch 600 Loss 0.2448 Accuracy 0.5517\n",
      "Epoch 61 Batch 650 Loss 0.2467 Accuracy 0.5515\n",
      "Epoch 61 Batch 700 Loss 0.2477 Accuracy 0.5515\n",
      "Epoch 61 Batch 750 Loss 0.2498 Accuracy 0.5514\n",
      "Epoch 61 Batch 800 Loss 0.2526 Accuracy 0.5513\n",
      "Epoch 61 Batch 850 Loss 0.2535 Accuracy 0.5513\n",
      "Epoch 61 Batch 900 Loss 0.2549 Accuracy 0.5511\n",
      "Epoch 61 Batch 950 Loss 0.2566 Accuracy 0.5510\n",
      "Epoch 61 Batch 1000 Loss 0.2574 Accuracy 0.5510\n",
      "Epoch 61 Batch 1050 Loss 0.2586 Accuracy 0.5508\n",
      "Epoch 61 Batch 1100 Loss 0.2600 Accuracy 0.5509\n",
      "Epoch 61 Batch 1150 Loss 0.2617 Accuracy 0.5507\n",
      "Epoch 61 Batch 1200 Loss 0.2627 Accuracy 0.5507\n",
      "Epoch 61 Loss 0.2641 Accuracy 0.5506\n",
      "=================================================================\n",
      "Epoch 62 Batch 0 Loss 0.2117 Accuracy 0.5609\n",
      "Epoch 62 Batch 50 Loss 0.2165 Accuracy 0.5558\n",
      "Epoch 62 Batch 100 Loss 0.2219 Accuracy 0.5541\n",
      "Epoch 62 Batch 150 Loss 0.2190 Accuracy 0.5534\n",
      "Epoch 62 Batch 200 Loss 0.2228 Accuracy 0.5533\n",
      "Epoch 62 Batch 250 Loss 0.2252 Accuracy 0.5533\n",
      "Epoch 62 Batch 300 Loss 0.2258 Accuracy 0.5534\n",
      "Epoch 62 Batch 350 Loss 0.2291 Accuracy 0.5533\n",
      "Epoch 62 Batch 400 Loss 0.2305 Accuracy 0.5534\n",
      "Epoch 62 Batch 450 Loss 0.2339 Accuracy 0.5531\n",
      "Epoch 62 Batch 500 Loss 0.2359 Accuracy 0.5528\n",
      "Epoch 62 Batch 550 Loss 0.2398 Accuracy 0.5525\n",
      "Epoch 62 Batch 600 Loss 0.2410 Accuracy 0.5523\n",
      "Epoch 62 Batch 650 Loss 0.2431 Accuracy 0.5520\n",
      "Epoch 62 Batch 700 Loss 0.2450 Accuracy 0.5519\n",
      "Epoch 62 Batch 750 Loss 0.2461 Accuracy 0.5519\n",
      "Epoch 62 Batch 800 Loss 0.2481 Accuracy 0.5518\n",
      "Epoch 62 Batch 850 Loss 0.2496 Accuracy 0.5516\n",
      "Epoch 62 Batch 900 Loss 0.2520 Accuracy 0.5515\n",
      "Epoch 62 Batch 950 Loss 0.2534 Accuracy 0.5515\n",
      "Epoch 62 Batch 1000 Loss 0.2550 Accuracy 0.5515\n",
      "Epoch 62 Batch 1050 Loss 0.2573 Accuracy 0.5513\n",
      "Epoch 62 Batch 1100 Loss 0.2592 Accuracy 0.5511\n",
      "Epoch 62 Batch 1150 Loss 0.2613 Accuracy 0.5509\n",
      "Epoch 62 Batch 1200 Loss 0.2632 Accuracy 0.5507\n",
      "Epoch 62 Loss 0.2644 Accuracy 0.5506\n",
      "=================================================================\n",
      "Epoch 63 Batch 0 Loss 0.1958 Accuracy 0.5641\n",
      "Epoch 63 Batch 50 Loss 0.2258 Accuracy 0.5546\n",
      "Epoch 63 Batch 100 Loss 0.2273 Accuracy 0.5545\n",
      "Epoch 63 Batch 150 Loss 0.2268 Accuracy 0.5547\n",
      "Epoch 63 Batch 200 Loss 0.2253 Accuracy 0.5548\n",
      "Epoch 63 Batch 250 Loss 0.2276 Accuracy 0.5550\n",
      "Epoch 63 Batch 300 Loss 0.2291 Accuracy 0.5546\n",
      "Epoch 63 Batch 350 Loss 0.2313 Accuracy 0.5543\n",
      "Epoch 63 Batch 400 Loss 0.2344 Accuracy 0.5536\n",
      "Epoch 63 Batch 450 Loss 0.2378 Accuracy 0.5532\n",
      "Epoch 63 Batch 500 Loss 0.2393 Accuracy 0.5528\n",
      "Epoch 63 Batch 550 Loss 0.2424 Accuracy 0.5523\n",
      "Epoch 63 Batch 600 Loss 0.2446 Accuracy 0.5520\n",
      "Epoch 63 Batch 650 Loss 0.2446 Accuracy 0.5520\n",
      "Epoch 63 Batch 700 Loss 0.2460 Accuracy 0.5520\n",
      "Epoch 63 Batch 750 Loss 0.2474 Accuracy 0.5518\n",
      "Epoch 63 Batch 800 Loss 0.2486 Accuracy 0.5518\n",
      "Epoch 63 Batch 850 Loss 0.2497 Accuracy 0.5518\n",
      "Epoch 63 Batch 900 Loss 0.2518 Accuracy 0.5518\n",
      "Epoch 63 Batch 950 Loss 0.2540 Accuracy 0.5517\n",
      "Epoch 63 Batch 1000 Loss 0.2553 Accuracy 0.5516\n",
      "Epoch 63 Batch 1050 Loss 0.2570 Accuracy 0.5514\n",
      "Epoch 63 Batch 1100 Loss 0.2589 Accuracy 0.5512\n",
      "Epoch 63 Batch 1150 Loss 0.2597 Accuracy 0.5511\n",
      "Epoch 63 Batch 1200 Loss 0.2609 Accuracy 0.5510\n",
      "Epoch 63 Loss 0.2620 Accuracy 0.5509\n",
      "=================================================================\n",
      "Epoch 64 Batch 0 Loss 0.1283 Accuracy 0.5734\n",
      "Epoch 64 Batch 50 Loss 0.2051 Accuracy 0.5543\n",
      "Epoch 64 Batch 100 Loss 0.2094 Accuracy 0.5551\n",
      "Epoch 64 Batch 150 Loss 0.2161 Accuracy 0.5555\n",
      "Epoch 64 Batch 200 Loss 0.2185 Accuracy 0.5550\n",
      "Epoch 64 Batch 250 Loss 0.2245 Accuracy 0.5546\n",
      "Epoch 64 Batch 300 Loss 0.2266 Accuracy 0.5541\n",
      "Epoch 64 Batch 350 Loss 0.2276 Accuracy 0.5535\n",
      "Epoch 64 Batch 400 Loss 0.2280 Accuracy 0.5534\n",
      "Epoch 64 Batch 450 Loss 0.2291 Accuracy 0.5531\n",
      "Epoch 64 Batch 500 Loss 0.2326 Accuracy 0.5528\n",
      "Epoch 64 Batch 550 Loss 0.2354 Accuracy 0.5523\n",
      "Epoch 64 Batch 600 Loss 0.2367 Accuracy 0.5524\n",
      "Epoch 64 Batch 650 Loss 0.2386 Accuracy 0.5522\n",
      "Epoch 64 Batch 700 Loss 0.2408 Accuracy 0.5519\n",
      "Epoch 64 Batch 750 Loss 0.2425 Accuracy 0.5518\n",
      "Epoch 64 Batch 800 Loss 0.2440 Accuracy 0.5518\n",
      "Epoch 64 Batch 850 Loss 0.2458 Accuracy 0.5519\n",
      "Epoch 64 Batch 900 Loss 0.2482 Accuracy 0.5518\n",
      "Epoch 64 Batch 950 Loss 0.2508 Accuracy 0.5517\n",
      "Epoch 64 Batch 1000 Loss 0.2515 Accuracy 0.5517\n",
      "Epoch 64 Batch 1050 Loss 0.2531 Accuracy 0.5515\n",
      "Epoch 64 Batch 1100 Loss 0.2544 Accuracy 0.5514\n",
      "Epoch 64 Batch 1150 Loss 0.2558 Accuracy 0.5512\n",
      "Epoch 64 Batch 1200 Loss 0.2570 Accuracy 0.5512\n",
      "Epoch 64 Loss 0.2587 Accuracy 0.5511\n",
      "=================================================================\n",
      "Epoch 65 Batch 0 Loss 0.2204 Accuracy 0.5516\n",
      "Epoch 65 Batch 50 Loss 0.2097 Accuracy 0.5555\n",
      "Epoch 65 Batch 100 Loss 0.2164 Accuracy 0.5552\n",
      "Epoch 65 Batch 150 Loss 0.2180 Accuracy 0.5544\n",
      "Epoch 65 Batch 200 Loss 0.2216 Accuracy 0.5540\n",
      "Epoch 65 Batch 250 Loss 0.2255 Accuracy 0.5538\n",
      "Epoch 65 Batch 300 Loss 0.2258 Accuracy 0.5538\n",
      "Epoch 65 Batch 350 Loss 0.2269 Accuracy 0.5533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 400 Loss 0.2296 Accuracy 0.5531\n",
      "Epoch 65 Batch 450 Loss 0.2310 Accuracy 0.5528\n",
      "Epoch 65 Batch 500 Loss 0.2331 Accuracy 0.5527\n",
      "Epoch 65 Batch 550 Loss 0.2359 Accuracy 0.5527\n",
      "Epoch 65 Batch 600 Loss 0.2368 Accuracy 0.5525\n",
      "Epoch 65 Batch 650 Loss 0.2399 Accuracy 0.5524\n",
      "Epoch 65 Batch 700 Loss 0.2418 Accuracy 0.5523\n",
      "Epoch 65 Batch 750 Loss 0.2447 Accuracy 0.5520\n",
      "Epoch 65 Batch 800 Loss 0.2471 Accuracy 0.5518\n",
      "Epoch 65 Batch 850 Loss 0.2489 Accuracy 0.5518\n",
      "Epoch 65 Batch 900 Loss 0.2514 Accuracy 0.5515\n",
      "Epoch 65 Batch 950 Loss 0.2532 Accuracy 0.5514\n",
      "Epoch 65 Batch 1000 Loss 0.2553 Accuracy 0.5512\n",
      "Epoch 65 Batch 1050 Loss 0.2567 Accuracy 0.5511\n",
      "Epoch 65 Batch 1100 Loss 0.2577 Accuracy 0.5511\n",
      "Epoch 65 Batch 1150 Loss 0.2590 Accuracy 0.5511\n",
      "Epoch 65 Batch 1200 Loss 0.2604 Accuracy 0.5511\n",
      "Epoch 65 Loss 0.2613 Accuracy 0.5511\n",
      "=================================================================\n",
      "Epoch 66 Batch 0 Loss 0.2395 Accuracy 0.5266\n",
      "Epoch 66 Batch 50 Loss 0.2112 Accuracy 0.5565\n",
      "Epoch 66 Batch 100 Loss 0.2124 Accuracy 0.5553\n",
      "Epoch 66 Batch 150 Loss 0.2158 Accuracy 0.5544\n",
      "Epoch 66 Batch 200 Loss 0.2173 Accuracy 0.5548\n",
      "Epoch 66 Batch 250 Loss 0.2191 Accuracy 0.5550\n",
      "Epoch 66 Batch 300 Loss 0.2230 Accuracy 0.5547\n",
      "Epoch 66 Batch 350 Loss 0.2257 Accuracy 0.5543\n",
      "Epoch 66 Batch 400 Loss 0.2278 Accuracy 0.5534\n",
      "Epoch 66 Batch 450 Loss 0.2305 Accuracy 0.5534\n",
      "Epoch 66 Batch 500 Loss 0.2315 Accuracy 0.5532\n",
      "Epoch 66 Batch 550 Loss 0.2347 Accuracy 0.5527\n",
      "Epoch 66 Batch 600 Loss 0.2378 Accuracy 0.5526\n",
      "Epoch 66 Batch 650 Loss 0.2409 Accuracy 0.5525\n",
      "Epoch 66 Batch 700 Loss 0.2433 Accuracy 0.5522\n",
      "Epoch 66 Batch 750 Loss 0.2440 Accuracy 0.5523\n",
      "Epoch 66 Batch 800 Loss 0.2460 Accuracy 0.5522\n",
      "Epoch 66 Batch 850 Loss 0.2485 Accuracy 0.5520\n",
      "Epoch 66 Batch 900 Loss 0.2507 Accuracy 0.5520\n",
      "Epoch 66 Batch 950 Loss 0.2519 Accuracy 0.5517\n",
      "Epoch 66 Batch 1000 Loss 0.2527 Accuracy 0.5516\n",
      "Epoch 66 Batch 1050 Loss 0.2550 Accuracy 0.5515\n",
      "Epoch 66 Batch 1100 Loss 0.2566 Accuracy 0.5516\n",
      "Epoch 66 Batch 1150 Loss 0.2581 Accuracy 0.5515\n",
      "Epoch 66 Batch 1200 Loss 0.2593 Accuracy 0.5514\n",
      "Epoch 66 Loss 0.2604 Accuracy 0.5513\n",
      "=================================================================\n",
      "Epoch 67 Batch 0 Loss 0.1825 Accuracy 0.5609\n",
      "Epoch 67 Batch 50 Loss 0.2205 Accuracy 0.5530\n",
      "Epoch 67 Batch 100 Loss 0.2189 Accuracy 0.5539\n",
      "Epoch 67 Batch 150 Loss 0.2270 Accuracy 0.5539\n",
      "Epoch 67 Batch 200 Loss 0.2277 Accuracy 0.5538\n",
      "Epoch 67 Batch 250 Loss 0.2316 Accuracy 0.5536\n",
      "Epoch 67 Batch 300 Loss 0.2347 Accuracy 0.5526\n",
      "Epoch 67 Batch 350 Loss 0.2360 Accuracy 0.5527\n",
      "Epoch 67 Batch 400 Loss 0.2388 Accuracy 0.5526\n",
      "Epoch 67 Batch 450 Loss 0.2403 Accuracy 0.5523\n",
      "Epoch 67 Batch 500 Loss 0.2416 Accuracy 0.5519\n",
      "Epoch 67 Batch 550 Loss 0.2437 Accuracy 0.5518\n",
      "Epoch 67 Batch 600 Loss 0.2439 Accuracy 0.5517\n",
      "Epoch 67 Batch 650 Loss 0.2451 Accuracy 0.5518\n",
      "Epoch 67 Batch 700 Loss 0.2457 Accuracy 0.5519\n",
      "Epoch 67 Batch 750 Loss 0.2468 Accuracy 0.5519\n",
      "Epoch 67 Batch 800 Loss 0.2498 Accuracy 0.5518\n",
      "Epoch 67 Batch 850 Loss 0.2507 Accuracy 0.5518\n",
      "Epoch 67 Batch 900 Loss 0.2517 Accuracy 0.5517\n",
      "Epoch 67 Batch 950 Loss 0.2529 Accuracy 0.5516\n",
      "Epoch 67 Batch 1000 Loss 0.2542 Accuracy 0.5515\n",
      "Epoch 67 Batch 1050 Loss 0.2549 Accuracy 0.5514\n",
      "Epoch 67 Batch 1100 Loss 0.2568 Accuracy 0.5514\n",
      "Epoch 67 Batch 1150 Loss 0.2583 Accuracy 0.5513\n",
      "Epoch 67 Batch 1200 Loss 0.2589 Accuracy 0.5512\n",
      "Epoch 67 Loss 0.2608 Accuracy 0.5511\n",
      "=================================================================\n",
      "Epoch 68 Batch 0 Loss 0.1398 Accuracy 0.5359\n",
      "Epoch 68 Batch 50 Loss 0.2169 Accuracy 0.5532\n",
      "Epoch 68 Batch 100 Loss 0.2169 Accuracy 0.5535\n",
      "Epoch 68 Batch 150 Loss 0.2217 Accuracy 0.5526\n",
      "Epoch 68 Batch 200 Loss 0.2265 Accuracy 0.5528\n",
      "Epoch 68 Batch 250 Loss 0.2243 Accuracy 0.5531\n",
      "Epoch 68 Batch 300 Loss 0.2292 Accuracy 0.5523\n",
      "Epoch 68 Batch 350 Loss 0.2311 Accuracy 0.5525\n",
      "Epoch 68 Batch 400 Loss 0.2337 Accuracy 0.5520\n",
      "Epoch 68 Batch 450 Loss 0.2350 Accuracy 0.5518\n",
      "Epoch 68 Batch 500 Loss 0.2353 Accuracy 0.5519\n",
      "Epoch 68 Batch 550 Loss 0.2373 Accuracy 0.5518\n",
      "Epoch 68 Batch 600 Loss 0.2393 Accuracy 0.5516\n",
      "Epoch 68 Batch 650 Loss 0.2404 Accuracy 0.5516\n",
      "Epoch 68 Batch 700 Loss 0.2434 Accuracy 0.5514\n",
      "Epoch 68 Batch 750 Loss 0.2451 Accuracy 0.5514\n",
      "Epoch 68 Batch 800 Loss 0.2462 Accuracy 0.5515\n",
      "Epoch 68 Batch 850 Loss 0.2492 Accuracy 0.5515\n",
      "Epoch 68 Batch 900 Loss 0.2509 Accuracy 0.5515\n",
      "Epoch 68 Batch 950 Loss 0.2529 Accuracy 0.5516\n",
      "Epoch 68 Batch 1000 Loss 0.2546 Accuracy 0.5515\n",
      "Epoch 68 Batch 1050 Loss 0.2557 Accuracy 0.5515\n",
      "Epoch 68 Batch 1100 Loss 0.2565 Accuracy 0.5514\n",
      "Epoch 68 Batch 1150 Loss 0.2583 Accuracy 0.5513\n",
      "Epoch 68 Batch 1200 Loss 0.2602 Accuracy 0.5512\n",
      "Epoch 68 Loss 0.2610 Accuracy 0.5510\n",
      "=================================================================\n",
      "Epoch 69 Batch 0 Loss 0.1613 Accuracy 0.5312\n",
      "Epoch 69 Batch 50 Loss 0.2170 Accuracy 0.5515\n",
      "Epoch 69 Batch 100 Loss 0.2153 Accuracy 0.5527\n",
      "Epoch 69 Batch 150 Loss 0.2125 Accuracy 0.5540\n",
      "Epoch 69 Batch 200 Loss 0.2174 Accuracy 0.5534\n",
      "Epoch 69 Batch 250 Loss 0.2227 Accuracy 0.5534\n",
      "Epoch 69 Batch 300 Loss 0.2252 Accuracy 0.5536\n",
      "Epoch 69 Batch 350 Loss 0.2264 Accuracy 0.5535\n",
      "Epoch 69 Batch 400 Loss 0.2285 Accuracy 0.5534\n",
      "Epoch 69 Batch 450 Loss 0.2304 Accuracy 0.5536\n",
      "Epoch 69 Batch 500 Loss 0.2319 Accuracy 0.5532\n",
      "Epoch 69 Batch 550 Loss 0.2320 Accuracy 0.5535\n",
      "Epoch 69 Batch 600 Loss 0.2355 Accuracy 0.5532\n",
      "Epoch 69 Batch 650 Loss 0.2376 Accuracy 0.5529\n",
      "Epoch 69 Batch 700 Loss 0.2393 Accuracy 0.5528\n",
      "Epoch 69 Batch 750 Loss 0.2423 Accuracy 0.5526\n",
      "Epoch 69 Batch 800 Loss 0.2435 Accuracy 0.5523\n",
      "Epoch 69 Batch 850 Loss 0.2448 Accuracy 0.5523\n",
      "Epoch 69 Batch 900 Loss 0.2464 Accuracy 0.5522\n",
      "Epoch 69 Batch 950 Loss 0.2472 Accuracy 0.5523\n",
      "Epoch 69 Batch 1000 Loss 0.2491 Accuracy 0.5522\n",
      "Epoch 69 Batch 1050 Loss 0.2514 Accuracy 0.5519\n",
      "Epoch 69 Batch 1100 Loss 0.2522 Accuracy 0.5519\n",
      "Epoch 69 Batch 1150 Loss 0.2531 Accuracy 0.5518\n",
      "Epoch 69 Batch 1200 Loss 0.2544 Accuracy 0.5516\n",
      "Epoch 69 Loss 0.2560 Accuracy 0.5516\n",
      "=================================================================\n",
      "Epoch 70 Batch 0 Loss 0.1520 Accuracy 0.5516\n",
      "Epoch 70 Batch 50 Loss 0.2007 Accuracy 0.5535\n",
      "Epoch 70 Batch 100 Loss 0.2081 Accuracy 0.5534\n",
      "Epoch 70 Batch 150 Loss 0.2099 Accuracy 0.5539\n",
      "Epoch 70 Batch 200 Loss 0.2149 Accuracy 0.5539\n",
      "Epoch 70 Batch 250 Loss 0.2164 Accuracy 0.5540\n",
      "Epoch 70 Batch 300 Loss 0.2197 Accuracy 0.5537\n",
      "Epoch 70 Batch 350 Loss 0.2211 Accuracy 0.5538\n",
      "Epoch 70 Batch 400 Loss 0.2247 Accuracy 0.5539\n",
      "Epoch 70 Batch 450 Loss 0.2261 Accuracy 0.5539\n",
      "Epoch 70 Batch 500 Loss 0.2293 Accuracy 0.5536\n",
      "Epoch 70 Batch 550 Loss 0.2317 Accuracy 0.5535\n",
      "Epoch 70 Batch 600 Loss 0.2343 Accuracy 0.5532\n",
      "Epoch 70 Batch 650 Loss 0.2369 Accuracy 0.5529\n",
      "Epoch 70 Batch 700 Loss 0.2386 Accuracy 0.5528\n",
      "Epoch 70 Batch 750 Loss 0.2418 Accuracy 0.5525\n",
      "Epoch 70 Batch 800 Loss 0.2439 Accuracy 0.5523\n",
      "Epoch 70 Batch 850 Loss 0.2449 Accuracy 0.5522\n",
      "Epoch 70 Batch 900 Loss 0.2460 Accuracy 0.5524\n",
      "Epoch 70 Batch 950 Loss 0.2482 Accuracy 0.5521\n",
      "Epoch 70 Batch 1000 Loss 0.2497 Accuracy 0.5521\n",
      "Epoch 70 Batch 1050 Loss 0.2515 Accuracy 0.5519\n",
      "Epoch 70 Batch 1100 Loss 0.2528 Accuracy 0.5519\n",
      "Epoch 70 Batch 1150 Loss 0.2551 Accuracy 0.5517\n",
      "Epoch 70 Batch 1200 Loss 0.2563 Accuracy 0.5517\n",
      "Epoch 70 Loss 0.2570 Accuracy 0.5516\n",
      "=================================================================\n",
      "Epoch 71 Batch 0 Loss 0.2095 Accuracy 0.5625\n",
      "Epoch 71 Batch 50 Loss 0.2115 Accuracy 0.5564\n",
      "Epoch 71 Batch 100 Loss 0.2117 Accuracy 0.5576\n",
      "Epoch 71 Batch 150 Loss 0.2149 Accuracy 0.5565\n",
      "Epoch 71 Batch 200 Loss 0.2183 Accuracy 0.5555\n",
      "Epoch 71 Batch 250 Loss 0.2212 Accuracy 0.5548\n",
      "Epoch 71 Batch 300 Loss 0.2236 Accuracy 0.5546\n",
      "Epoch 71 Batch 350 Loss 0.2257 Accuracy 0.5544\n",
      "Epoch 71 Batch 400 Loss 0.2278 Accuracy 0.5543\n",
      "Epoch 71 Batch 450 Loss 0.2306 Accuracy 0.5541\n",
      "Epoch 71 Batch 500 Loss 0.2330 Accuracy 0.5537\n",
      "Epoch 71 Batch 550 Loss 0.2342 Accuracy 0.5532\n",
      "Epoch 71 Batch 600 Loss 0.2356 Accuracy 0.5531\n",
      "Epoch 71 Batch 650 Loss 0.2379 Accuracy 0.5529\n",
      "Epoch 71 Batch 700 Loss 0.2390 Accuracy 0.5528\n",
      "Epoch 71 Batch 750 Loss 0.2406 Accuracy 0.5526\n",
      "Epoch 71 Batch 800 Loss 0.2428 Accuracy 0.5525\n",
      "Epoch 71 Batch 850 Loss 0.2448 Accuracy 0.5523\n",
      "Epoch 71 Batch 900 Loss 0.2460 Accuracy 0.5523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Batch 950 Loss 0.2472 Accuracy 0.5523\n",
      "Epoch 71 Batch 1000 Loss 0.2487 Accuracy 0.5522\n",
      "Epoch 71 Batch 1050 Loss 0.2500 Accuracy 0.5521\n",
      "Epoch 71 Batch 1100 Loss 0.2523 Accuracy 0.5518\n",
      "Epoch 71 Batch 1150 Loss 0.2529 Accuracy 0.5519\n",
      "Epoch 71 Batch 1200 Loss 0.2541 Accuracy 0.5518\n",
      "Epoch 71 Loss 0.2562 Accuracy 0.5516\n",
      "=================================================================\n",
      "Epoch 72 Batch 0 Loss 0.2479 Accuracy 0.5406\n",
      "Epoch 72 Batch 50 Loss 0.2130 Accuracy 0.5552\n",
      "Epoch 72 Batch 100 Loss 0.2115 Accuracy 0.5564\n",
      "Epoch 72 Batch 150 Loss 0.2141 Accuracy 0.5562\n",
      "Epoch 72 Batch 200 Loss 0.2153 Accuracy 0.5565\n",
      "Epoch 72 Batch 250 Loss 0.2176 Accuracy 0.5555\n",
      "Epoch 72 Batch 300 Loss 0.2208 Accuracy 0.5553\n",
      "Epoch 72 Batch 350 Loss 0.2243 Accuracy 0.5547\n",
      "Epoch 72 Batch 400 Loss 0.2269 Accuracy 0.5543\n",
      "Epoch 72 Batch 450 Loss 0.2292 Accuracy 0.5542\n",
      "Epoch 72 Batch 500 Loss 0.2303 Accuracy 0.5542\n",
      "Epoch 72 Batch 550 Loss 0.2327 Accuracy 0.5542\n",
      "Epoch 72 Batch 600 Loss 0.2345 Accuracy 0.5540\n",
      "Epoch 72 Batch 650 Loss 0.2365 Accuracy 0.5538\n",
      "Epoch 72 Batch 700 Loss 0.2384 Accuracy 0.5536\n",
      "Epoch 72 Batch 750 Loss 0.2406 Accuracy 0.5534\n",
      "Epoch 72 Batch 800 Loss 0.2422 Accuracy 0.5532\n",
      "Epoch 72 Batch 850 Loss 0.2429 Accuracy 0.5532\n",
      "Epoch 72 Batch 900 Loss 0.2443 Accuracy 0.5529\n",
      "Epoch 72 Batch 950 Loss 0.2463 Accuracy 0.5527\n",
      "Epoch 72 Batch 1000 Loss 0.2469 Accuracy 0.5527\n",
      "Epoch 72 Batch 1050 Loss 0.2480 Accuracy 0.5526\n",
      "Epoch 72 Batch 1100 Loss 0.2501 Accuracy 0.5525\n",
      "Epoch 72 Batch 1150 Loss 0.2524 Accuracy 0.5522\n",
      "Epoch 72 Batch 1200 Loss 0.2539 Accuracy 0.5520\n",
      "Epoch 72 Loss 0.2555 Accuracy 0.5519\n",
      "=================================================================\n",
      "Epoch 73 Batch 0 Loss 0.2022 Accuracy 0.5531\n",
      "Epoch 73 Batch 50 Loss 0.2253 Accuracy 0.5544\n",
      "Epoch 73 Batch 100 Loss 0.2183 Accuracy 0.5552\n",
      "Epoch 73 Batch 150 Loss 0.2232 Accuracy 0.5545\n",
      "Epoch 73 Batch 200 Loss 0.2235 Accuracy 0.5546\n",
      "Epoch 73 Batch 250 Loss 0.2237 Accuracy 0.5536\n",
      "Epoch 73 Batch 300 Loss 0.2248 Accuracy 0.5541\n",
      "Epoch 73 Batch 350 Loss 0.2258 Accuracy 0.5538\n",
      "Epoch 73 Batch 400 Loss 0.2285 Accuracy 0.5537\n",
      "Epoch 73 Batch 450 Loss 0.2312 Accuracy 0.5535\n",
      "Epoch 73 Batch 500 Loss 0.2350 Accuracy 0.5533\n",
      "Epoch 73 Batch 550 Loss 0.2368 Accuracy 0.5534\n",
      "Epoch 73 Batch 600 Loss 0.2365 Accuracy 0.5535\n",
      "Epoch 73 Batch 650 Loss 0.2378 Accuracy 0.5535\n",
      "Epoch 73 Batch 700 Loss 0.2398 Accuracy 0.5532\n",
      "Epoch 73 Batch 750 Loss 0.2406 Accuracy 0.5532\n",
      "Epoch 73 Batch 800 Loss 0.2424 Accuracy 0.5530\n",
      "Epoch 73 Batch 850 Loss 0.2438 Accuracy 0.5529\n",
      "Epoch 73 Batch 900 Loss 0.2455 Accuracy 0.5528\n",
      "Epoch 73 Batch 950 Loss 0.2482 Accuracy 0.5526\n",
      "Epoch 73 Batch 1000 Loss 0.2494 Accuracy 0.5527\n",
      "Epoch 73 Batch 1050 Loss 0.2509 Accuracy 0.5524\n",
      "Epoch 73 Batch 1100 Loss 0.2527 Accuracy 0.5521\n",
      "Epoch 73 Batch 1150 Loss 0.2539 Accuracy 0.5520\n",
      "Epoch 73 Batch 1200 Loss 0.2552 Accuracy 0.5520\n",
      "Epoch 73 Loss 0.2565 Accuracy 0.5518\n",
      "=================================================================\n",
      "Epoch 74 Batch 0 Loss 0.2491 Accuracy 0.5609\n",
      "Epoch 74 Batch 50 Loss 0.2152 Accuracy 0.5557\n",
      "Epoch 74 Batch 100 Loss 0.2092 Accuracy 0.5552\n",
      "Epoch 74 Batch 150 Loss 0.2163 Accuracy 0.5543\n",
      "Epoch 74 Batch 200 Loss 0.2179 Accuracy 0.5534\n",
      "Epoch 74 Batch 250 Loss 0.2227 Accuracy 0.5532\n",
      "Epoch 74 Batch 300 Loss 0.2266 Accuracy 0.5523\n",
      "Epoch 74 Batch 350 Loss 0.2272 Accuracy 0.5525\n",
      "Epoch 74 Batch 400 Loss 0.2284 Accuracy 0.5528\n",
      "Epoch 74 Batch 450 Loss 0.2290 Accuracy 0.5528\n",
      "Epoch 74 Batch 500 Loss 0.2304 Accuracy 0.5530\n",
      "Epoch 74 Batch 550 Loss 0.2312 Accuracy 0.5533\n",
      "Epoch 74 Batch 600 Loss 0.2326 Accuracy 0.5530\n",
      "Epoch 74 Batch 650 Loss 0.2353 Accuracy 0.5527\n",
      "Epoch 74 Batch 700 Loss 0.2369 Accuracy 0.5527\n",
      "Epoch 74 Batch 750 Loss 0.2385 Accuracy 0.5526\n",
      "Epoch 74 Batch 800 Loss 0.2404 Accuracy 0.5524\n",
      "Epoch 74 Batch 850 Loss 0.2417 Accuracy 0.5524\n",
      "Epoch 74 Batch 900 Loss 0.2448 Accuracy 0.5523\n",
      "Epoch 74 Batch 950 Loss 0.2469 Accuracy 0.5521\n",
      "Epoch 74 Batch 1000 Loss 0.2482 Accuracy 0.5520\n",
      "Epoch 74 Batch 1050 Loss 0.2498 Accuracy 0.5519\n",
      "Epoch 74 Batch 1100 Loss 0.2515 Accuracy 0.5519\n",
      "Epoch 74 Batch 1150 Loss 0.2533 Accuracy 0.5517\n",
      "Epoch 74 Batch 1200 Loss 0.2548 Accuracy 0.5517\n",
      "Epoch 74 Loss 0.2563 Accuracy 0.5517\n",
      "=================================================================\n",
      "Epoch 75 Batch 0 Loss 0.2587 Accuracy 0.5688\n",
      "Epoch 75 Batch 50 Loss 0.2144 Accuracy 0.5528\n",
      "Epoch 75 Batch 100 Loss 0.2066 Accuracy 0.5558\n",
      "Epoch 75 Batch 150 Loss 0.2154 Accuracy 0.5543\n",
      "Epoch 75 Batch 200 Loss 0.2177 Accuracy 0.5542\n",
      "Epoch 75 Batch 250 Loss 0.2168 Accuracy 0.5543\n",
      "Epoch 75 Batch 300 Loss 0.2209 Accuracy 0.5543\n",
      "Epoch 75 Batch 350 Loss 0.2231 Accuracy 0.5542\n",
      "Epoch 75 Batch 400 Loss 0.2254 Accuracy 0.5544\n",
      "Epoch 75 Batch 450 Loss 0.2279 Accuracy 0.5543\n",
      "Epoch 75 Batch 500 Loss 0.2305 Accuracy 0.5542\n",
      "Epoch 75 Batch 550 Loss 0.2326 Accuracy 0.5540\n",
      "Epoch 75 Batch 600 Loss 0.2358 Accuracy 0.5539\n",
      "Epoch 75 Batch 650 Loss 0.2378 Accuracy 0.5536\n",
      "Epoch 75 Batch 700 Loss 0.2404 Accuracy 0.5533\n",
      "Epoch 75 Batch 750 Loss 0.2419 Accuracy 0.5529\n",
      "Epoch 75 Batch 800 Loss 0.2436 Accuracy 0.5528\n",
      "Epoch 75 Batch 850 Loss 0.2449 Accuracy 0.5529\n",
      "Epoch 75 Batch 900 Loss 0.2458 Accuracy 0.5525\n",
      "Epoch 75 Batch 950 Loss 0.2478 Accuracy 0.5523\n",
      "Epoch 75 Batch 1000 Loss 0.2490 Accuracy 0.5524\n",
      "Epoch 75 Batch 1050 Loss 0.2505 Accuracy 0.5523\n",
      "Epoch 75 Batch 1100 Loss 0.2513 Accuracy 0.5522\n",
      "Epoch 75 Batch 1150 Loss 0.2520 Accuracy 0.5521\n",
      "Epoch 75 Batch 1200 Loss 0.2536 Accuracy 0.5520\n",
      "Epoch 75 Loss 0.2546 Accuracy 0.5521\n",
      "=================================================================\n",
      "Epoch 76 Batch 0 Loss 0.0941 Accuracy 0.5750\n",
      "Epoch 76 Batch 50 Loss 0.1990 Accuracy 0.5544\n",
      "Epoch 76 Batch 100 Loss 0.2013 Accuracy 0.5546\n",
      "Epoch 76 Batch 150 Loss 0.2087 Accuracy 0.5533\n",
      "Epoch 76 Batch 200 Loss 0.2132 Accuracy 0.5533\n",
      "Epoch 76 Batch 250 Loss 0.2154 Accuracy 0.5535\n",
      "Epoch 76 Batch 300 Loss 0.2186 Accuracy 0.5535\n",
      "Epoch 76 Batch 350 Loss 0.2215 Accuracy 0.5535\n",
      "Epoch 76 Batch 400 Loss 0.2237 Accuracy 0.5534\n",
      "Epoch 76 Batch 450 Loss 0.2267 Accuracy 0.5530\n",
      "Epoch 76 Batch 500 Loss 0.2297 Accuracy 0.5527\n",
      "Epoch 76 Batch 550 Loss 0.2309 Accuracy 0.5528\n",
      "Epoch 76 Batch 600 Loss 0.2325 Accuracy 0.5527\n",
      "Epoch 76 Batch 650 Loss 0.2342 Accuracy 0.5526\n",
      "Epoch 76 Batch 700 Loss 0.2365 Accuracy 0.5525\n",
      "Epoch 76 Batch 750 Loss 0.2379 Accuracy 0.5527\n",
      "Epoch 76 Batch 800 Loss 0.2403 Accuracy 0.5525\n",
      "Epoch 76 Batch 850 Loss 0.2414 Accuracy 0.5526\n",
      "Epoch 76 Batch 900 Loss 0.2440 Accuracy 0.5524\n",
      "Epoch 76 Batch 950 Loss 0.2462 Accuracy 0.5521\n",
      "Epoch 76 Batch 1000 Loss 0.2466 Accuracy 0.5521\n",
      "Epoch 76 Batch 1050 Loss 0.2480 Accuracy 0.5521\n",
      "Epoch 76 Batch 1100 Loss 0.2494 Accuracy 0.5522\n",
      "Epoch 76 Batch 1150 Loss 0.2503 Accuracy 0.5522\n",
      "Epoch 76 Batch 1200 Loss 0.2516 Accuracy 0.5521\n",
      "Epoch 76 Loss 0.2528 Accuracy 0.5520\n",
      "=================================================================\n",
      "Epoch 77 Batch 0 Loss 0.1354 Accuracy 0.5453\n",
      "Epoch 77 Batch 50 Loss 0.2109 Accuracy 0.5525\n",
      "Epoch 77 Batch 100 Loss 0.2101 Accuracy 0.5549\n",
      "Epoch 77 Batch 150 Loss 0.2129 Accuracy 0.5553\n",
      "Epoch 77 Batch 200 Loss 0.2136 Accuracy 0.5550\n",
      "Epoch 77 Batch 250 Loss 0.2156 Accuracy 0.5549\n",
      "Epoch 77 Batch 300 Loss 0.2153 Accuracy 0.5554\n",
      "Epoch 77 Batch 350 Loss 0.2191 Accuracy 0.5551\n",
      "Epoch 77 Batch 400 Loss 0.2221 Accuracy 0.5547\n",
      "Epoch 77 Batch 450 Loss 0.2250 Accuracy 0.5544\n",
      "Epoch 77 Batch 500 Loss 0.2256 Accuracy 0.5540\n",
      "Epoch 77 Batch 550 Loss 0.2280 Accuracy 0.5539\n",
      "Epoch 77 Batch 600 Loss 0.2310 Accuracy 0.5536\n",
      "Epoch 77 Batch 650 Loss 0.2328 Accuracy 0.5535\n",
      "Epoch 77 Batch 700 Loss 0.2351 Accuracy 0.5533\n",
      "Epoch 77 Batch 750 Loss 0.2365 Accuracy 0.5532\n",
      "Epoch 77 Batch 800 Loss 0.2392 Accuracy 0.5529\n",
      "Epoch 77 Batch 850 Loss 0.2407 Accuracy 0.5528\n",
      "Epoch 77 Batch 900 Loss 0.2417 Accuracy 0.5529\n",
      "Epoch 77 Batch 950 Loss 0.2438 Accuracy 0.5527\n",
      "Epoch 77 Batch 1000 Loss 0.2455 Accuracy 0.5528\n",
      "Epoch 77 Batch 1050 Loss 0.2472 Accuracy 0.5526\n",
      "Epoch 77 Batch 1100 Loss 0.2490 Accuracy 0.5524\n",
      "Epoch 77 Batch 1150 Loss 0.2507 Accuracy 0.5524\n",
      "Epoch 77 Batch 1200 Loss 0.2525 Accuracy 0.5523\n",
      "Epoch 77 Loss 0.2534 Accuracy 0.5522\n",
      "=================================================================\n",
      "Epoch 78 Batch 0 Loss 0.1831 Accuracy 0.5469\n",
      "Epoch 78 Batch 50 Loss 0.2158 Accuracy 0.5547\n",
      "Epoch 78 Batch 100 Loss 0.2116 Accuracy 0.5549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Batch 150 Loss 0.2144 Accuracy 0.5545\n",
      "Epoch 78 Batch 200 Loss 0.2172 Accuracy 0.5545\n",
      "Epoch 78 Batch 250 Loss 0.2191 Accuracy 0.5547\n",
      "Epoch 78 Batch 300 Loss 0.2209 Accuracy 0.5548\n",
      "Epoch 78 Batch 350 Loss 0.2232 Accuracy 0.5543\n",
      "Epoch 78 Batch 400 Loss 0.2239 Accuracy 0.5545\n",
      "Epoch 78 Batch 450 Loss 0.2246 Accuracy 0.5547\n",
      "Epoch 78 Batch 500 Loss 0.2258 Accuracy 0.5548\n",
      "Epoch 78 Batch 550 Loss 0.2276 Accuracy 0.5546\n",
      "Epoch 78 Batch 600 Loss 0.2296 Accuracy 0.5544\n",
      "Epoch 78 Batch 650 Loss 0.2325 Accuracy 0.5541\n",
      "Epoch 78 Batch 700 Loss 0.2334 Accuracy 0.5540\n",
      "Epoch 78 Batch 750 Loss 0.2343 Accuracy 0.5537\n",
      "Epoch 78 Batch 800 Loss 0.2367 Accuracy 0.5535\n",
      "Epoch 78 Batch 850 Loss 0.2382 Accuracy 0.5534\n",
      "Epoch 78 Batch 900 Loss 0.2412 Accuracy 0.5530\n",
      "Epoch 78 Batch 950 Loss 0.2417 Accuracy 0.5529\n",
      "Epoch 78 Batch 1000 Loss 0.2446 Accuracy 0.5526\n",
      "Epoch 78 Batch 1050 Loss 0.2466 Accuracy 0.5525\n",
      "Epoch 78 Batch 1100 Loss 0.2477 Accuracy 0.5525\n",
      "Epoch 78 Batch 1150 Loss 0.2492 Accuracy 0.5524\n",
      "Epoch 78 Batch 1200 Loss 0.2506 Accuracy 0.5524\n",
      "Epoch 78 Loss 0.2517 Accuracy 0.5523\n",
      "=================================================================\n",
      "Epoch 79 Batch 0 Loss 0.1969 Accuracy 0.5516\n",
      "Epoch 79 Batch 50 Loss 0.2307 Accuracy 0.5517\n",
      "Epoch 79 Batch 100 Loss 0.2229 Accuracy 0.5537\n",
      "Epoch 79 Batch 150 Loss 0.2242 Accuracy 0.5531\n",
      "Epoch 79 Batch 200 Loss 0.2231 Accuracy 0.5535\n",
      "Epoch 79 Batch 250 Loss 0.2229 Accuracy 0.5541\n",
      "Epoch 79 Batch 300 Loss 0.2245 Accuracy 0.5544\n",
      "Epoch 79 Batch 350 Loss 0.2255 Accuracy 0.5543\n",
      "Epoch 79 Batch 400 Loss 0.2279 Accuracy 0.5541\n",
      "Epoch 79 Batch 450 Loss 0.2297 Accuracy 0.5541\n",
      "Epoch 79 Batch 500 Loss 0.2324 Accuracy 0.5540\n",
      "Epoch 79 Batch 550 Loss 0.2339 Accuracy 0.5539\n",
      "Epoch 79 Batch 600 Loss 0.2367 Accuracy 0.5536\n",
      "Epoch 79 Batch 650 Loss 0.2373 Accuracy 0.5537\n",
      "Epoch 79 Batch 700 Loss 0.2379 Accuracy 0.5537\n",
      "Epoch 79 Batch 750 Loss 0.2398 Accuracy 0.5535\n",
      "Epoch 79 Batch 800 Loss 0.2408 Accuracy 0.5533\n",
      "Epoch 79 Batch 850 Loss 0.2420 Accuracy 0.5532\n",
      "Epoch 79 Batch 900 Loss 0.2435 Accuracy 0.5530\n",
      "Epoch 79 Batch 950 Loss 0.2455 Accuracy 0.5528\n",
      "Epoch 79 Batch 1000 Loss 0.2468 Accuracy 0.5528\n",
      "Epoch 79 Batch 1050 Loss 0.2482 Accuracy 0.5527\n",
      "Epoch 79 Batch 1100 Loss 0.2496 Accuracy 0.5525\n",
      "Epoch 79 Batch 1150 Loss 0.2512 Accuracy 0.5524\n",
      "Epoch 79 Batch 1200 Loss 0.2523 Accuracy 0.5523\n",
      "Epoch 79 Loss 0.2532 Accuracy 0.5522\n",
      "=================================================================\n",
      "Epoch 80 Batch 0 Loss 0.1980 Accuracy 0.5453\n",
      "Epoch 80 Batch 50 Loss 0.2204 Accuracy 0.5540\n",
      "Epoch 80 Batch 100 Loss 0.2161 Accuracy 0.5546\n",
      "Epoch 80 Batch 150 Loss 0.2182 Accuracy 0.5542\n",
      "Epoch 80 Batch 200 Loss 0.2194 Accuracy 0.5545\n",
      "Epoch 80 Batch 250 Loss 0.2190 Accuracy 0.5547\n",
      "Epoch 80 Batch 300 Loss 0.2200 Accuracy 0.5542\n",
      "Epoch 80 Batch 350 Loss 0.2201 Accuracy 0.5542\n",
      "Epoch 80 Batch 400 Loss 0.2207 Accuracy 0.5543\n",
      "Epoch 80 Batch 450 Loss 0.2237 Accuracy 0.5539\n",
      "Epoch 80 Batch 500 Loss 0.2247 Accuracy 0.5539\n",
      "Epoch 80 Batch 550 Loss 0.2271 Accuracy 0.5537\n",
      "Epoch 80 Batch 600 Loss 0.2292 Accuracy 0.5539\n",
      "Epoch 80 Batch 650 Loss 0.2308 Accuracy 0.5538\n",
      "Epoch 80 Batch 700 Loss 0.2322 Accuracy 0.5536\n",
      "Epoch 80 Batch 750 Loss 0.2339 Accuracy 0.5534\n",
      "Epoch 80 Batch 800 Loss 0.2354 Accuracy 0.5534\n",
      "Epoch 80 Batch 850 Loss 0.2367 Accuracy 0.5533\n",
      "Epoch 80 Batch 900 Loss 0.2382 Accuracy 0.5532\n",
      "Epoch 80 Batch 950 Loss 0.2399 Accuracy 0.5530\n",
      "Epoch 80 Batch 1000 Loss 0.2410 Accuracy 0.5529\n",
      "Epoch 80 Batch 1050 Loss 0.2425 Accuracy 0.5528\n",
      "Epoch 80 Batch 1100 Loss 0.2449 Accuracy 0.5527\n",
      "Epoch 80 Batch 1150 Loss 0.2467 Accuracy 0.5527\n",
      "Epoch 80 Batch 1200 Loss 0.2484 Accuracy 0.5526\n",
      "Epoch 80 Loss 0.2496 Accuracy 0.5525\n",
      "=================================================================\n",
      "Epoch 81 Batch 0 Loss 0.2774 Accuracy 0.5312\n",
      "Epoch 81 Batch 50 Loss 0.1987 Accuracy 0.5545\n",
      "Epoch 81 Batch 100 Loss 0.2010 Accuracy 0.5553\n",
      "Epoch 81 Batch 150 Loss 0.2137 Accuracy 0.5551\n",
      "Epoch 81 Batch 200 Loss 0.2154 Accuracy 0.5554\n",
      "Epoch 81 Batch 250 Loss 0.2166 Accuracy 0.5556\n",
      "Epoch 81 Batch 300 Loss 0.2200 Accuracy 0.5548\n",
      "Epoch 81 Batch 350 Loss 0.2258 Accuracy 0.5542\n",
      "Epoch 81 Batch 400 Loss 0.2259 Accuracy 0.5539\n",
      "Epoch 81 Batch 450 Loss 0.2274 Accuracy 0.5538\n",
      "Epoch 81 Batch 500 Loss 0.2274 Accuracy 0.5536\n",
      "Epoch 81 Batch 550 Loss 0.2285 Accuracy 0.5537\n",
      "Epoch 81 Batch 600 Loss 0.2300 Accuracy 0.5536\n",
      "Epoch 81 Batch 650 Loss 0.2316 Accuracy 0.5535\n",
      "Epoch 81 Batch 700 Loss 0.2353 Accuracy 0.5531\n",
      "Epoch 81 Batch 750 Loss 0.2367 Accuracy 0.5530\n",
      "Epoch 81 Batch 800 Loss 0.2384 Accuracy 0.5530\n",
      "Epoch 81 Batch 850 Loss 0.2407 Accuracy 0.5529\n",
      "Epoch 81 Batch 900 Loss 0.2412 Accuracy 0.5530\n",
      "Epoch 81 Batch 950 Loss 0.2427 Accuracy 0.5529\n",
      "Epoch 81 Batch 1000 Loss 0.2440 Accuracy 0.5530\n",
      "Epoch 81 Batch 1050 Loss 0.2449 Accuracy 0.5529\n",
      "Epoch 81 Batch 1100 Loss 0.2466 Accuracy 0.5528\n",
      "Epoch 81 Batch 1150 Loss 0.2475 Accuracy 0.5527\n",
      "Epoch 81 Batch 1200 Loss 0.2483 Accuracy 0.5527\n",
      "Epoch 81 Loss 0.2496 Accuracy 0.5527\n",
      "=================================================================\n",
      "Epoch 82 Batch 0 Loss 0.2155 Accuracy 0.5453\n",
      "Epoch 82 Batch 50 Loss 0.2105 Accuracy 0.5551\n",
      "Epoch 82 Batch 100 Loss 0.2129 Accuracy 0.5533\n",
      "Epoch 82 Batch 150 Loss 0.2167 Accuracy 0.5535\n",
      "Epoch 82 Batch 200 Loss 0.2174 Accuracy 0.5544\n",
      "Epoch 82 Batch 250 Loss 0.2162 Accuracy 0.5544\n",
      "Epoch 82 Batch 300 Loss 0.2220 Accuracy 0.5542\n",
      "Epoch 82 Batch 350 Loss 0.2238 Accuracy 0.5542\n",
      "Epoch 82 Batch 400 Loss 0.2262 Accuracy 0.5540\n",
      "Epoch 82 Batch 450 Loss 0.2280 Accuracy 0.5541\n",
      "Epoch 82 Batch 500 Loss 0.2311 Accuracy 0.5536\n",
      "Epoch 82 Batch 550 Loss 0.2327 Accuracy 0.5536\n",
      "Epoch 82 Batch 600 Loss 0.2335 Accuracy 0.5536\n",
      "Epoch 82 Batch 650 Loss 0.2343 Accuracy 0.5537\n",
      "Epoch 82 Batch 700 Loss 0.2364 Accuracy 0.5535\n",
      "Epoch 82 Batch 750 Loss 0.2384 Accuracy 0.5534\n",
      "Epoch 82 Batch 800 Loss 0.2406 Accuracy 0.5531\n",
      "Epoch 82 Batch 850 Loss 0.2427 Accuracy 0.5531\n",
      "Epoch 82 Batch 900 Loss 0.2440 Accuracy 0.5528\n",
      "Epoch 82 Batch 950 Loss 0.2458 Accuracy 0.5528\n",
      "Epoch 82 Batch 1000 Loss 0.2471 Accuracy 0.5526\n",
      "Epoch 82 Batch 1050 Loss 0.2483 Accuracy 0.5525\n",
      "Epoch 82 Batch 1100 Loss 0.2494 Accuracy 0.5525\n",
      "Epoch 82 Batch 1150 Loss 0.2504 Accuracy 0.5525\n",
      "Epoch 82 Batch 1200 Loss 0.2518 Accuracy 0.5523\n",
      "Epoch 82 Loss 0.2528 Accuracy 0.5523\n",
      "=================================================================\n",
      "Epoch 83 Batch 0 Loss 0.2397 Accuracy 0.5594\n",
      "Epoch 83 Batch 50 Loss 0.2005 Accuracy 0.5581\n",
      "Epoch 83 Batch 100 Loss 0.2008 Accuracy 0.5568\n",
      "Epoch 83 Batch 150 Loss 0.2076 Accuracy 0.5560\n",
      "Epoch 83 Batch 200 Loss 0.2106 Accuracy 0.5552\n",
      "Epoch 83 Batch 250 Loss 0.2102 Accuracy 0.5554\n",
      "Epoch 83 Batch 300 Loss 0.2143 Accuracy 0.5550\n",
      "Epoch 83 Batch 350 Loss 0.2168 Accuracy 0.5548\n",
      "Epoch 83 Batch 400 Loss 0.2202 Accuracy 0.5544\n",
      "Epoch 83 Batch 450 Loss 0.2236 Accuracy 0.5539\n",
      "Epoch 83 Batch 500 Loss 0.2250 Accuracy 0.5541\n",
      "Epoch 83 Batch 550 Loss 0.2274 Accuracy 0.5542\n",
      "Epoch 83 Batch 600 Loss 0.2276 Accuracy 0.5541\n",
      "Epoch 83 Batch 650 Loss 0.2289 Accuracy 0.5542\n",
      "Epoch 83 Batch 700 Loss 0.2315 Accuracy 0.5541\n",
      "Epoch 83 Batch 750 Loss 0.2348 Accuracy 0.5537\n",
      "Epoch 83 Batch 800 Loss 0.2359 Accuracy 0.5536\n",
      "Epoch 83 Batch 850 Loss 0.2360 Accuracy 0.5534\n",
      "Epoch 83 Batch 900 Loss 0.2378 Accuracy 0.5533\n",
      "Epoch 83 Batch 950 Loss 0.2394 Accuracy 0.5530\n",
      "Epoch 83 Batch 1000 Loss 0.2413 Accuracy 0.5530\n",
      "Epoch 83 Batch 1050 Loss 0.2420 Accuracy 0.5529\n",
      "Epoch 83 Batch 1100 Loss 0.2426 Accuracy 0.5529\n",
      "Epoch 83 Batch 1150 Loss 0.2438 Accuracy 0.5529\n",
      "Epoch 83 Batch 1200 Loss 0.2465 Accuracy 0.5527\n",
      "Epoch 83 Loss 0.2475 Accuracy 0.5527\n",
      "=================================================================\n",
      "Epoch 84 Batch 0 Loss 0.1099 Accuracy 0.5734\n",
      "Epoch 84 Batch 50 Loss 0.1938 Accuracy 0.5554\n",
      "Epoch 84 Batch 100 Loss 0.2054 Accuracy 0.5543\n",
      "Epoch 84 Batch 150 Loss 0.2076 Accuracy 0.5554\n",
      "Epoch 84 Batch 200 Loss 0.2110 Accuracy 0.5552\n",
      "Epoch 84 Batch 250 Loss 0.2144 Accuracy 0.5548\n",
      "Epoch 84 Batch 300 Loss 0.2147 Accuracy 0.5546\n",
      "Epoch 84 Batch 350 Loss 0.2172 Accuracy 0.5543\n",
      "Epoch 84 Batch 400 Loss 0.2192 Accuracy 0.5542\n",
      "Epoch 84 Batch 450 Loss 0.2225 Accuracy 0.5542\n",
      "Epoch 84 Batch 500 Loss 0.2227 Accuracy 0.5543\n",
      "Epoch 84 Batch 550 Loss 0.2254 Accuracy 0.5538\n",
      "Epoch 84 Batch 600 Loss 0.2277 Accuracy 0.5537\n",
      "Epoch 84 Batch 650 Loss 0.2292 Accuracy 0.5535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Batch 700 Loss 0.2318 Accuracy 0.5532\n",
      "Epoch 84 Batch 750 Loss 0.2343 Accuracy 0.5532\n",
      "Epoch 84 Batch 800 Loss 0.2363 Accuracy 0.5530\n",
      "Epoch 84 Batch 850 Loss 0.2379 Accuracy 0.5528\n",
      "Epoch 84 Batch 900 Loss 0.2390 Accuracy 0.5528\n",
      "Epoch 84 Batch 950 Loss 0.2409 Accuracy 0.5528\n",
      "Epoch 84 Batch 1000 Loss 0.2420 Accuracy 0.5530\n",
      "Epoch 84 Batch 1050 Loss 0.2433 Accuracy 0.5530\n",
      "Epoch 84 Batch 1100 Loss 0.2456 Accuracy 0.5529\n",
      "Epoch 84 Batch 1150 Loss 0.2476 Accuracy 0.5528\n",
      "Epoch 84 Batch 1200 Loss 0.2488 Accuracy 0.5527\n",
      "Epoch 84 Loss 0.2503 Accuracy 0.5527\n",
      "=================================================================\n",
      "Epoch 85 Batch 0 Loss 0.2231 Accuracy 0.5391\n",
      "Epoch 85 Batch 50 Loss 0.1907 Accuracy 0.5594\n",
      "Epoch 85 Batch 100 Loss 0.2113 Accuracy 0.5585\n",
      "Epoch 85 Batch 150 Loss 0.2143 Accuracy 0.5567\n",
      "Epoch 85 Batch 200 Loss 0.2154 Accuracy 0.5564\n",
      "Epoch 85 Batch 250 Loss 0.2137 Accuracy 0.5564\n",
      "Epoch 85 Batch 300 Loss 0.2143 Accuracy 0.5563\n",
      "Epoch 85 Batch 350 Loss 0.2168 Accuracy 0.5564\n",
      "Epoch 85 Batch 400 Loss 0.2179 Accuracy 0.5561\n",
      "Epoch 85 Batch 450 Loss 0.2188 Accuracy 0.5560\n",
      "Epoch 85 Batch 500 Loss 0.2215 Accuracy 0.5555\n",
      "Epoch 85 Batch 550 Loss 0.2229 Accuracy 0.5554\n",
      "Epoch 85 Batch 600 Loss 0.2245 Accuracy 0.5552\n",
      "Epoch 85 Batch 650 Loss 0.2274 Accuracy 0.5549\n",
      "Epoch 85 Batch 700 Loss 0.2294 Accuracy 0.5547\n",
      "Epoch 85 Batch 750 Loss 0.2325 Accuracy 0.5544\n",
      "Epoch 85 Batch 800 Loss 0.2336 Accuracy 0.5544\n",
      "Epoch 85 Batch 850 Loss 0.2357 Accuracy 0.5542\n",
      "Epoch 85 Batch 900 Loss 0.2379 Accuracy 0.5540\n",
      "Epoch 85 Batch 950 Loss 0.2398 Accuracy 0.5537\n",
      "Epoch 85 Batch 1000 Loss 0.2415 Accuracy 0.5536\n",
      "Epoch 85 Batch 1050 Loss 0.2424 Accuracy 0.5535\n",
      "Epoch 85 Batch 1100 Loss 0.2444 Accuracy 0.5535\n",
      "Epoch 85 Batch 1150 Loss 0.2457 Accuracy 0.5534\n",
      "Epoch 85 Batch 1200 Loss 0.2474 Accuracy 0.5531\n",
      "Epoch 85 Loss 0.2488 Accuracy 0.5529\n",
      "=================================================================\n",
      "Epoch 86 Batch 0 Loss 0.2509 Accuracy 0.5625\n",
      "Epoch 86 Batch 50 Loss 0.2097 Accuracy 0.5536\n",
      "Epoch 86 Batch 100 Loss 0.2062 Accuracy 0.5560\n",
      "Epoch 86 Batch 150 Loss 0.2060 Accuracy 0.5555\n",
      "Epoch 86 Batch 200 Loss 0.2102 Accuracy 0.5550\n",
      "Epoch 86 Batch 250 Loss 0.2156 Accuracy 0.5544\n",
      "Epoch 86 Batch 300 Loss 0.2171 Accuracy 0.5546\n",
      "Epoch 86 Batch 350 Loss 0.2185 Accuracy 0.5546\n",
      "Epoch 86 Batch 400 Loss 0.2207 Accuracy 0.5546\n",
      "Epoch 86 Batch 450 Loss 0.2232 Accuracy 0.5545\n",
      "Epoch 86 Batch 500 Loss 0.2256 Accuracy 0.5543\n",
      "Epoch 86 Batch 550 Loss 0.2269 Accuracy 0.5544\n",
      "Epoch 86 Batch 600 Loss 0.2293 Accuracy 0.5542\n",
      "Epoch 86 Batch 650 Loss 0.2307 Accuracy 0.5542\n",
      "Epoch 86 Batch 700 Loss 0.2320 Accuracy 0.5540\n",
      "Epoch 86 Batch 750 Loss 0.2334 Accuracy 0.5539\n",
      "Epoch 86 Batch 800 Loss 0.2354 Accuracy 0.5539\n",
      "Epoch 86 Batch 850 Loss 0.2379 Accuracy 0.5537\n",
      "Epoch 86 Batch 900 Loss 0.2393 Accuracy 0.5537\n",
      "Epoch 86 Batch 950 Loss 0.2411 Accuracy 0.5535\n",
      "Epoch 86 Batch 1000 Loss 0.2424 Accuracy 0.5534\n",
      "Epoch 86 Batch 1050 Loss 0.2438 Accuracy 0.5533\n",
      "Epoch 86 Batch 1100 Loss 0.2461 Accuracy 0.5531\n",
      "Epoch 86 Batch 1150 Loss 0.2475 Accuracy 0.5529\n",
      "Epoch 86 Batch 1200 Loss 0.2479 Accuracy 0.5530\n",
      "Epoch 86 Loss 0.2490 Accuracy 0.5528\n",
      "=================================================================\n",
      "Epoch 87 Batch 0 Loss 0.1839 Accuracy 0.5656\n",
      "Epoch 87 Batch 50 Loss 0.2042 Accuracy 0.5548\n",
      "Epoch 87 Batch 100 Loss 0.2030 Accuracy 0.5553\n",
      "Epoch 87 Batch 150 Loss 0.2051 Accuracy 0.5559\n",
      "Epoch 87 Batch 200 Loss 0.2076 Accuracy 0.5557\n",
      "Epoch 87 Batch 250 Loss 0.2107 Accuracy 0.5556\n",
      "Epoch 87 Batch 300 Loss 0.2136 Accuracy 0.5551\n",
      "Epoch 87 Batch 350 Loss 0.2150 Accuracy 0.5548\n",
      "Epoch 87 Batch 400 Loss 0.2183 Accuracy 0.5546\n",
      "Epoch 87 Batch 450 Loss 0.2190 Accuracy 0.5546\n",
      "Epoch 87 Batch 500 Loss 0.2228 Accuracy 0.5545\n",
      "Epoch 87 Batch 550 Loss 0.2247 Accuracy 0.5544\n",
      "Epoch 87 Batch 600 Loss 0.2269 Accuracy 0.5542\n",
      "Epoch 87 Batch 650 Loss 0.2280 Accuracy 0.5542\n",
      "Epoch 87 Batch 700 Loss 0.2307 Accuracy 0.5541\n",
      "Epoch 87 Batch 750 Loss 0.2337 Accuracy 0.5538\n",
      "Epoch 87 Batch 800 Loss 0.2360 Accuracy 0.5536\n",
      "Epoch 87 Batch 850 Loss 0.2378 Accuracy 0.5535\n",
      "Epoch 87 Batch 900 Loss 0.2392 Accuracy 0.5535\n",
      "Epoch 87 Batch 950 Loss 0.2406 Accuracy 0.5533\n",
      "Epoch 87 Batch 1000 Loss 0.2428 Accuracy 0.5531\n",
      "Epoch 87 Batch 1050 Loss 0.2444 Accuracy 0.5530\n",
      "Epoch 87 Batch 1100 Loss 0.2463 Accuracy 0.5529\n",
      "Epoch 87 Batch 1150 Loss 0.2480 Accuracy 0.5528\n",
      "Epoch 87 Batch 1200 Loss 0.2490 Accuracy 0.5527\n",
      "Epoch 87 Loss 0.2503 Accuracy 0.5526\n",
      "=================================================================\n",
      "Epoch 88 Batch 0 Loss 0.2009 Accuracy 0.5688\n",
      "Epoch 88 Batch 50 Loss 0.2074 Accuracy 0.5569\n",
      "Epoch 88 Batch 100 Loss 0.2020 Accuracy 0.5561\n",
      "Epoch 88 Batch 150 Loss 0.2043 Accuracy 0.5561\n",
      "Epoch 88 Batch 200 Loss 0.2068 Accuracy 0.5559\n",
      "Epoch 88 Batch 250 Loss 0.2092 Accuracy 0.5557\n",
      "Epoch 88 Batch 300 Loss 0.2105 Accuracy 0.5555\n",
      "Epoch 88 Batch 350 Loss 0.2127 Accuracy 0.5553\n",
      "Epoch 88 Batch 400 Loss 0.2176 Accuracy 0.5547\n",
      "Epoch 88 Batch 450 Loss 0.2192 Accuracy 0.5544\n",
      "Epoch 88 Batch 500 Loss 0.2230 Accuracy 0.5543\n",
      "Epoch 88 Batch 550 Loss 0.2240 Accuracy 0.5545\n",
      "Epoch 88 Batch 600 Loss 0.2272 Accuracy 0.5543\n",
      "Epoch 88 Batch 650 Loss 0.2288 Accuracy 0.5540\n",
      "Epoch 88 Batch 700 Loss 0.2313 Accuracy 0.5540\n",
      "Epoch 88 Batch 750 Loss 0.2333 Accuracy 0.5539\n",
      "Epoch 88 Batch 800 Loss 0.2352 Accuracy 0.5538\n",
      "Epoch 88 Batch 850 Loss 0.2371 Accuracy 0.5538\n",
      "Epoch 88 Batch 900 Loss 0.2392 Accuracy 0.5538\n",
      "Epoch 88 Batch 950 Loss 0.2408 Accuracy 0.5537\n",
      "Epoch 88 Batch 1000 Loss 0.2430 Accuracy 0.5534\n",
      "Epoch 88 Batch 1050 Loss 0.2445 Accuracy 0.5533\n",
      "Epoch 88 Batch 1100 Loss 0.2455 Accuracy 0.5533\n",
      "Epoch 88 Batch 1150 Loss 0.2466 Accuracy 0.5531\n",
      "Epoch 88 Batch 1200 Loss 0.2478 Accuracy 0.5530\n",
      "Epoch 88 Loss 0.2487 Accuracy 0.5529\n",
      "=================================================================\n",
      "Epoch 89 Batch 0 Loss 0.1215 Accuracy 0.5437\n",
      "Epoch 89 Batch 50 Loss 0.2052 Accuracy 0.5577\n",
      "Epoch 89 Batch 100 Loss 0.2010 Accuracy 0.5569\n",
      "Epoch 89 Batch 150 Loss 0.2034 Accuracy 0.5563\n",
      "Epoch 89 Batch 200 Loss 0.2093 Accuracy 0.5550\n",
      "Epoch 89 Batch 250 Loss 0.2110 Accuracy 0.5548\n",
      "Epoch 89 Batch 300 Loss 0.2145 Accuracy 0.5548\n",
      "Epoch 89 Batch 350 Loss 0.2188 Accuracy 0.5542\n",
      "Epoch 89 Batch 400 Loss 0.2201 Accuracy 0.5540\n",
      "Epoch 89 Batch 450 Loss 0.2213 Accuracy 0.5542\n",
      "Epoch 89 Batch 500 Loss 0.2242 Accuracy 0.5541\n",
      "Epoch 89 Batch 550 Loss 0.2253 Accuracy 0.5541\n",
      "Epoch 89 Batch 600 Loss 0.2291 Accuracy 0.5539\n",
      "Epoch 89 Batch 650 Loss 0.2305 Accuracy 0.5538\n",
      "Epoch 89 Batch 700 Loss 0.2314 Accuracy 0.5538\n",
      "Epoch 89 Batch 750 Loss 0.2334 Accuracy 0.5539\n",
      "Epoch 89 Batch 800 Loss 0.2353 Accuracy 0.5539\n",
      "Epoch 89 Batch 850 Loss 0.2373 Accuracy 0.5536\n",
      "Epoch 89 Batch 900 Loss 0.2394 Accuracy 0.5533\n",
      "Epoch 89 Batch 950 Loss 0.2412 Accuracy 0.5533\n",
      "Epoch 89 Batch 1000 Loss 0.2424 Accuracy 0.5533\n",
      "Epoch 89 Batch 1050 Loss 0.2437 Accuracy 0.5532\n",
      "Epoch 89 Batch 1100 Loss 0.2451 Accuracy 0.5531\n",
      "Epoch 89 Batch 1150 Loss 0.2466 Accuracy 0.5530\n",
      "Epoch 89 Batch 1200 Loss 0.2471 Accuracy 0.5529\n",
      "Epoch 89 Loss 0.2474 Accuracy 0.5529\n",
      "=================================================================\n",
      "Epoch 90 Batch 0 Loss 0.2554 Accuracy 0.5391\n",
      "Epoch 90 Batch 50 Loss 0.2062 Accuracy 0.5552\n",
      "Epoch 90 Batch 100 Loss 0.2026 Accuracy 0.5557\n",
      "Epoch 90 Batch 150 Loss 0.2046 Accuracy 0.5559\n",
      "Epoch 90 Batch 200 Loss 0.2118 Accuracy 0.5556\n",
      "Epoch 90 Batch 250 Loss 0.2125 Accuracy 0.5554\n",
      "Epoch 90 Batch 300 Loss 0.2133 Accuracy 0.5553\n",
      "Epoch 90 Batch 350 Loss 0.2160 Accuracy 0.5553\n",
      "Epoch 90 Batch 400 Loss 0.2181 Accuracy 0.5555\n",
      "Epoch 90 Batch 450 Loss 0.2202 Accuracy 0.5552\n",
      "Epoch 90 Batch 500 Loss 0.2247 Accuracy 0.5548\n",
      "Epoch 90 Batch 550 Loss 0.2257 Accuracy 0.5546\n",
      "Epoch 90 Batch 600 Loss 0.2288 Accuracy 0.5543\n",
      "Epoch 90 Batch 650 Loss 0.2307 Accuracy 0.5540\n",
      "Epoch 90 Batch 700 Loss 0.2325 Accuracy 0.5539\n",
      "Epoch 90 Batch 750 Loss 0.2341 Accuracy 0.5538\n",
      "Epoch 90 Batch 800 Loss 0.2363 Accuracy 0.5535\n",
      "Epoch 90 Batch 850 Loss 0.2378 Accuracy 0.5536\n",
      "Epoch 90 Batch 900 Loss 0.2399 Accuracy 0.5535\n",
      "Epoch 90 Batch 950 Loss 0.2412 Accuracy 0.5535\n",
      "Epoch 90 Batch 1000 Loss 0.2424 Accuracy 0.5534\n",
      "Epoch 90 Batch 1050 Loss 0.2446 Accuracy 0.5532\n",
      "Epoch 90 Batch 1100 Loss 0.2452 Accuracy 0.5532\n",
      "Epoch 90 Batch 1150 Loss 0.2469 Accuracy 0.5531\n",
      "Epoch 90 Batch 1200 Loss 0.2478 Accuracy 0.5532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Loss 0.2495 Accuracy 0.5531\n",
      "=================================================================\n",
      "Epoch 91 Batch 0 Loss 0.1748 Accuracy 0.5797\n",
      "Epoch 91 Batch 50 Loss 0.2035 Accuracy 0.5541\n",
      "Epoch 91 Batch 100 Loss 0.2132 Accuracy 0.5544\n",
      "Epoch 91 Batch 150 Loss 0.2154 Accuracy 0.5544\n",
      "Epoch 91 Batch 200 Loss 0.2136 Accuracy 0.5546\n",
      "Epoch 91 Batch 250 Loss 0.2162 Accuracy 0.5546\n",
      "Epoch 91 Batch 300 Loss 0.2170 Accuracy 0.5547\n",
      "Epoch 91 Batch 350 Loss 0.2209 Accuracy 0.5547\n",
      "Epoch 91 Batch 400 Loss 0.2218 Accuracy 0.5548\n",
      "Epoch 91 Batch 450 Loss 0.2215 Accuracy 0.5547\n",
      "Epoch 91 Batch 500 Loss 0.2235 Accuracy 0.5546\n",
      "Epoch 91 Batch 550 Loss 0.2254 Accuracy 0.5542\n",
      "Epoch 91 Batch 600 Loss 0.2260 Accuracy 0.5544\n",
      "Epoch 91 Batch 650 Loss 0.2272 Accuracy 0.5543\n",
      "Epoch 91 Batch 700 Loss 0.2295 Accuracy 0.5542\n",
      "Epoch 91 Batch 750 Loss 0.2308 Accuracy 0.5542\n",
      "Epoch 91 Batch 800 Loss 0.2328 Accuracy 0.5542\n",
      "Epoch 91 Batch 850 Loss 0.2346 Accuracy 0.5543\n",
      "Epoch 91 Batch 900 Loss 0.2366 Accuracy 0.5542\n",
      "Epoch 91 Batch 950 Loss 0.2384 Accuracy 0.5539\n",
      "Epoch 91 Batch 1000 Loss 0.2402 Accuracy 0.5536\n",
      "Epoch 91 Batch 1050 Loss 0.2426 Accuracy 0.5534\n",
      "Epoch 91 Batch 1100 Loss 0.2432 Accuracy 0.5534\n",
      "Epoch 91 Batch 1150 Loss 0.2449 Accuracy 0.5533\n",
      "Epoch 91 Batch 1200 Loss 0.2450 Accuracy 0.5534\n",
      "Epoch 91 Loss 0.2460 Accuracy 0.5532\n",
      "=================================================================\n",
      "Epoch 92 Batch 0 Loss 0.1073 Accuracy 0.5578\n",
      "Epoch 92 Batch 50 Loss 0.1828 Accuracy 0.5551\n",
      "Epoch 92 Batch 100 Loss 0.1995 Accuracy 0.5551\n",
      "Epoch 92 Batch 150 Loss 0.2068 Accuracy 0.5553\n",
      "Epoch 92 Batch 200 Loss 0.2099 Accuracy 0.5550\n",
      "Epoch 92 Batch 250 Loss 0.2115 Accuracy 0.5547\n",
      "Epoch 92 Batch 300 Loss 0.2110 Accuracy 0.5545\n",
      "Epoch 92 Batch 350 Loss 0.2136 Accuracy 0.5546\n",
      "Epoch 92 Batch 400 Loss 0.2142 Accuracy 0.5547\n",
      "Epoch 92 Batch 450 Loss 0.2175 Accuracy 0.5545\n",
      "Epoch 92 Batch 500 Loss 0.2187 Accuracy 0.5546\n",
      "Epoch 92 Batch 550 Loss 0.2196 Accuracy 0.5545\n",
      "Epoch 92 Batch 600 Loss 0.2218 Accuracy 0.5546\n",
      "Epoch 92 Batch 650 Loss 0.2249 Accuracy 0.5546\n",
      "Epoch 92 Batch 700 Loss 0.2265 Accuracy 0.5545\n",
      "Epoch 92 Batch 750 Loss 0.2296 Accuracy 0.5543\n",
      "Epoch 92 Batch 800 Loss 0.2307 Accuracy 0.5542\n",
      "Epoch 92 Batch 850 Loss 0.2335 Accuracy 0.5541\n",
      "Epoch 92 Batch 900 Loss 0.2353 Accuracy 0.5540\n",
      "Epoch 92 Batch 950 Loss 0.2372 Accuracy 0.5538\n",
      "Epoch 92 Batch 1000 Loss 0.2386 Accuracy 0.5537\n",
      "Epoch 92 Batch 1050 Loss 0.2397 Accuracy 0.5537\n",
      "Epoch 92 Batch 1100 Loss 0.2407 Accuracy 0.5536\n",
      "Epoch 92 Batch 1150 Loss 0.2425 Accuracy 0.5535\n",
      "Epoch 92 Batch 1200 Loss 0.2431 Accuracy 0.5535\n",
      "Epoch 92 Loss 0.2447 Accuracy 0.5534\n",
      "=================================================================\n",
      "Epoch 93 Batch 0 Loss 0.1612 Accuracy 0.5891\n",
      "Epoch 93 Batch 50 Loss 0.1986 Accuracy 0.5555\n",
      "Epoch 93 Batch 100 Loss 0.2095 Accuracy 0.5556\n",
      "Epoch 93 Batch 150 Loss 0.2090 Accuracy 0.5558\n",
      "Epoch 93 Batch 200 Loss 0.2079 Accuracy 0.5559\n",
      "Epoch 93 Batch 250 Loss 0.2107 Accuracy 0.5556\n",
      "Epoch 93 Batch 300 Loss 0.2113 Accuracy 0.5560\n",
      "Epoch 93 Batch 350 Loss 0.2129 Accuracy 0.5558\n",
      "Epoch 93 Batch 400 Loss 0.2152 Accuracy 0.5559\n",
      "Epoch 93 Batch 450 Loss 0.2180 Accuracy 0.5558\n",
      "Epoch 93 Batch 500 Loss 0.2187 Accuracy 0.5555\n",
      "Epoch 93 Batch 550 Loss 0.2207 Accuracy 0.5554\n",
      "Epoch 93 Batch 600 Loss 0.2218 Accuracy 0.5553\n",
      "Epoch 93 Batch 650 Loss 0.2242 Accuracy 0.5548\n",
      "Epoch 93 Batch 700 Loss 0.2273 Accuracy 0.5546\n",
      "Epoch 93 Batch 750 Loss 0.2303 Accuracy 0.5543\n",
      "Epoch 93 Batch 800 Loss 0.2309 Accuracy 0.5543\n",
      "Epoch 93 Batch 850 Loss 0.2336 Accuracy 0.5542\n",
      "Epoch 93 Batch 900 Loss 0.2359 Accuracy 0.5540\n",
      "Epoch 93 Batch 950 Loss 0.2375 Accuracy 0.5539\n",
      "Epoch 93 Batch 1000 Loss 0.2393 Accuracy 0.5537\n",
      "Epoch 93 Batch 1050 Loss 0.2404 Accuracy 0.5536\n",
      "Epoch 93 Batch 1100 Loss 0.2421 Accuracy 0.5535\n",
      "Epoch 93 Batch 1150 Loss 0.2442 Accuracy 0.5534\n",
      "Epoch 93 Batch 1200 Loss 0.2455 Accuracy 0.5533\n",
      "Epoch 93 Loss 0.2470 Accuracy 0.5531\n",
      "=================================================================\n",
      "Epoch 94 Batch 0 Loss 0.2045 Accuracy 0.5547\n",
      "Epoch 94 Batch 50 Loss 0.2035 Accuracy 0.5537\n",
      "Epoch 94 Batch 100 Loss 0.2024 Accuracy 0.5550\n",
      "Epoch 94 Batch 150 Loss 0.2047 Accuracy 0.5555\n",
      "Epoch 94 Batch 200 Loss 0.2048 Accuracy 0.5559\n",
      "Epoch 94 Batch 250 Loss 0.2049 Accuracy 0.5563\n",
      "Epoch 94 Batch 300 Loss 0.2073 Accuracy 0.5564\n",
      "Epoch 94 Batch 350 Loss 0.2094 Accuracy 0.5564\n",
      "Epoch 94 Batch 400 Loss 0.2129 Accuracy 0.5565\n",
      "Epoch 94 Batch 450 Loss 0.2141 Accuracy 0.5561\n",
      "Epoch 94 Batch 500 Loss 0.2165 Accuracy 0.5560\n",
      "Epoch 94 Batch 550 Loss 0.2202 Accuracy 0.5556\n",
      "Epoch 94 Batch 600 Loss 0.2226 Accuracy 0.5556\n",
      "Epoch 94 Batch 650 Loss 0.2242 Accuracy 0.5554\n",
      "Epoch 94 Batch 700 Loss 0.2259 Accuracy 0.5552\n",
      "Epoch 94 Batch 750 Loss 0.2287 Accuracy 0.5548\n",
      "Epoch 94 Batch 800 Loss 0.2299 Accuracy 0.5546\n",
      "Epoch 94 Batch 850 Loss 0.2323 Accuracy 0.5544\n",
      "Epoch 94 Batch 900 Loss 0.2346 Accuracy 0.5543\n",
      "Epoch 94 Batch 950 Loss 0.2366 Accuracy 0.5542\n",
      "Epoch 94 Batch 1000 Loss 0.2385 Accuracy 0.5540\n",
      "Epoch 94 Batch 1050 Loss 0.2408 Accuracy 0.5539\n",
      "Epoch 94 Batch 1100 Loss 0.2431 Accuracy 0.5537\n",
      "Epoch 94 Batch 1150 Loss 0.2444 Accuracy 0.5536\n",
      "Epoch 94 Batch 1200 Loss 0.2459 Accuracy 0.5535\n",
      "Epoch 94 Loss 0.2478 Accuracy 0.5534\n",
      "=================================================================\n",
      "Epoch 95 Batch 0 Loss 0.1951 Accuracy 0.5531\n",
      "Epoch 95 Batch 50 Loss 0.2031 Accuracy 0.5544\n",
      "Epoch 95 Batch 100 Loss 0.2080 Accuracy 0.5544\n",
      "Epoch 95 Batch 150 Loss 0.2093 Accuracy 0.5547\n",
      "Epoch 95 Batch 200 Loss 0.2079 Accuracy 0.5549\n",
      "Epoch 95 Batch 250 Loss 0.2109 Accuracy 0.5549\n",
      "Epoch 95 Batch 300 Loss 0.2105 Accuracy 0.5550\n",
      "Epoch 95 Batch 350 Loss 0.2139 Accuracy 0.5552\n",
      "Epoch 95 Batch 400 Loss 0.2183 Accuracy 0.5547\n",
      "Epoch 95 Batch 450 Loss 0.2184 Accuracy 0.5547\n",
      "Epoch 95 Batch 500 Loss 0.2225 Accuracy 0.5546\n",
      "Epoch 95 Batch 550 Loss 0.2235 Accuracy 0.5545\n",
      "Epoch 95 Batch 600 Loss 0.2261 Accuracy 0.5543\n",
      "Epoch 95 Batch 650 Loss 0.2270 Accuracy 0.5544\n",
      "Epoch 95 Batch 700 Loss 0.2298 Accuracy 0.5543\n",
      "Epoch 95 Batch 750 Loss 0.2318 Accuracy 0.5541\n",
      "Epoch 95 Batch 800 Loss 0.2344 Accuracy 0.5540\n",
      "Epoch 95 Batch 850 Loss 0.2361 Accuracy 0.5539\n",
      "Epoch 95 Batch 900 Loss 0.2376 Accuracy 0.5538\n",
      "Epoch 95 Batch 950 Loss 0.2394 Accuracy 0.5536\n",
      "Epoch 95 Batch 1000 Loss 0.2414 Accuracy 0.5535\n",
      "Epoch 95 Batch 1050 Loss 0.2428 Accuracy 0.5535\n",
      "Epoch 95 Batch 1100 Loss 0.2453 Accuracy 0.5534\n",
      "Epoch 95 Batch 1150 Loss 0.2464 Accuracy 0.5533\n",
      "Epoch 95 Batch 1200 Loss 0.2482 Accuracy 0.5532\n",
      "Epoch 95 Loss 0.2493 Accuracy 0.5532\n",
      "=================================================================\n",
      "Epoch 96 Batch 0 Loss 0.2474 Accuracy 0.5500\n",
      "Epoch 96 Batch 50 Loss 0.2022 Accuracy 0.5567\n",
      "Epoch 96 Batch 100 Loss 0.2097 Accuracy 0.5557\n",
      "Epoch 96 Batch 150 Loss 0.2108 Accuracy 0.5558\n",
      "Epoch 96 Batch 200 Loss 0.2153 Accuracy 0.5551\n",
      "Epoch 96 Batch 250 Loss 0.2138 Accuracy 0.5555\n",
      "Epoch 96 Batch 300 Loss 0.2159 Accuracy 0.5553\n",
      "Epoch 96 Batch 350 Loss 0.2164 Accuracy 0.5551\n",
      "Epoch 96 Batch 400 Loss 0.2185 Accuracy 0.5548\n",
      "Epoch 96 Batch 450 Loss 0.2206 Accuracy 0.5546\n",
      "Epoch 96 Batch 500 Loss 0.2238 Accuracy 0.5542\n",
      "Epoch 96 Batch 550 Loss 0.2254 Accuracy 0.5541\n",
      "Epoch 96 Batch 600 Loss 0.2288 Accuracy 0.5540\n",
      "Epoch 96 Batch 650 Loss 0.2307 Accuracy 0.5537\n",
      "Epoch 96 Batch 700 Loss 0.2310 Accuracy 0.5539\n",
      "Epoch 96 Batch 750 Loss 0.2330 Accuracy 0.5537\n",
      "Epoch 96 Batch 800 Loss 0.2338 Accuracy 0.5537\n",
      "Epoch 96 Batch 850 Loss 0.2359 Accuracy 0.5537\n",
      "Epoch 96 Batch 900 Loss 0.2386 Accuracy 0.5538\n",
      "Epoch 96 Batch 950 Loss 0.2406 Accuracy 0.5536\n",
      "Epoch 96 Batch 1000 Loss 0.2421 Accuracy 0.5536\n",
      "Epoch 96 Batch 1050 Loss 0.2435 Accuracy 0.5535\n",
      "Epoch 96 Batch 1100 Loss 0.2453 Accuracy 0.5535\n",
      "Epoch 96 Batch 1150 Loss 0.2474 Accuracy 0.5534\n",
      "Epoch 96 Batch 1200 Loss 0.2484 Accuracy 0.5534\n",
      "Epoch 96 Loss 0.2493 Accuracy 0.5532\n",
      "=================================================================\n",
      "Epoch 97 Batch 0 Loss 0.3478 Accuracy 0.5437\n",
      "Epoch 97 Batch 50 Loss 0.1877 Accuracy 0.5564\n",
      "Epoch 97 Batch 100 Loss 0.1880 Accuracy 0.5573\n",
      "Epoch 97 Batch 150 Loss 0.1943 Accuracy 0.5569\n",
      "Epoch 97 Batch 200 Loss 0.1989 Accuracy 0.5563\n",
      "Epoch 97 Batch 250 Loss 0.2028 Accuracy 0.5559\n",
      "Epoch 97 Batch 300 Loss 0.2049 Accuracy 0.5557\n",
      "Epoch 97 Batch 350 Loss 0.2093 Accuracy 0.5551\n",
      "Epoch 97 Batch 400 Loss 0.2118 Accuracy 0.5548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Batch 450 Loss 0.2145 Accuracy 0.5545\n",
      "Epoch 97 Batch 500 Loss 0.2170 Accuracy 0.5543\n",
      "Epoch 97 Batch 550 Loss 0.2176 Accuracy 0.5545\n",
      "Epoch 97 Batch 600 Loss 0.2201 Accuracy 0.5545\n",
      "Epoch 97 Batch 650 Loss 0.2225 Accuracy 0.5545\n",
      "Epoch 97 Batch 700 Loss 0.2259 Accuracy 0.5542\n",
      "Epoch 97 Batch 750 Loss 0.2293 Accuracy 0.5541\n",
      "Epoch 97 Batch 800 Loss 0.2319 Accuracy 0.5540\n",
      "Epoch 97 Batch 850 Loss 0.2348 Accuracy 0.5539\n",
      "Epoch 97 Batch 900 Loss 0.2369 Accuracy 0.5538\n",
      "Epoch 97 Batch 950 Loss 0.2382 Accuracy 0.5537\n",
      "Epoch 97 Batch 1000 Loss 0.2403 Accuracy 0.5538\n",
      "Epoch 97 Batch 1050 Loss 0.2410 Accuracy 0.5537\n",
      "Epoch 97 Batch 1100 Loss 0.2429 Accuracy 0.5536\n",
      "Epoch 97 Batch 1150 Loss 0.2441 Accuracy 0.5535\n",
      "Epoch 97 Batch 1200 Loss 0.2459 Accuracy 0.5533\n",
      "Epoch 97 Loss 0.2462 Accuracy 0.5533\n",
      "=================================================================\n",
      "Epoch 98 Batch 0 Loss 0.2325 Accuracy 0.5469\n",
      "Epoch 98 Batch 50 Loss 0.2104 Accuracy 0.5565\n",
      "Epoch 98 Batch 100 Loss 0.2126 Accuracy 0.5560\n",
      "Epoch 98 Batch 150 Loss 0.2149 Accuracy 0.5553\n",
      "Epoch 98 Batch 200 Loss 0.2160 Accuracy 0.5556\n",
      "Epoch 98 Batch 250 Loss 0.2165 Accuracy 0.5555\n",
      "Epoch 98 Batch 300 Loss 0.2162 Accuracy 0.5556\n",
      "Epoch 98 Batch 350 Loss 0.2177 Accuracy 0.5551\n",
      "Epoch 98 Batch 400 Loss 0.2180 Accuracy 0.5547\n",
      "Epoch 98 Batch 450 Loss 0.2220 Accuracy 0.5544\n",
      "Epoch 98 Batch 500 Loss 0.2236 Accuracy 0.5542\n",
      "Epoch 98 Batch 550 Loss 0.2257 Accuracy 0.5543\n",
      "Epoch 98 Batch 600 Loss 0.2269 Accuracy 0.5546\n",
      "Epoch 98 Batch 650 Loss 0.2280 Accuracy 0.5545\n",
      "Epoch 98 Batch 700 Loss 0.2304 Accuracy 0.5544\n",
      "Epoch 98 Batch 750 Loss 0.2331 Accuracy 0.5543\n",
      "Epoch 98 Batch 800 Loss 0.2349 Accuracy 0.5542\n",
      "Epoch 98 Batch 850 Loss 0.2375 Accuracy 0.5540\n",
      "Epoch 98 Batch 900 Loss 0.2385 Accuracy 0.5539\n",
      "Epoch 98 Batch 950 Loss 0.2406 Accuracy 0.5538\n",
      "Epoch 98 Batch 1000 Loss 0.2416 Accuracy 0.5536\n",
      "Epoch 98 Batch 1050 Loss 0.2424 Accuracy 0.5536\n",
      "Epoch 98 Batch 1100 Loss 0.2433 Accuracy 0.5535\n",
      "Epoch 98 Batch 1150 Loss 0.2452 Accuracy 0.5535\n",
      "Epoch 98 Batch 1200 Loss 0.2462 Accuracy 0.5535\n",
      "Epoch 98 Loss 0.2477 Accuracy 0.5534\n",
      "=================================================================\n",
      "Epoch 99 Batch 0 Loss 0.1217 Accuracy 0.5688\n",
      "Epoch 99 Batch 50 Loss 0.1920 Accuracy 0.5585\n",
      "Epoch 99 Batch 100 Loss 0.1936 Accuracy 0.5585\n",
      "Epoch 99 Batch 150 Loss 0.2016 Accuracy 0.5577\n",
      "Epoch 99 Batch 200 Loss 0.2042 Accuracy 0.5569\n",
      "Epoch 99 Batch 250 Loss 0.2071 Accuracy 0.5567\n",
      "Epoch 99 Batch 300 Loss 0.2095 Accuracy 0.5566\n",
      "Epoch 99 Batch 350 Loss 0.2116 Accuracy 0.5563\n",
      "Epoch 99 Batch 400 Loss 0.2131 Accuracy 0.5560\n",
      "Epoch 99 Batch 450 Loss 0.2152 Accuracy 0.5559\n",
      "Epoch 99 Batch 500 Loss 0.2190 Accuracy 0.5557\n",
      "Epoch 99 Batch 550 Loss 0.2209 Accuracy 0.5556\n",
      "Epoch 99 Batch 600 Loss 0.2233 Accuracy 0.5554\n",
      "Epoch 99 Batch 650 Loss 0.2257 Accuracy 0.5553\n",
      "Epoch 99 Batch 700 Loss 0.2311 Accuracy 0.5547\n",
      "Epoch 99 Batch 750 Loss 0.2334 Accuracy 0.5546\n",
      "Epoch 99 Batch 800 Loss 0.2353 Accuracy 0.5544\n",
      "Epoch 99 Batch 850 Loss 0.2366 Accuracy 0.5544\n",
      "Epoch 99 Batch 900 Loss 0.2384 Accuracy 0.5542\n",
      "Epoch 99 Batch 950 Loss 0.2403 Accuracy 0.5540\n",
      "Epoch 99 Batch 1000 Loss 0.2414 Accuracy 0.5537\n",
      "Epoch 99 Batch 1050 Loss 0.2428 Accuracy 0.5536\n",
      "Epoch 99 Batch 1100 Loss 0.2440 Accuracy 0.5535\n",
      "Epoch 99 Batch 1150 Loss 0.2447 Accuracy 0.5536\n",
      "Epoch 99 Batch 1200 Loss 0.2458 Accuracy 0.5536\n",
      "Epoch 99 Loss 0.2478 Accuracy 0.5533\n",
      "=================================================================\n",
      "Epoch 100 Batch 0 Loss 0.2709 Accuracy 0.5562\n",
      "Epoch 100 Batch 50 Loss 0.2054 Accuracy 0.5582\n",
      "Epoch 100 Batch 100 Loss 0.2054 Accuracy 0.5569\n",
      "Epoch 100 Batch 150 Loss 0.2032 Accuracy 0.5575\n",
      "Epoch 100 Batch 200 Loss 0.2054 Accuracy 0.5571\n",
      "Epoch 100 Batch 250 Loss 0.2070 Accuracy 0.5564\n",
      "Epoch 100 Batch 300 Loss 0.2097 Accuracy 0.5561\n",
      "Epoch 100 Batch 350 Loss 0.2130 Accuracy 0.5556\n",
      "Epoch 100 Batch 400 Loss 0.2165 Accuracy 0.5555\n",
      "Epoch 100 Batch 450 Loss 0.2182 Accuracy 0.5555\n",
      "Epoch 100 Batch 500 Loss 0.2209 Accuracy 0.5553\n",
      "Epoch 100 Batch 550 Loss 0.2226 Accuracy 0.5555\n",
      "Epoch 100 Batch 600 Loss 0.2252 Accuracy 0.5553\n",
      "Epoch 100 Batch 650 Loss 0.2270 Accuracy 0.5553\n",
      "Epoch 100 Batch 700 Loss 0.2298 Accuracy 0.5549\n",
      "Epoch 100 Batch 750 Loss 0.2319 Accuracy 0.5547\n",
      "Epoch 100 Batch 800 Loss 0.2339 Accuracy 0.5545\n",
      "Epoch 100 Batch 850 Loss 0.2357 Accuracy 0.5543\n",
      "Epoch 100 Batch 900 Loss 0.2374 Accuracy 0.5542\n",
      "Epoch 100 Batch 950 Loss 0.2390 Accuracy 0.5540\n",
      "Epoch 100 Batch 1000 Loss 0.2400 Accuracy 0.5542\n",
      "Epoch 100 Batch 1050 Loss 0.2409 Accuracy 0.5541\n",
      "Epoch 100 Batch 1100 Loss 0.2427 Accuracy 0.5539\n",
      "Epoch 100 Batch 1150 Loss 0.2440 Accuracy 0.5538\n",
      "Epoch 100 Batch 1200 Loss 0.2452 Accuracy 0.5538\n",
      "Epoch 100 Loss 0.2462 Accuracy 0.5537\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_info_to_plot=[]\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "            loss_info_to_plot.append(train_loss.result())            \n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "    print('=================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(loss_info_to_plot)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Попробуй сделать это.\n",
      "Predicted translation: ['<start>', 'try', 'to', 'do', 'that', '.']\n"
     ]
    }
   ],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [1]\n",
    "    end_token = [2]\n",
    "  \n",
    "    sentence = preprocess_sentence(inp_sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    encoder_input = tf.expand_dims(inputs, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "    decoder_input = [1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "\n",
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "    sentence = inp_lang_tokenizer.encode(sentence)\n",
    "  \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "translate(u'Попробуй сделать это.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3LLCx3ZE0Ls",
    "outputId": "b64aa087-8232-474e-e3c7-98c186081845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Вы все еще дома?\n",
      "Predicted translation: ['<start>', 'are', 'you', 'still', 'at', 'home', '?']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Вы все еще дома?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUQVLVqUE1YW",
    "outputId": "b13a0911-5243-463c-ac2b-5381fe027c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Попробуй сделать это.\n",
      "Predicted translation: ['<start>', 'try', 'to', 'do', 'that', '.']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Попробуй сделать это.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f09_hUFx9EJh",
    "outputId": "c23aaff7-a432-45f3-f15e-4cfa2b5d74a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Я люблю, когда идет снег.\n",
      "Predicted translation: ['<start>', 'i', 'like', 'snow', 'when', '.']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Я люблю, когда идет снег.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7c5p8rmkHQG",
    "outputId": "d4682d71-f778-41f5-e4a9-2e1235976123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Я никогда такого не делаю.\n",
      "Predicted translation: ['<start>', 'i', 'never', 'do', 'that', '.']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Я никогда такого не делаю.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
