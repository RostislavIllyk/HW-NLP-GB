{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cINqgGpKXURp"
   },
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUOg4C8sZNpw"
   },
   "source": [
    "мы будем использовать данные http://www.labinform.ru/pub/named_entities/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzi6ApNLZg6X"
   },
   "source": [
    "**Проверить насколько хорошо работает NER**\n",
    "\n",
    "1. взять нер из nltk\n",
    "2. проверить deeppavlov\n",
    "3. написать свой нер попробовать разные подходы (с доп информацией без) так же с учётом соседей и без них\n",
    "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP1LgaNUtaOz"
   },
   "source": [
    "при обучении своего нера незабудьте разделить выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1622593697743,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "hrc5ocDkaS1e"
   },
   "outputs": [],
   "source": [
    "import corus\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прочтем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1972,
     "status": "ok",
     "timestamp": 1622593709720,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "vPVv6lhT3ftA",
    "outputId": "ae2f6c07-afc5-4093-b843-eed25d6d749b"
   },
   "outputs": [],
   "source": [
    "#!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1622593849417,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "UEdS2pAS3fod",
    "outputId": "402714b1-6931-41ce-ef39-f68080d9a29e",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ne5Markup(\n",
       "    id='001',\n",
       "    text='Россия рассчитывает на конструктивное воздействие США на Грузию\\r\\n\\r\\n04/08/2008 12:08\\r\\n\\r\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\r\\n\\r\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\r\\n\\r\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ',\n",
       "    spans=[Ne5Span(\n",
       "         index='T1',\n",
       "         type='GEOPOLIT',\n",
       "         start=0,\n",
       "         stop=6,\n",
       "         text='Россия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T2',\n",
       "         type='GEOPOLIT',\n",
       "         start=50,\n",
       "         stop=53,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T3',\n",
       "         type='GEOPOLIT',\n",
       "         start=57,\n",
       "         stop=63,\n",
       "         text='Грузию'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T4',\n",
       "         type='LOC',\n",
       "         start=87,\n",
       "         stop=93,\n",
       "         text='МОСКВА'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T5',\n",
       "         type='MEDIA',\n",
       "         start=103,\n",
       "         stop=114,\n",
       "         text='РИА Новости'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T6',\n",
       "         type='GEOPOLIT',\n",
       "         start=116,\n",
       "         stop=122,\n",
       "         text='Россия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T7',\n",
       "         type='GEOPOLIT',\n",
       "         start=141,\n",
       "         stop=144,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T8',\n",
       "         type='GEOPOLIT',\n",
       "         start=161,\n",
       "         stop=168,\n",
       "         text='Тбилиси'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T9',\n",
       "         type='GEOPOLIT',\n",
       "         start=301,\n",
       "         stop=307,\n",
       "         text='России'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T10',\n",
       "         type='PER',\n",
       "         start=308,\n",
       "         stop=324,\n",
       "         text='Григорий Карасин'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T11',\n",
       "         type='GEOPOLIT',\n",
       "         start=383,\n",
       "         stop=386,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T12',\n",
       "         type='PER',\n",
       "         start=387,\n",
       "         stop=402,\n",
       "         text='Дэниэлом Фридом'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T13',\n",
       "         type='GEOPOLIT',\n",
       "         start=505,\n",
       "         stop=517,\n",
       "         text='Южной Осетии'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T14',\n",
       "         type='GEOPOLIT',\n",
       "         start=703,\n",
       "         stop=709,\n",
       "         text='Россия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T15',\n",
       "         type='GEOPOLIT',\n",
       "         start=723,\n",
       "         stop=730,\n",
       "         text='Тбилиси'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T16',\n",
       "         type='GEOPOLIT',\n",
       "         start=815,\n",
       "         stop=825,\n",
       "         text='Вашингтона'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T17',\n",
       "         type='ORG',\n",
       "         start=838,\n",
       "         stop=841,\n",
       "         text='МИД'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T18',\n",
       "         type='GEOPOLIT',\n",
       "         start=842,\n",
       "         stop=848,\n",
       "         text='России'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from corus import load_ne5\n",
    "\n",
    "# !!! load_ne5 на моей машине (под Windows) по умолчанию не работал. Пришлось лезть \n",
    "# во внутрь и добавить в строку где читается файл кодировку   \"utf-8\".\n",
    "\n",
    "\n",
    "path = './Collection5/'\n",
    "records = load_ne5(path)\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Воспользуемся NER из nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = next(records).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\\r\\n\\r\\n05/08/2008 10:32\\r\\n\\r\\nМОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\\r\\n\\r\\n\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\\r\\n\\r\\nПо сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\\r\\n\\r\\nКомиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\\r\\n\\r\\n\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\\r\\n\\r\\nКомиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Thomas Hammarberg', 'PERSON'),\n",
       " ('Комиссар', 'PERSON'),\n",
       " ('МОСКВА', 'ORGANIZATION'),\n",
       " ('РИА Новости', 'ORGANIZATION'),\n",
       " ('СЕ', 'ORGANIZATION'),\n",
       " ('Совета Европы', 'PERSON'),\n",
       " ('Хаммарберг', 'PERSON')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. deeppavlov - ужастная программа никак и нигде не хотела запускаться... \n",
    "\n",
    "__Смотрите отдельный ноутбучек.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Попробуем написать свой нер попробовать разные подходы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uRuODJpkIqlv"
   },
   "outputs": [],
   "source": [
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XDdnL6EXJRt9"
   },
   "outputs": [],
   "source": [
    "records = corus.load_ne5(path)\n",
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "       \n",
    "        result = 'None'        \n",
    "        \n",
    "        for item in rec.spans:            \n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'PER'):\n",
    "                result = 'PER'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'ORG'):\n",
    "                result = 'ORG'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'MEDIA'):\n",
    "                result = 'MEDIA'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'LOC'):\n",
    "                result = 'LOC'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'GEOPOLIT'):\n",
    "                result = 'GEOPOLIT'\n",
    "                break\n",
    "                \n",
    "    \n",
    "        words.append([token.text, result])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skYaNCiC5xM4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None        219214\n",
       "PER          21200\n",
       "ORG          13651\n",
       "LOC           4568\n",
       "GEOPOLIT      4356\n",
       "MEDIA         2482\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Россия</td>\n",
       "      <td>GEOPOLIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>рассчитывает</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>на</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>конструктивное</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>воздействие</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word       tag\n",
       "0          Россия  GEOPOLIT\n",
       "1    рассчитывает      None\n",
       "2              на      None\n",
       "3  конструктивное      None\n",
       "4     воздействие      None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_standardization(input_data):\n",
    "        return input_data\n",
    "\n",
    "def data_prep(train_data, seq_len=1, vocab_size = 30000):    \n",
    "    \n",
    "    vocab_size = 30000\n",
    "    #seq_len = 1\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=seq_len)\n",
    "\n",
    "\n",
    "    # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "    text_data = train_data.map(lambda x, y: x)\n",
    "    vectorize_layer.adapt(text_data)\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.fc2 = Dense(50, activation='relu')\n",
    "        self.fc3 = Dense(len(df_words['tag'].value_counts()), activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        return self.fc3(concat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12444/12444 [==============================] - 175s 14ms/step - loss: 0.2965 - accuracy: 0.9120 - val_loss: 0.2312 - val_accuracy: 0.9292\n",
      "Epoch 2/5\n",
      "12444/12444 [==============================] - 175s 14ms/step - loss: 0.1414 - accuracy: 0.9556 - val_loss: 0.2486 - val_accuracy: 0.8850\n",
      "Epoch 3/5\n",
      "12444/12444 [==============================] - 177s 14ms/step - loss: 0.1273 - accuracy: 0.9582 - val_loss: 0.2372 - val_accuracy: 0.9307\n",
      "Epoch 4/5\n",
      "12444/12444 [==============================] - 177s 14ms/step - loss: 0.1223 - accuracy: 0.9588 - val_loss: 0.2667 - val_accuracy: 0.9262\n",
      "Epoch 5/5\n",
      "12444/12444 [==============================] - 175s 14ms/step - loss: 0.1200 - accuracy: 0.9594 - val_loss: 0.2871 - val_accuracy: 0.9049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x247f4c50cd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "vectorize_layer = data_prep(train_data, seq_len = 1, vocab_size = vocab_size)\n",
    "\n",
    "\n",
    "mmodel = modelNER()\n",
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12444/12444 [==============================] - 175s 14ms/step - loss: 0.3102 - accuracy: 0.9073 - val_loss: 0.2310 - val_accuracy: 0.9300\n",
      "Epoch 2/5\n",
      "12444/12444 [==============================] - 176s 14ms/step - loss: 0.1452 - accuracy: 0.9550 - val_loss: 0.2617 - val_accuracy: 0.8854\n",
      "Epoch 3/5\n",
      "12444/12444 [==============================] - 174s 14ms/step - loss: 0.1287 - accuracy: 0.9579 - val_loss: 0.2325 - val_accuracy: 0.9333\n",
      "Epoch 4/5\n",
      "12444/12444 [==============================] - 174s 14ms/step - loss: 0.1235 - accuracy: 0.9586 - val_loss: 0.2391 - val_accuracy: 0.9325\n",
      "Epoch 5/5\n",
      "12444/12444 [==============================] - 174s 14ms/step - loss: 0.1203 - accuracy: 0.9591 - val_loss: 0.2596 - val_accuracy: 0.9282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x247f5099e20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "vectorize_layer = data_prep(train_data, seq_len = 3, vocab_size = vocab_size)\n",
    "\n",
    "\n",
    "mmodel = modelNER()\n",
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12444/12444 [==============================] - 173s 14ms/step - loss: 0.3080 - accuracy: 0.9080 - val_loss: 0.2304 - val_accuracy: 0.9299\n",
      "Epoch 2/5\n",
      "12444/12444 [==============================] - 173s 14ms/step - loss: 0.1443 - accuracy: 0.9550 - val_loss: 0.3033 - val_accuracy: 0.8846\n",
      "Epoch 3/5\n",
      "12444/12444 [==============================] - 173s 14ms/step - loss: 0.1284 - accuracy: 0.9579 - val_loss: 0.3247 - val_accuracy: 0.8749\n",
      "Epoch 4/5\n",
      "12444/12444 [==============================] - 174s 14ms/step - loss: 0.1233 - accuracy: 0.9585 - val_loss: 0.3047 - val_accuracy: 0.8741\n",
      "Epoch 5/5\n",
      "12444/12444 [==============================] - 175s 14ms/step - loss: 0.1203 - accuracy: 0.9590 - val_loss: 0.3231 - val_accuracy: 0.8731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x247f53c40a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "vectorize_layer = data_prep(train_data, seq_len = 5, vocab_size = vocab_size)\n",
    "\n",
    "\n",
    "mmodel = modelNER()\n",
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длинна послндовательности практически не влияет на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMYWUe+YbrGuKh6AT1qUB4a",
   "collapsed_sections": [],
   "name": "HW5-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
