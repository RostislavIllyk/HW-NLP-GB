{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkpcHsV8RWHA"
   },
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAQBOJRARev7"
   },
   "source": [
    "**Написать теггер на данных с руским языком**\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации  \n",
    "2. написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки  \n",
    "3. вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)  \n",
    "4. взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)    \n",
    "5. сравнить все реализованные методы сделать выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_16J0ER8WOJx"
   },
   "source": [
    "## загрузка данных"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4611,
     "status": "ok",
     "timestamp": 1617782512502,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "yPRx8Cu_RDY1",
    "outputId": "9595e8e4-9731-4cfe-a67f-cfb0d9759cd4"
   },
   "source": [
    "!pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9wgL-33mWUyZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\nlp_test\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pyconll\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "#import gensim.downloader as api\n",
    "#word_vectors = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "vXxwW9NzW570"
   },
   "source": [
    "!mkdir datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2783,
     "status": "ok",
     "timestamp": 1617782637610,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "tpwgA3svWiRw",
    "outputId": "9d4e2aa6-bcc7-4793-f85d-2ce708f88ff8"
   },
   "source": [
    "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Oymo30RBWjjl"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 609,
     "status": "ok",
     "timestamp": 1617782704463,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "XBzFe82cXGNK",
    "outputId": "3c13d3e6-498a-47bc-e729-d2f953cddfb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])\n",
    "    \n",
    "    \n",
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sendsms.megafon.ru/status/send/0123456789ABCDEF', 'X')\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train:\n",
    "    for token in sent:\n",
    "        if len(token.form) == 47:\n",
    "            print((token.form, token.upos))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OshO48XLXQar"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23568564014423887"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "default_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dj4tV8ytXTry"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772537323492737"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dj4tV8ytXTry"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829828463586425"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.882081353418933"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9119991237825633"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NOUN') \n",
    "tag = backoff_tagger(fdata_train,  \n",
    "                     [\n",
    "                      UnigramTagger, \n",
    "                      BigramTagger, \n",
    "                      TrigramTagger\n",
    "                     ],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "        \n",
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label) \n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871526, 149809)\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.9435261011694133\n",
      "(871526, 1048576)\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.9471657735988946\n",
      "(871526, 149809)\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.9487497051191319\n"
     ]
    }
   ],
   "source": [
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "    \n",
    "\n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871526, 99485)\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.7532184140464395\n",
      "(871526, 1048576)\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.7722255922892866\n",
      "(871526, 99485)\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.7532605398847437\n"
     ]
    }
   ],
   "source": [
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "    \n",
    "\n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871526, 1198385)\n",
      "TfidfVectorizer_char + HashingVectorizer_word : 0.9441327132409935\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "coder_1 = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "coder_2 = HashingVectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "\n",
    "X_train_1 = coder_1.fit_transform(train_tok)\n",
    "X_test_1 = coder_1.transform(test_tok)\n",
    "\n",
    "X_train_2 = coder_2.fit_transform(train_tok)\n",
    "X_test_2 = coder_2.transform(test_tok)\n",
    "\n",
    "\n",
    "X_train = hstack((X_train_1,X_train_2))\n",
    "X_test = hstack((X_test_1,X_test_2))\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)    \n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "print('TfidfVectorizer_char + HashingVectorizer_word :', accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c599ce61703c4696900b7c90b508119c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['но', 'кто-то', 'идти', 'я', 'навстречу', 'и', ',', 'мочь', 'быть', ',', 'пройти', 'уже', 'пол', 'путь', '…']\n",
      "['CCONJ', 'PRON', 'VERB', 'PRON', 'ADV', 'CCONJ', 'PUNCT', 'VERB', 'VERB', 'PUNCT', 'VERB', 'ADV', 'NUM', 'NOUN', 'PUNCT']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1950de95073b4934ac415e570c9a511a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['поэтому', 'ничего', 'не', 'измениться', '.']\n",
      "['ADV', 'PRON', 'PART', 'VERB', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "just_texts_trian=[]\n",
    "just_poss_train =[]\n",
    "for sent in tqdm_notebook(fdata_train):\n",
    "    just_text_sent = []\n",
    "    just_pos_sent = []\n",
    "    for point in sent:\n",
    "        just_text_sent.append(morpher.parse(point[0].lower())[0].normal_form)\n",
    "        just_pos_sent.append(point[1])\n",
    "    \n",
    "    just_texts_trian.append(just_text_sent)\n",
    "    just_poss_train.append(just_pos_sent)\n",
    "        \n",
    "    \n",
    "print(just_texts_trian[-1])\n",
    "print(just_poss_train[-1])\n",
    "\n",
    "\n",
    "just_texts_test=[]\n",
    "just_poss_test =[]\n",
    "for sent in tqdm_notebook(fdata_test):\n",
    "    just_text_sent = []\n",
    "    just_pos_sent = []\n",
    "    for point in sent:\n",
    "        just_text_sent.append(morpher.parse(point[0].lower())[0].normal_form)        \n",
    "        just_pos_sent.append(point[1])\n",
    "    \n",
    "    just_texts_test.append(just_text_sent)\n",
    "    just_poss_test.append(just_pos_sent)\n",
    "        \n",
    "    \n",
    "print(just_texts_test[-1])\n",
    "print(just_poss_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = just_texts_trian\n",
    "\n",
    "dimention = 200   # int(39694**(1/2)) = 200\n",
    "\n",
    "modelW2V = Word2Vec(sentences=sentences, vector_size=dimention, window=5, min_count=1)\n",
    "modelFT = FastText(sentences=sentences, window=5, min_count=1, vector_size=dimention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorized_data(model, num=1):\n",
    "    train_tok = []\n",
    "    train_words = []\n",
    "    train_label = []\n",
    "    \n",
    "    for i in range(len(just_texts_trian)):\n",
    "        for j in range(len(just_texts_trian[i])):\n",
    "            nums_list=[]\n",
    "            truncated_sent =''\n",
    "            try:\n",
    "            \n",
    "                for k in range(num):\n",
    "                    if (j >= k):\n",
    "                        result = model.wv[just_texts_trian[i][j-k]]\n",
    "                        word_result = just_texts_trian[i][j-k]\n",
    "                    else:\n",
    "                        result = np.zeros(dimention)\n",
    "                        word_result = '-'\n",
    "                        \n",
    "                    nums_list = nums_list + list(result)\n",
    "                    truncated_sent = truncated_sent + word_result + ' '\n",
    "                    \n",
    "                train_tok.append(np.array(nums_list))\n",
    "                train_words.append(truncated_sent)    \n",
    "                train_label.append(just_poss_train[i][j])\n",
    "            except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "    test_tok = []\n",
    "    test_words = []    \n",
    "    test_label = []            \n",
    "    for i in range(len(just_texts_test)):\n",
    "        for j in range(len(just_texts_test[i])):\n",
    "            nums_list=[]\n",
    "            truncated_sent =''\n",
    "            try:\n",
    "            \n",
    "                for k in range(num):\n",
    "                    if (j >= k):\n",
    "                        result = model.wv[just_texts_test[i][j-k]]\n",
    "                        word_result = just_texts_test[i][j-k]\n",
    "                    else:\n",
    "                        result = np.zeros(dimention)\n",
    "                        word_result = '-'\n",
    "                        \n",
    "                    nums_list = nums_list + list(result)\n",
    "                    truncated_sent = truncated_sent + word_result + ' '\n",
    "                    \n",
    "                test_tok.append(np.array(nums_list))\n",
    "                test_words.append(truncated_sent)                        \n",
    "                test_label.append(just_poss_test[i][j])\n",
    "            except:\n",
    "                    pass\n",
    "\n",
    "    return train_tok, train_label, test_tok, test_label, train_words, test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tok, train_label, test_tok, test_label, train_words, test_words = get_vectorized_data(model, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train_label), len(train_tok), len(test_label), len(test_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, num=1):\n",
    "    train_tok, train_label, test_tok, test_label, train_words, test_words = get_vectorized_data(model, num=num)\n",
    "    print(len(train_label), len(train_tok), len(test_label), len(test_tok))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    train_enc_labels = le.fit_transform(train_label) \n",
    "    test_enc_labels = le.transform(test_label)\n",
    "    print(le.classes_)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_tok = scaler.fit_transform(train_tok)\n",
    "    test_tok = scaler.fit_transform(test_tok)    \n",
    "\n",
    "\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, verbose=1, n_jobs=-1)\n",
    "    lr.fit(train_tok, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(test_tok)\n",
    "\n",
    "    print('LogisticRegression', 'word vector bild on', num, 'item(s)')\n",
    "    print(model)\n",
    "    print('Scor: ', accuracy_score(test_enc_labels, pred))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Протестируем FastText с разными эмбедингами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 1 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.901914198092542\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 2 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.9022512047989755\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 3 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.9012233343443534\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 10.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 4 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.9018046709129511\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 118692 118692\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 13.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 5 item(s)\n",
      "FastText(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.9018720722542378\n"
     ]
    }
   ],
   "source": [
    "run_test(modelFT, num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Протестируем Word2Vec с разными эмбедингами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 114586 114586\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 1 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8177875133087812\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 110739 110739\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 2 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8294096930620648\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 107686 107686\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 3 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8325594784837398\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 105044 105044\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 10.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 4 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8317181371615704\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871526 871526 102719 102719\n",
      "['ADJ' 'ADP' 'ADV' 'AUX' 'CCONJ' 'DET' 'INTJ' 'NOUN' 'NUM' 'PART' 'PRON'\n",
      " 'PROPN' 'PUNCT' 'SCONJ' 'SYM' 'VERB' 'X' None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 14.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word vector bild on 5 item(s)\n",
      "Word2Vec(vocab=39694, vector_size=200, alpha=0.025)\n",
      "Scor:  0.8303235039281924\n"
     ]
    }
   ],
   "source": [
    "run_test(modelW2V, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D, Bidirectional, LSTM\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM',\n",
       "       'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X',\n",
       "       None], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 39694\n",
    "sequence_length = 3\n",
    "\n",
    "train_tok, train_label, test_tok, test_label, train_words, test_words = get_vectorized_data(modelFT, num=sequence_length)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label) \n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((1024,), (1024,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((1024,), (1024,)), types: (tf.string, tf.int32)>\n",
      "Epoch 1/15\n",
      "851/851 [==============================] - 88s 100ms/step - loss: 0.8171 - accuracy: 0.7279 - val_loss: 0.4792 - val_accuracy: 0.8103\n",
      "Epoch 2/15\n",
      "851/851 [==============================] - 85s 99ms/step - loss: 0.3528 - accuracy: 0.8521 - val_loss: 0.4485 - val_accuracy: 0.8213\n",
      "Epoch 3/15\n",
      "851/851 [==============================] - 86s 101ms/step - loss: 0.3111 - accuracy: 0.8672 - val_loss: 0.4512 - val_accuracy: 0.8231\n",
      "Epoch 4/15\n",
      "851/851 [==============================] - 85s 100ms/step - loss: 0.2908 - accuracy: 0.8752 - val_loss: 0.4564 - val_accuracy: 0.8243\n",
      "Epoch 5/15\n",
      "851/851 [==============================] - 85s 99ms/step - loss: 0.2750 - accuracy: 0.8815 - val_loss: 0.4622 - val_accuracy: 0.8253\n",
      "Epoch 6/15\n",
      "851/851 [==============================] - 85s 100ms/step - loss: 0.2620 - accuracy: 0.8866 - val_loss: 0.4709 - val_accuracy: 0.8260\n",
      "Epoch 7/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2510 - accuracy: 0.8909 - val_loss: 0.4791 - val_accuracy: 0.8257\n",
      "Epoch 8/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2420 - accuracy: 0.8941 - val_loss: 0.4888 - val_accuracy: 0.8244\n",
      "Epoch 9/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2346 - accuracy: 0.8967 - val_loss: 0.4953 - val_accuracy: 0.8247\n",
      "Epoch 10/15\n",
      "851/851 [==============================] - 85s 100ms/step - loss: 0.2281 - accuracy: 0.8990 - val_loss: 0.5029 - val_accuracy: 0.8244\n",
      "Epoch 11/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2225 - accuracy: 0.9014 - val_loss: 0.5108 - val_accuracy: 0.8242\n",
      "Epoch 12/15\n",
      "851/851 [==============================] - 84s 99ms/step - loss: 0.2175 - accuracy: 0.9030 - val_loss: 0.5172 - val_accuracy: 0.8239\n",
      "Epoch 13/15\n",
      "851/851 [==============================] - 85s 100ms/step - loss: 0.2130 - accuracy: 0.9048 - val_loss: 0.5270 - val_accuracy: 0.8236\n",
      "Epoch 14/15\n",
      "851/851 [==============================] - 86s 101ms/step - loss: 0.2088 - accuracy: 0.9065 - val_loss: 0.5341 - val_accuracy: 0.8227\n",
      "Epoch 15/15\n",
      "851/851 [==============================] - 86s 101ms/step - loss: 0.2051 - accuracy: 0.9079 - val_loss: 0.5401 - val_accuracy: 0.8230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d423b03130>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_words, train_enc_labels))\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((test_words, test_enc_labels))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "text_ds = train_dataset.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "embedding_dim=200    # int(39694**(1/2)) = 200\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  Bidirectional(LSTM(units=32, return_sequences=True)),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(len(set(train_label)), activation=tf.keras.activations.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_dataset,\n",
    "    validation_data=test_dataset, \n",
    "    epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для __nltk.tag__ лучший вариант это:\n",
    "Комбинация из  __DefaultTagger UnigramTagger BigramTagger TrigramTagger__  \n",
    "0.9119991237825633\n",
    "\n",
    "Для __Vectorizer__ лучший вариант это:\n",
    "Комбинация из __LogisticRegression__ поверх __TfidfVectorizer__ при условии __analyzer='char'__  \n",
    "0.9487497051191319\n",
    "\n",
    "Для __FastText__, __Word2Vec__ лучший вариант это:\n",
    "Комбинация из __LogisticRegression__ поверх __FastText__ при условии эмбединга из __2-х слов__  \n",
    "0.9022512047989755\n",
    "\n",
    "Для __LSTM NN__ c __Embedding__ лучший вариант это:  \n",
    "0.8260\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для данной задачи побуквенный подход к кодированию слов выглядит предпочтительней чем по словный."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMYWUe+YbrGuKh6AT1qUB4a",
   "collapsed_sections": [],
   "name": "HW5-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
