{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ede994b",
   "metadata": {},
   "source": [
    "# HW 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a7722",
   "metadata": {},
   "source": [
    "Создайте мешок слов с помощью\n",
    "__sklearn.feature_extraction.text.CountVectorizer.fit_transform()__.   Применим его к __'tweet_stemmed'__\n",
    "и __'tweet_lemmatized'__ отдельно.\n",
    "\n",
    " Игнорируем слова, частота которых в документе строго превышает порог __0.9__ с\n",
    "помощью __max_df__.  \n",
    "\n",
    "Ограничим количество слов, попадающий в мешок, с помощью __max_features =\n",
    "1000__.  \n",
    "\n",
    "Исключим стоп-слова с помощью __stop_words='english'__.\n",
    "\n",
    "Отобразим __Bag-of-Words__ модель как __DataFrame. columns__ необходимо извлечь с\n",
    "помощью __CountVectorizer.get_feature_names()__.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419ab6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442eabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8797d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_stem</th>\n",
       "      <th>tweet_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>NaN</td>\n",
       "      <td>thought factory: left-right polarisation! #tru...</td>\n",
       "      <td>[thought, factory, left, right, polarisation, ...</td>\n",
       "      <td>[thought, factori, left, right, polaris, trump...</td>\n",
       "      <td>[thought, factori, left, right, polaris, trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling like a mermaid ð #hairflip #neverre...</td>\n",
       "      <td>[feeling, like, mermaid, hairflip, neverready,...</td>\n",
       "      <td>[feel, like, mermaid, hairflip, neverreadi, fo...</td>\n",
       "      <td>[feel, like, mermaid, hairflip, neverreadi, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>NaN</td>\n",
       "      <td>#hillary #campaigned today in #ohio((omg)) &amp;am...</td>\n",
       "      <td>[hillary, campaigned, today, ohio, omg, amp, u...</td>\n",
       "      <td>[hillari, campaign, today, ohio, omg, amp, use...</td>\n",
       "      <td>[hillari, campaign, today, ohio, omg, amp, use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>NaN</td>\n",
       "      <td>happy, at work conference: right mindset leads...</td>\n",
       "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
       "      <td>[happi, work, confer, right, mindset, lead, cu...</td>\n",
       "      <td>[happi, work, confer, right, mindset, lead, cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>NaN</td>\n",
       "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
       "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
       "      <td>[song, glad, free, download, shoegaz, newmus, ...</td>\n",
       "      <td>[song, glad, free, download, shoegaz, newmus, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet  \\\n",
       "0        0.0   @user when a father is dysfunctional and is s...   \n",
       "1        0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2        0.0                                bihday your majesty   \n",
       "3        0.0  #model   i love u take with u all the time in ...   \n",
       "4        0.0             factsguide: society now    #motivation   \n",
       "...      ...                                                ...   \n",
       "49154    NaN  thought factory: left-right polarisation! #tru...   \n",
       "49155    NaN  feeling like a mermaid ð #hairflip #neverre...   \n",
       "49156    NaN  #hillary #campaigned today in #ohio((omg)) &am...   \n",
       "49157    NaN  happy, at work conference: right mindset leads...   \n",
       "49158    NaN  my   song \"so glad\" free download!  #shoegaze ...   \n",
       "\n",
       "                                             clean_tweet  \\\n",
       "0      [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1      [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                      [bihday, majesty]   \n",
       "3                          [model, love, take, time, ur]   \n",
       "4                      [factsguide, society, motivation]   \n",
       "...                                                  ...   \n",
       "49154  [thought, factory, left, right, polarisation, ...   \n",
       "49155  [feeling, like, mermaid, hairflip, neverready,...   \n",
       "49156  [hillary, campaigned, today, ohio, omg, amp, u...   \n",
       "49157  [happy, work, conference, right, mindset, lead...   \n",
       "49158  [song, glad, free, download, shoegaze, newmusi...   \n",
       "\n",
       "                                              tweet_stem  \\\n",
       "0      [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1      [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                      [bihday, majesti]   \n",
       "3                          [model, love, take, time, ur]   \n",
       "4                            [factsguid, societi, motiv]   \n",
       "...                                                  ...   \n",
       "49154  [thought, factori, left, right, polaris, trump...   \n",
       "49155  [feel, like, mermaid, hairflip, neverreadi, fo...   \n",
       "49156  [hillari, campaign, today, ohio, omg, amp, use...   \n",
       "49157  [happi, work, confer, right, mindset, lead, cu...   \n",
       "49158  [song, glad, free, download, shoegaz, newmus, ...   \n",
       "\n",
       "                                               tweet_lem  \n",
       "0      [father, dysfunct, selfish, drag, kid, dysfunc...  \n",
       "1      [thank, lyft, credit, use, caus, offer, wheelc...  \n",
       "2                                      [bihday, majesti]  \n",
       "3                          [model, love, take, time, ur]  \n",
       "4                            [factsguid, societi, motiv]  \n",
       "...                                                  ...  \n",
       "49154  [thought, factori, left, right, polaris, trump...  \n",
       "49155  [feel, like, mermaid, hairflip, neverreadi, fo...  \n",
       "49156  [hillari, campaign, today, ohio, omg, amp, use...  \n",
       "49157  [happi, work, confer, right, mindset, lead, cu...  \n",
       "49158  [song, glad, free, download, shoegaz, newmus, ...  \n",
       "\n",
       "[49159 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_obj('combine_df')\n",
    "df = df.drop(['id'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3cea002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17197"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eefa9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caus</th>\n",
       "      <th>credit</th>\n",
       "      <th>disapoint</th>\n",
       "      <th>getthank</th>\n",
       "      <th>lyft</th>\n",
       "      <th>offer</th>\n",
       "      <th>pdx</th>\n",
       "      <th>thank</th>\n",
       "      <th>use</th>\n",
       "      <th>van</th>\n",
       "      <th>wheelchair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    caus  credit  disapoint  getthank  lyft  offer  pdx  thank  use  van  \\\n",
       "0      0       0          0         0     0      0    0      1    0    0   \n",
       "1      0       0          0         0     1      0    0      0    0    0   \n",
       "2      0       1          0         0     0      0    0      0    0    0   \n",
       "3      0       0          0         0     0      0    0      0    1    0   \n",
       "4      1       0          0         0     0      0    0      0    0    0   \n",
       "5      0       0          0         0     0      1    0      0    0    0   \n",
       "6      0       0          0         0     0      0    0      0    0    0   \n",
       "7      0       0          0         0     0      0    0      0    0    1   \n",
       "8      0       0          0         0     0      0    1      0    0    0   \n",
       "9      0       0          1         0     0      0    0      0    0    0   \n",
       "10     0       0          0         1     0      0    0      0    0    0   \n",
       "\n",
       "    wheelchair  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            1  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  \n",
       "10           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_tweet_stem = CountVectorizer(max_df=0.9, max_features = 1000, stop_words='english', binary=True,)\n",
    "count_vectorizer_tweet_lem = CountVectorizer(max_df=0.9, max_features = 1000, stop_words='english', binary=True,)\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bag_of_words_tweet_stem = count_vectorizer_tweet_stem.fit_transform(df.loc[1,'tweet_stem'])\n",
    "bag_of_words_tweet_lem  = count_vectorizer_tweet_lem.fit_transform(df.loc[1,'tweet_lem'])\n",
    "\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names_tweet_stem = count_vectorizer_tweet_stem.get_feature_names()\n",
    "feature_names_tweet_lem = count_vectorizer_tweet_lem.get_feature_names()\n",
    "\n",
    "\n",
    "pd.DataFrame(bag_of_words_tweet_stem.toarray(), columns = feature_names_tweet_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221a85d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caus</th>\n",
       "      <th>credit</th>\n",
       "      <th>disapoint</th>\n",
       "      <th>getthank</th>\n",
       "      <th>lyft</th>\n",
       "      <th>offer</th>\n",
       "      <th>pdx</th>\n",
       "      <th>thank</th>\n",
       "      <th>use</th>\n",
       "      <th>van</th>\n",
       "      <th>wheelchair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    caus  credit  disapoint  getthank  lyft  offer  pdx  thank  use  van  \\\n",
       "0      0       0          0         0     0      0    0      1    0    0   \n",
       "1      0       0          0         0     1      0    0      0    0    0   \n",
       "2      0       1          0         0     0      0    0      0    0    0   \n",
       "3      0       0          0         0     0      0    0      0    1    0   \n",
       "4      1       0          0         0     0      0    0      0    0    0   \n",
       "5      0       0          0         0     0      1    0      0    0    0   \n",
       "6      0       0          0         0     0      0    0      0    0    0   \n",
       "7      0       0          0         0     0      0    0      0    0    1   \n",
       "8      0       0          0         0     0      0    1      0    0    0   \n",
       "9      0       0          1         0     0      0    0      0    0    0   \n",
       "10     0       0          0         1     0      0    0      0    0    0   \n",
       "\n",
       "    wheelchair  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            1  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  \n",
       "10           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bag_of_words_tweet_lem.toarray(), columns = feature_names_tweet_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2aae4",
   "metadata": {},
   "source": [
    "Создайте мешок слов с помощью\n",
    "__sklearn.feature_extraction.text.TfidfVectorizer.fit_transform()__.   Применим его к __'tweet_stemmed'__\n",
    "и __'tweet_lemmatized'__ отдельно.\n",
    "\n",
    " Игнорируем слова, частота которых в документе строго превышает порог __0.9__ с\n",
    "помощью __max_df__.  \n",
    "\n",
    "Ограничим количество слов, попадающий в мешок, с помощью __max_features =\n",
    "1000__.  \n",
    "\n",
    "Исключим стоп-слова с помощью __stop_words='english'__.\n",
    "\n",
    "Отобразим __Bag-of-Words__ модель как __DataFrame. columns__ необходимо извлечь с\n",
    "помощью __CountVectorizer.get_feature_names()__.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef1143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caus</th>\n",
       "      <th>credit</th>\n",
       "      <th>disapoint</th>\n",
       "      <th>getthank</th>\n",
       "      <th>lyft</th>\n",
       "      <th>offer</th>\n",
       "      <th>pdx</th>\n",
       "      <th>thank</th>\n",
       "      <th>use</th>\n",
       "      <th>van</th>\n",
       "      <th>wheelchair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    caus  credit  disapoint  getthank  lyft  offer  pdx  thank  use  van  \\\n",
       "0    0.0     0.0        0.0       0.0   0.0    0.0  0.0    1.0  0.0  0.0   \n",
       "1    0.0     0.0        0.0       0.0   1.0    0.0  0.0    0.0  0.0  0.0   \n",
       "2    0.0     1.0        0.0       0.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "3    0.0     0.0        0.0       0.0   0.0    0.0  0.0    0.0  1.0  0.0   \n",
       "4    1.0     0.0        0.0       0.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "5    0.0     0.0        0.0       0.0   0.0    1.0  0.0    0.0  0.0  0.0   \n",
       "6    0.0     0.0        0.0       0.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "7    0.0     0.0        0.0       0.0   0.0    0.0  0.0    0.0  0.0  1.0   \n",
       "8    0.0     0.0        0.0       0.0   0.0    0.0  1.0    0.0  0.0  0.0   \n",
       "9    0.0     0.0        1.0       0.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "10   0.0     0.0        0.0       1.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "\n",
       "    wheelchair  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "5          0.0  \n",
       "6          1.0  \n",
       "7          0.0  \n",
       "8          0.0  \n",
       "9          0.0  \n",
       "10         0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_tweet_stem = TfidfVectorizer(max_df=0.9, max_features = 1000, stop_words='english', binary=True,)\n",
    "count_vectorizer_tweet_lem = TfidfVectorizer(max_df=0.9, max_features = 1000, stop_words='english', binary=True,)\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bag_of_words_tweet_stem = count_vectorizer_tweet_stem.fit_transform(df.loc[1,'tweet_stem'])\n",
    "bag_of_words_tweet_lem  = count_vectorizer_tweet_lem.fit_transform(df.loc[1,'tweet_lem'])\n",
    "\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names_tweet_stem = count_vectorizer_tweet_stem.get_feature_names()\n",
    "feature_names_tweet_lem = count_vectorizer_tweet_lem.get_feature_names()\n",
    "\n",
    "\n",
    "pd.DataFrame(bag_of_words_tweet_stem.toarray(), columns = feature_names_tweet_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933aec03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caus</th>\n",
       "      <th>credit</th>\n",
       "      <th>disapoint</th>\n",
       "      <th>getthank</th>\n",
       "      <th>lyft</th>\n",
       "      <th>offer</th>\n",
       "      <th>pdx</th>\n",
       "      <th>thank</th>\n",
       "      <th>use</th>\n",
       "      <th>van</th>\n",
       "      <th>wheelchair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    caus  credit  disapoint  getthank  lyft  offer  pdx  thank  use  van  \\\n",
       "0    0.0     0.0        0.0       0.0   0.0    0.0  0.0    1.0  0.0  0.0   \n",
       "1    0.0     0.0        0.0       0.0   1.0    0.0  0.0    0.0  0.0  0.0   \n",
       "2    0.0     1.0        0.0       0.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "3    0.0     0.0        0.0       0.0   0.0    0.0  0.0    0.0  1.0  0.0   \n",
       "4    1.0     0.0        0.0       0.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "5    0.0     0.0        0.0       0.0   0.0    1.0  0.0    0.0  0.0  0.0   \n",
       "6    0.0     0.0        0.0       0.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "7    0.0     0.0        0.0       0.0   0.0    0.0  0.0    0.0  0.0  1.0   \n",
       "8    0.0     0.0        0.0       0.0   0.0    0.0  1.0    0.0  0.0  0.0   \n",
       "9    0.0     0.0        1.0       0.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "10   0.0     0.0        0.0       1.0   0.0    0.0  0.0    0.0  0.0  0.0   \n",
       "\n",
       "    wheelchair  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "5          0.0  \n",
       "6          1.0  \n",
       "7          0.0  \n",
       "8          0.0  \n",
       "9          0.0  \n",
       "10         0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bag_of_words_tweet_lem.toarray(), columns = feature_names_tweet_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e369401",
   "metadata": {},
   "source": [
    "Создайте мешок слов с помощью\n",
    "__sklearn.feature_extraction.text.HashingVectorizer.fit_transform()__.   Применим его к __'tweet_stemmed'__\n",
    "и __'tweet_lemmatized'__ отдельно.\n",
    "\n",
    " Игнорируем слова, частота которых в документе строго превышает порог __0.9__ с\n",
    "помощью __max_df__.  \n",
    "\n",
    "Ограничим количество слов, попадающий в мешок, с помощью __max_features =\n",
    "1000__.  \n",
    "\n",
    "Исключим стоп-слова с помощью __stop_words='english'__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f53b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_tweet_stem = HashingVectorizer(n_features = 1000, stop_words='english', binary=True,)\n",
    "count_vectorizer_tweet_lem = HashingVectorizer(n_features = 1000, stop_words='english', binary=True,)\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bag_of_words_tweet_stem = count_vectorizer_tweet_stem.fit_transform(df.loc[1,'tweet_stem'])\n",
    "bag_of_words_tweet_lem  = count_vectorizer_tweet_lem.fit_transform(df.loc[1,'tweet_lem'])\n",
    "bag_of_words_tweet_stem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e999618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_tweet_lem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a304d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d1ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
